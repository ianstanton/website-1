{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A Rust client for Kubernetes in the style of a more generic client-go, a runtime abstraction inspired by controller-runtime, and a derive macro for CRDs inspired by kubebuilder. Hosted by CNCF as a Sandbox Project</p> <p>These crates build upon Kubernetes apimachinery + api concepts to enable generic abstractions. These abstractions allow Rust reinterpretations of reflectors, controllers, and custom resource interfaces, so that you can write applications easily.</p>  <p> Getting Started  Community  Crates  Github</p>"},{"location":"adopters/","title":"Adopters","text":""},{"location":"adopters/#open-source","title":"Open Source","text":"<ul> <li>linkerd-policy-controller - the policy controllers for the Linkerd service mesh</li> <li>krustlet - a complete <code>WASM</code> running <code>kubelet</code></li> <li>stackable operators - (kafka, zookeeper, and more)</li> <li>bottlerocket-update-operator</li> <li>vector</li> <li>kdash tui - terminal dashboard for kubernetes</li> <li>logdna agent</li> <li>kubeapps pinniped</li> <li>kubectl-view-allocations - kubectl plugin to list resource allocations</li> <li>krator - kubernetes operators using state machines</li> <li>hahaha - an operator that cleans up sidecars after Jobs</li> <li>kubectl-watch - a kubectl plugin to provide a pretty delta change view of being watched kubernetes resources</li> <li>gateway-api - API bindings for Kubernetes Gateway API</li> <li>blixt - A Kubernetes Gateway API-based Layer 4 Load-Balancer for ingress</li> <li>databricks-kube-operator - GitOps/Helm style management of Databricks jobs</li> <li>mirrord - Run your local service in the context of your remote cluster.</li> </ul>"},{"location":"adopters/#companies","title":"Companies","text":"<ul> <li>AWS</li> <li>Buoyant</li> <li>Deis Labs</li> <li>Stackable</li> <li>Datadog</li> <li>logdna</li> <li>Bitnami</li> <li>Materialize</li> <li>Qualified</li> <li>TrueLayer</li> <li>ViacomCBS</li> <li>nais</li> <li>Kong</li> <li>MetalBear</li> </ul> <p>If you're using <code>kube-rs</code> in production and are not on this list, please submit a pull request!</p>"},{"location":"adopters/#reverse-dependencies","title":"Reverse Dependencies","text":"<p>Open source users of <code>kube</code> are additionally viewable through reverse dependency listings on both github and crates.io (for published resources). These will contain a more comprehensive/up-to-date list of adopters, with the caveat that some of these can be more experimental.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This document describes the high-level architecture of kube-rs.</p> <p>This is intended for contributors or people interested in architecture.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The kube-rs repository contains 5 main crates, examples and tests.</p> <p>The main crate that users generally import is <code>kube</code>, and it's a straight facade crate that re-exports from the four other crates:</p> <ul> <li><code>kube_core</code> -&gt; re-exported as <code>core</code></li> <li><code>kube_client</code> -&gt; re-exported as <code>api</code> + <code>client</code> + <code>config</code> + <code>discovery</code></li> <li><code>kube_derive</code> -&gt; re-exported as <code>CustomResource</code></li> <li><code>kube_runtime</code> -&gt; re-exported as <code>runtime</code></li> </ul> <p>In terms of dependencies between these 4:</p> <ul> <li><code>kube_core</code> is used by <code>kube_runtime</code>, <code>kube_derive</code> and <code>kube_client</code></li> <li><code>kube_client</code> is used by <code>kube_runtime</code></li> <li><code>kube_runtime</code> is the highest level abstraction</li> </ul> <p>The extra indirection crate <code>kube</code> is there to avoid cyclic dependencies between the client and the runtime (if the client re-exported the runtime then the two crates would be cyclically dependent).</p> <p>NB: We refer to these crates by their <code>crates.io</code> name using underscores for separators, but the folders have dashes as separators.</p> <p>When working on features/issues with <code>kube-rs</code> you will generally work inside one of these crates at a time, so we will focus on these in isolation, but talk about possible overlaps at the end.</p>"},{"location":"architecture/#kubernetes-ecosystem-considerations","title":"Kubernetes Ecosystem Considerations","text":"<p>The Rust ecosystem does not exist in a vaccum as we take heavy inspirations from the popular Go ecosystem. In particular:</p> <ul> <li><code>core</code> module contains invariants from apimachinery that is preseved across individual apis</li> <li><code>client::Client</code> is a re-envisioning of a generic client-go</li> <li><code>runtime::Controller</code> abstraction follows conventions in controller-runtime</li> <li><code>derive::CustomResource</code> derive macro for CRDs is loosely inspired by kubebuilder's annotations</li> </ul> <p>We do occasionally diverge on matters where following the go side is worse for the rust language, but when it comes to choosing names and finding out where some modules / functionality should reside; a precedent in <code>client-go</code>, <code>apimachinery</code>, <code>controller-runtime</code> and <code>kubebuilder</code> goes a long way.</p>"},{"location":"architecture/#generated-structs","title":"Generated Structs","text":"<p>We do not maintain the kubernetes types generated from the <code>swagger.json</code> or the protos at present moment, and we do not handle client-side validation of fields relating to these types (that's left to the api-server).</p> <p>We generally use k8s-openapi's Rust bindings for Kubernetes' builtin types types, see:</p> <ul> <li>github.com:k8s-openapi</li> <li>docs.rs:k8s-openapi</li> </ul> <p>We also maintain an experimental set of Protobuf bindings, see k8s-pb.</p>"},{"location":"architecture/#crate-overviews","title":"Crate Overviews","text":""},{"location":"architecture/#kube-core","title":"kube-core","text":"<p>This crate only contains types relevant to the Kubernetes API, abstractions analogous to what you'll find inside apimachinery, and extra Rust traits that help us with generics further down in <code>kube-client</code>.</p> <p>Starting out with the basic type modules first:</p> <ul> <li><code>metadata</code>: the various metadata types; <code>ObjectMeta</code>, <code>ListMeta</code>, <code>TypeMeta</code></li> <li><code>request</code> + <code>response</code> + <code>subresource</code>: a sans-IO style http interface for the API</li> <li><code>watch</code>: a generic enum and behaviour for the watch api</li> <li><code>params</code>: generic parameters passed to sans-IO request interface (<code>ListParams</code> etc, called <code>ListOptions</code> in apimachinery)</li> </ul> <p>Then there are traits</p> <ul> <li><code>crd</code>: a versioned <code>CustomResourceExt</code> trait for <code>kube-derive</code></li> <li><code>object</code> generic conveniences for iterating over typed lists of objects, and objects following spec/status conventions</li> <li><code>resource</code>: a <code>Resource</code> trait for <code>kube-client</code>'s <code>Api</code> + a convenience <code>ResourceExt</code> trait for users</li> </ul> <p>The most important export here is the <code>Resource</code> trait and its impls. It is a pretty complex trait, with an associated type called <code>DynamicType</code> (that is default empty). Every <code>ObjectMeta</code>-using type that comes from <code>k8s-openapi</code> gets a blanket impl of <code>Resource</code> so we can use them generically (in <code>kube_client::Api</code>).</p> <p>Finally, there are two modules used by the higher level <code>discovery</code> module (in <code>kube-client</code>) and they have similar counterparts in apimachinery/restmapper + apimachinery/group_version:</p> <ul> <li><code>discovery</code>: types returned by the discovery api; capabilities, verbs, scopes, key info</li> <li><code>gvk</code>: partial type information to infer api types</li> </ul> <p>The main type here from these two modules is <code>ApiResource</code> because it can also be used to construct a <code>kube_client::Api</code> instance without compile-time type information (both <code>DynamicObject</code> and <code>Object</code> has <code>Resource</code> impls where <code>DynamicType = ApiResource</code>).</p>"},{"location":"architecture/#kube-client","title":"kube-client","text":""},{"location":"architecture/#config","title":"config","text":"<p>Contains logic for determining the runtime environment (local kubeconfigs or in-cluster) so that we can construct our <code>Config</code> from either source.</p> <ul> <li><code>Config</code> is the source-agnostic type (with all the information needed by our <code>Client</code>)</li> <li><code>Kubeconfig</code> is for loading from <code>~/.kube/config</code> or from any number of kubeconfig like files set by <code>KUBECONFIG</code> evar.</li> <li><code>Config::from_cluster_env</code> reads environment variables that are injected when running inside a pod</li> </ul> <p>In general this module has similar functionality to the upstream client-go/clientcmd module.</p>"},{"location":"architecture/#client","title":"client","text":"<p>The <code>Client</code> is one of the most complicated parts of <code>kube-rs</code>, because it has the most generic interface. People can mock the <code>Client</code>, people can replace individual components and force inject headers, people can choose their own tls stack, and - in theory - use whatever http clients they want.</p> <p>Generally, the <code>Client</code> is created from the properties of a <code>Config</code> to create a particular <code>hyper::Client</code> with a pre-configured amount of tower::Layers (see <code>TryFrom&lt;Config&gt; for Client</code>), but users can also pass in an arbitrary <code>tower::Service</code> (to fully customise or to mock). The signature restrictions on <code>Client::new</code> is commensurately large.</p> <p>The <code>tls</code> module contains the <code>openssl</code> or <code>rustls</code> interfaces to let users pick their tls stacks. The connectors created in that module is passed to <code>hyper::Client</code> based on feature selection.</p> <p>The <code>Client</code> can be created from a particular type of using the properties in the <code>Config</code> to configure its layers. Some of our layers come straight from tower-http:</p> <ul> <li><code>tower_http::DecompressionLayer</code> to deal with gzip compression</li> <li><code>tower_http::TraceLayer</code> to propagate http request information onto tracing spans.</li> <li><code>tower_http::AddAuthorizationLayer</code> to set bearer tokens / basic auth (when needed)</li> </ul> <p>but we also have our own layers in the <code>middleware</code> module:</p> <ul> <li><code>BaseUriLayer</code> prefixes <code>Config::base_url</code> to requests</li> <li><code>AuthLayer</code> configures either <code>AddAuthorizationLayer</code> or <code>AsyncFilterLayer&lt;RefreshableToken&gt;</code> depending on authentication method in the kubeconfig. <code>AsyncFilterLayer&lt;RefreshableToken&gt;</code> is like <code>AddAuthorizationLayer</code>, but with a token that's refreshed when necessary.</li> </ul> <p>(The <code>middleware</code> module is kept small to avoid mixing the business logic (<code>client::auth</code> openid connect oauth provider logic) with the tower layering glue.)</p> <p>The exported layers and tls connectors are mainly exposed through the <code>config_ext</code> module's <code>ConfigExt</code> trait which is only implemented by <code>Config</code> (because the config has all the properties needed for this in general, and it helps minimise our api surface).</p> <p>Finally, the <code>Client</code> manages other key aspects of IO the protocol such as:</p> <ul> <li><code>Client::connect</code> performs an HTTP Upgrade for specialised verbs</li> <li><code>Client::request</code> handles 90% of all requests</li> <li><code>Client::request_events</code> handles streaming <code>watch</code> eventss using <code>tokio_utils</code>'s <code>FramedRead</code> codec</li> <li><code>Client::request_status</code> handles <code>Either&lt;T, Status&gt;</code> responses from kubernetes</li> </ul>"},{"location":"architecture/#api","title":"api","text":"<p>The generic <code>Api</code> type and its methods.</p> <p>Builds on top of the <code>Request</code> / <code>Response</code> interface in <code>kube_core</code> by parametrising over a generic type <code>K</code> that implement <code>Resource</code> (plus whatever else is needed).</p> <p>The <code>Api</code> absorbs a <code>Client</code> on construction and is then configured with its <code>Scope</code> (through its <code>::namespaced</code> / <code>::default_namespaced</code> or <code>::all</code> constructors).</p> <p>For dynamic types (<code>Object</code> and <code>DynamicObject</code>) it has slightly more complicated constructors which have the <code>_with</code> suffix.</p> <p>The <code>core_methods</code> and most <code>subresource</code> methods generally follow this recipe:</p> <ul> <li>create <code>Request</code></li> <li>store the kubernetes verb in the [<code>http::Extensions</code>] object</li> <li>call the request with the <code>Client</code> and tell it what type(s) to deserialize into</li> </ul> <p>Some subresource methods (behind the <code>ws</code> feature) use the <code>remote_command</code> module's <code>AttachedProcess</code> interface expecting a duplex stream to deal with specialised websocket verbs (<code>exec</code> and <code>attach</code>) and is calling <code>Client::connect</code> first to get that stream.</p>"},{"location":"architecture/#discovery","title":"discovery","text":"<p>Deals with dynamic discovery of what apis are available on the api-server. Normally this can be used to discover custom resources, but also certain standard resources that vary between providers.</p> <p>The <code>Discovery</code> client can be used to do a full recursive sweep of api-groups into all api resources (through <code>filter</code>/<code>exclude</code> -&gt; <code>run</code>) and then the users can periodically re-<code>run</code> to keep the cache up to date (as kubernetes is being upgraded behind the scenes).</p> <p>The <code>discovery</code> module also contains a way to run smaller queries through the <code>oneshot</code> module; e.g. resolving resource name when having group version kind, resolving every resource within one specific group, or even one group at a pinned version.</p> <p>The equivalent Go logic is found in client-go/discovery</p>"},{"location":"architecture/#kube-derive","title":"kube-derive","text":"<p>The smallest crate. A simple derive proc_macro to generate Kubernetes wrapper structs and trait impls around a data struct.</p> <p>Uses <code>darling</code> to parse <code>#[kube(attrs...)]</code> then uses <code>syn</code> and <code>quote</code> to produce a suitable syntax tree based on the attributes requested.</p> <p>It ultimately contains a lot of ugly json coercing from attributes into serialization code, but this is code that everyone working with custom resources need.</p> <p>It has hooks into <code>schemars</code> when using <code>JsonSchema</code> to ensure the correct type of CRD schema is attached to the right part of the generated custom resource definition.</p>"},{"location":"architecture/#kube-runtime","title":"kube-runtime","text":"<p>The highest level crate that deals with the highest level abstractions (such as controllers/watchers/reflectors) and specific Kubernetes apis that need common care (finalisers, waiting for conditions, event publishing).</p>"},{"location":"architecture/#watcher","title":"watcher","text":"<p>The <code>watcher</code> module contains state machine wrappers around <code>Api::watch</code> that will watch and auto-recover on allowable failures. The <code>watcher</code> fn is the general purpose one that is similar to informers in Go land, and will watch a collection of objects. The <code>watch_object</code> is a specialised version of this that watches a single object.</p>"},{"location":"architecture/#reflector","title":"reflector","text":"<p>The <code>reflector</code> module contains wrappers around <code>watcher</code> that will cache objects in memory. The <code>reflector</code> fn wraps a <code>watcher</code> and a state <code>Store</code> that is updated on every event emitted by the <code>watcher</code>.</p> <p>The reason for the difference between <code>watcher::Event</code> (created by <code>watcher</code>) and <code>kube::api::WatchEvent</code> (created by <code>Api::watch</code>) is that <code>watcher</code> will deals with desync errors and do a full relist whose result is then propagated as a single event, ensuring the <code>reflector</code> can do a single, atomic update to its state <code>Store</code>.</p>"},{"location":"architecture/#controller","title":"controller","text":"<p>The <code>controller</code> module contains the <code>Controller</code> type and its associated definitions.</p> <p>The <code>Controller</code> is configured to watch one root object (configured via <code>::new</code>), and several owned objects (via <code>::owns</code>), and - once <code>::run</code> - it will hit a users <code>reconcile</code> function for every change to the root object or any of its child objects (and internally it will traverse up the object tree - usually through owner references - to find the affected root object).</p> <p>The user is then meant to provide an idempotent <code>reconcile</code> fn, that does not know what underlying object was changed, to ensure the state configured in its crd, is what can be seen in the world.</p> <p>To manage this, a vector of watchers is converted into a set of streams of the same type by mapping the watchers so they have the same output type. This is why <code>watches</code> and <code>owns</code> differ: <code>owns</code> looks up <code>OwnerReferences</code>, but <code>watches</code> need you to define the relation yourself with a <code>mapper</code>. The mappers we support are <code>trigger_owners</code>, <code>trigger_self</code>, and the custom <code>trigger_with</code>.</p> <p>Once we have combined the stream of streams we essentially have a flattened super stream with events from multiple watchers that will act as our input events. With this, the <code>applier</code> can start running its fairly complex machinery:</p> <ol> <li>new input events get sent to the <code>scheduler</code></li> <li>scheduled events are then passed them through a <code>Runner</code> preventing duplicate parallel requests for the same object</li> <li>when running, we send the affected object to the users <code>reconciler</code> fn and await that future</li> <li>a) on success, prepare the users <code>Action</code> (generally a slow requeue several minutes from now)</li> <li>b) on failure, prepare a <code>Action</code> based on the users error policy (generally a backoff'd requeue with shorter initial delay)</li> <li>Map resulting <code>Action</code>s through an ad-hoc <code>scheduler</code> channel</li> <li>Resulting requeue requests through the channel are picked up at the top of <code>applier</code> and merged with input events in step 1.</li> </ol> <p>Ideally, the process runs forever, and it minimises unnecessary reconcile calls (like users changing more than one related object while one reconcile is already happening).</p> <p>See controller internals for some more information on this.</p>"},{"location":"architecture/#finalizer","title":"finalizer","text":"<p>Contains a helper wrapper <code>finalizer</code> for a <code>reconcile</code> fn used by a <code>Controller</code> when a user is using finalizers to handle garbage collection.</p> <p>This lets the user focus on simply selecting the type of behaviour they would like to exhibit based on whether the object is being deleted or it's just being regularly reconciled (through enum matching on <code>finalizer::Event</code>). This lets the user elide checking for potential deletion timestamps and manage the state machinery of <code>metadata.finalizers</code> through jsonpatching.</p>"},{"location":"architecture/#wait","title":"wait","text":"<p>Contains helpers for waiting for <code>conditions</code>, or objects to be fully removed (i.e. waiting for finalizers post delete).</p> <p>These build upon <code>watch_object</code> with specific mappers.</p>"},{"location":"architecture/#events","title":"events","text":"<p>Contains an event <code>Recorder</code> ala client-go/events that controllers can hook into, to publish events related to their reconciliations.</p>"},{"location":"architecture/#crate-delineation-and-overlaps","title":"Crate Delineation and Overlaps","text":"<p>When working on the the client machinery, it's important to realise that there are effectively 5 layers involved:</p> <ol> <li>Sans-IO request builder (in <code>kube_core::Request</code>)</li> <li>IO (in <code>kube_client::Client</code>)</li> <li>Typing (in <code>kube_client::Api</code>)</li> <li>Helpers for using the API correctly (e.g.<code>kube_runtime::watcher</code>)</li> <li>High-level abstractions for specific tasks (e.g. <code>kube_runtime::controller</code>)</li> </ol> <p>At level 3, we essentially have what the K8s team calls a basic client. As a consequence, new methods/subresources typically cross 2 crate boundaries (<code>kube_core</code>, <code>kube_client</code>), and needs to touch 3 main modules.</p> <p>Similarly, there are also the traits and types that define what an api means in <code>kube_core</code> like <code>Resource</code> and <code>ApiResource</code>. If modifying these, then changes to <code>kube-derive</code> are likely necessary, as it needs to directly implement this for users.</p> <p>These types of cross-crate dependencies are why we expose <code>kube</code> as a single versioned facade crate that users can upgrade atomically (without being caught in the middle of a publish cycle). This also gives us better compatibility with <code>dependabot</code>.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<ul> <li>see https://github.com/kube-rs/kube/compare/0.78.0...main</li> </ul>"},{"location":"changelog/#0780--2023-01-06","title":"0.78.0 / 2023-01-06","text":""},{"location":"changelog/#kubernetes-bump","title":"Kubernetes Bump","text":"<p>This release brings in the new <code>k8s-openapi</code> release for <code>1.26</code> structs, and sets our MK8SV to <code>1.21</code>. Be sure to upgrade <code>k8s-openapi</code> and <code>kube</code> simultaneously to avoid multiple version errors:</p> <pre><code>cargo upgrade -p k8s-openapi -p kube -i\n</code></pre>"},{"location":"changelog/#whats-changed","title":"What's Changed","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>reflector: add helper function to the <code>Store</code> by @eliad-wiz in https://github.com/kube-rs/kube/pull/1111</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Bump <code>k8s-openapi@0.17.0</code> and MK8SV by @clux in https://github.com/kube-rs/kube/pull/1116</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Remove deprecated <code>Config::timeout</code> by @clux in https://github.com/kube-rs/kube/pull/1113</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>fix shell exec exiting message loop when terminalSizeReceiver is dropped by @erebe in https://github.com/kube-rs/kube/pull/1112</li> </ul>"},{"location":"changelog/#0770--2022-12-15","title":"0.77.0 / 2022-12-15","text":""},{"location":"changelog/#highlights","title":"Highlights","text":"<p>This release saw numerous improvements across various parts of the codebase with lots of help from external contributors. Look for improvements in error handling, client exec behaviour, dynamic object conversion, certificate handling, and last, but not least; lots of enhancements in the <code>config</code> module. Huge thanks to everyone who contributed!</p>"},{"location":"changelog/#config-enhancements","title":"<code>Config</code> Enhancements","text":"<p>Kubeconfigs relying on <code>ExecConfig</code> for auth should now work with a lot more cases (with improvements to script interactivity, cert passing, env-drop, and windows behaviour). We further aligned our <code>Kubeconfig</code> parsing with client-go's behaviour, and also exposed <code>Kubeconfig::merge</code>. Finally, we now pass <code>Config::tls_server_name</code> through to the <code>Client</code>, which has let us include a better rustls workaround for the long-standing ip issue (enabled by default).</p>"},{"location":"changelog/#whats-changed_1","title":"What's Changed","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add <code>DynamicObjects::try_parse</code> for typed object conversion by @jmintb in https://github.com/kube-rs/kube/pull/1061</li> <li>Add <code>ExecConfig::drop_env</code> to filter host evars for auth providers by @aviramha in https://github.com/kube-rs/kube/pull/1062</li> <li>Add support for terminal size when executing command inside a container by @armandpicard in https://github.com/kube-rs/kube/pull/983</li> <li>add cmd-drop-env to AuthProviderConfig by @aviramha in https://github.com/kube-rs/kube/pull/1074</li> <li>Check for client cert with exec by @rcanderson23 in https://github.com/kube-rs/kube/pull/1089</li> <li>Change <code>Kubeconfig::merge</code> fn to public. by @goenning in https://github.com/kube-rs/kube/pull/1100</li> <li>Fix interactivity in auth exec by @armandpicard in https://github.com/kube-rs/kube/pull/1083</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>[windows] skip window creation on auth exec by @goenning in https://github.com/kube-rs/kube/pull/1095</li> <li>Add <code>Config::tls_server_name</code> and validate when using rustls by @clux in https://github.com/kube-rs/kube/pull/1104</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Remove deprecated <code>ResourceExt::name</code> by @clux in https://github.com/kube-rs/kube/pull/1105</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Bump tracing dependency to 0.1.36 by @teozkr in https://github.com/kube-rs/kube/pull/1070</li> <li>Improve error message on azure auth not being supported by @goenning in https://github.com/kube-rs/kube/pull/1082</li> <li>exec: ensure certs always end with a new line by @goenning in https://github.com/kube-rs/kube/pull/1096</li> <li>fix: align kube-rs with client-go config parsing by @goenning in https://github.com/kube-rs/kube/pull/1077</li> <li>Return error from <code>watcher</code> when kinds do not support watch by @clux in https://github.com/kube-rs/kube/pull/1101</li> </ul>"},{"location":"changelog/#0760--2022-10-28","title":"0.76.0 / 2022-10-28","text":""},{"location":"changelog/#highlights_1","title":"Highlights","text":""},{"location":"changelog/#derivecustomresource-now-supports-schemas-with-untagged-enums","title":"<code>#[derive(CustomResource)]</code> now supports schemas with untagged enums","text":"<p>Expanding on our existing support for storing Rust's struct enums in CRDs, Kube will now try to convert <code>#[serde(untagged)]</code> enums as well. Note that if the same field is present in multiple untagged variants then they must all have the same shape.</p>"},{"location":"changelog/#removed-deprecated-try_flatten_-functions","title":"Removed deprecated <code>try_flatten_*</code> functions","text":"<p>These have been deprecated since 0.72, and are replaced by the equivalent <code>WatchStreamExt</code> methods.</p>"},{"location":"changelog/#whats-changed_2","title":"What's Changed","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Adds example to <code>Controller::watches</code> by @Dav1dde in https://github.com/kube-rs/kube/pull/1026</li> <li>Discovery: Add <code>ApiGroup::resources_by_stability</code> by @imuxin in https://github.com/kube-rs/kube/pull/1022</li> <li>Add support for untagged enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/1028</li> <li>Derive PartialEq for DynamicObject by @pbzweihander in https://github.com/kube-rs/kube/pull/1048</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Runtime: Remove deprecated util <code>try_flatten_</code> helpers by @clux in https://github.com/kube-rs/kube/pull/1019</li> <li>Remove <code>native-tls</code> feature by @kazk in https://github.com/kube-rs/kube/pull/1044</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>add fieldManager querystring to all operations by @goenning in https://github.com/kube-rs/kube/pull/1031</li> <li>Add verify_tls1x_signature for NoCertVerification by @rvql in https://github.com/kube-rs/kube/pull/1034</li> <li>Fix compatibility with schemars' preserve_order feature by @teozkr in https://github.com/kube-rs/kube/pull/1050</li> <li>Hoist enum values from subschemas by @teozkr in https://github.com/kube-rs/kube/pull/1051</li> </ul>"},{"location":"changelog/#0750--2022-09-21","title":"0.75.0 / 2022-09-21","text":""},{"location":"changelog/#highlights_2","title":"Highlights","text":""},{"location":"changelog/#upgrade-k8s-openapi-to-016-for-kubernetes-125","title":"Upgrade <code>k8s-openapi</code> to 0.16 for Kubernetes 1.25","text":"<p>The update to k8s-openapi@0.16.0 makes this the first release with tentative Kubernetes 1.25 support. While the new structs and apis now exist, we recommend holding off on using 1.25 until a deserialization bug in the apiserver is resolved upstream. See #997 / #1008 for details.</p> <p>To upgrade, ensure you bump both <code>kube</code> and <code>k8s-openapi</code>:</p> <pre><code>cargo upgrade kube k8s-openapi\n</code></pre>"},{"location":"changelog/#newold-configincluster-default-to-connect-in-cluster","title":"New/Old <code>Config::incluster</code> default to connect in cluster","text":"<p>Our previous default of connecting to the Kubernetes apiserver via <code>kubernetes.default.svc</code> has been reverted back to use the old environment variables after Kubernetes updated their position that the environment variables are not legacy. This does unfortunately regress on <code>rustls</code> support, so for those users we have included a <code>Config::incluster_dns</code> to work around the old rustls issue while it is open.</p>"},{"location":"changelog/#controller-error_policy-extension","title":"Controller <code>error_policy</code> extension","text":"<p>The <code>error_policy</code> fn now has access to the <code>object</code> that failed the reconciliation to ease metric creation / failure attribution. The following change is needed on the user side:</p> <pre><code>-fn error_policy(error: &amp;Error, ctx: Arc&lt;Data&gt;) -&gt; Action {\n+fn error_policy(_obj: Arc&lt;YourObject&gt;, error: &amp;Error, ctx: Arc&lt;Data&gt;) -&gt; Action {\n</code></pre>"},{"location":"changelog/#polish--subresources--conversion","title":"Polish / Subresources / Conversion","text":"<p>There are also a slew of ergonomics improvements, closing of gaps in subresources, adding initial support for <code>ConversionReview</code>, making <code>Api::namespaced</code> impossible to use for non-namepaced resources (a common pitfall), as well as many great fixes to the edge cases in portforwarding and finalizers. Many of these changes came from first time contributors. A huge thank you to everyone involved.</p>"},{"location":"changelog/#whats-changed_3","title":"What's Changed","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Make <code>Config::auth_info</code> public by @danrspencer in https://github.com/kube-rs/kube/pull/959</li> <li>Make raw <code>Client::send</code> method public by @tiagolobocastro in https://github.com/kube-rs/kube/pull/972</li> <li>Make <code>types</code> on <code>AdmissionRequest</code> and <code>AdmissionResponse</code> public by @clux in https://github.com/kube-rs/kube/pull/977</li> <li>Add <code>#[serde(default)]</code> to metadata field of <code>DynamicObject</code> by @pbzweihander in https://github.com/kube-rs/kube/pull/987</li> <li>Add <code>create_subresource</code> method to <code>Api</code> and <code>create_token_request</code> method to <code>Api&lt;ServiceAccount&gt;</code> by @pbzweihander in https://github.com/kube-rs/kube/pull/989</li> <li>Controller: impl Eq and PartialEq for <code>Action</code> by @Sherlock-Holo in https://github.com/kube-rs/kube/pull/993</li> <li>Add support for CRD <code>ConversionReview</code> types by @MikailBag in https://github.com/kube-rs/kube/pull/999</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Constrain Resource trait and Api::namespaced by Scope by @clux in https://github.com/kube-rs/kube/pull/956</li> <li>Add connect/read/write timeouts to <code>Config</code> by @goenning in https://github.com/kube-rs/kube/pull/971</li> <li>Controller: Include the object being reconciled in the <code>error_policy</code> by @felipesere in https://github.com/kube-rs/kube/pull/995</li> <li><code>Config</code>: New <code>incluster</code> and <code>incluster_dns</code> constructors by @olix0r in https://github.com/kube-rs/kube/pull/1001</li> <li>Upgrade <code>k8s-openapi</code> to 0.16 by @clux in https://github.com/kube-rs/kube/pull/1008</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Remove <code>tracing::instrument</code> from <code>apply_debug_overrides</code> by @kazk in https://github.com/kube-rs/kube/pull/958</li> <li>fix duplicate finalizers race condition by @alex-hunt-materialize in https://github.com/kube-rs/kube/pull/965</li> <li>fix: portforward connection cleanup by @tiagolobocastro in https://github.com/kube-rs/kube/pull/973</li> </ul>"},{"location":"changelog/#0740--2022-07-09","title":"0.74.0 / 2022-07-09","text":""},{"location":"changelog/#highlights_3","title":"Highlights","text":""},{"location":"changelog/#polish-bug-fixes-guidelines-ci-improvements-and-new-contributors","title":"Polish, bug fixes, guidelines, ci improvements, and new contributors","text":"<p>This release features smaller improvements/additions/cleanups/fixes, many of which are from new first-time contributors! Thank you everyone! The listed deadlock fix was backported to 0.73.1.</p> <p>We have also been trying to clarify and prove a lot more of our external-facing guarantees, and as a result:</p> <ul> <li>We have codified our Kubernetes versioning policy </li> <li>The Rust version policy has extended its support range</li> <li>Our CI has been extended</li> </ul>"},{"location":"changelog/#resourceextname-deprecation","title":"<code>ResourceExt::name</code> deprecation","text":"<p>A consequence of all the policy writing and the improved clarity we have decided to deprecate the common <code>ResourceExt::name</code> helper.</p> <p>This method could panic and it is unexpected for the users and bad for our consistency. To get the old functionality, you can replace any <code>.name()</code> call on a Kubernetes resources with <code>.name_unchecked()</code>; but as the name implies, it can panic (in a local setting, or during admission). We recommend you replace it with the new <code>ResourceExt::name_any</code> for a general identifier:</p> <pre><code>-pod.name()\n+pod.name_any()\n</code></pre>"},{"location":"changelog/#whats-changed_4","title":"What's Changed","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Add support for passing the <code>fieldValidation</code> query parameter on patch by @phroggyy in https://github.com/kube-rs/kube/pull/929</li> <li>Add <code>conditions::is_job_completed</code> by @clux in https://github.com/kube-rs/kube/pull/935</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Deprecate <code>ResourceExt::name</code> in favour of safe name_* alternatives by @clux in https://github.com/kube-rs/kube/pull/945</li> </ul>"},{"location":"changelog/#removed_3","title":"Removed","text":"<ul> <li>Remove <code>#[kube(apiextensions)]</code> flag from <code>kube-derive</code> by @clux in https://github.com/kube-rs/kube/pull/920</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Document every public derived fn from kube-derive by @clux in https://github.com/kube-rs/kube/pull/919</li> <li>fix applier hangs which can happen with many watched objects by @moustafab in https://github.com/kube-rs/kube/pull/925</li> <li>Applier: Improve reconciler reschedule context to avoid deadlocking on full channel by @teozkr in https://github.com/kube-rs/kube/pull/932</li> <li>Fix deserialization issue in AdmissionResponse by @clux in https://github.com/kube-rs/kube/pull/939</li> <li>Admission controller example fixes by @Alibirb in https://github.com/kube-rs/kube/pull/950</li> </ul>"},{"location":"changelog/#0731--2022-06-03","title":"0.73.1 / 2022-06-03","text":""},{"location":"changelog/#highlights_4","title":"Highlights","text":"<p>This patch release fixes a bug causing <code>applier</code> and <code>Controller</code> to deadlock when too many Kubernetes object change events were ingested at once. All users of <code>applier</code> and <code>Controller</code> are encouraged to upgrade as quickly as possible. Older versions are also affected, this bug is believed to have existed since the original release of <code>kube_runtime</code>.</p>"},{"location":"changelog/#whats-changed_5","title":"What's Changed","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>[0.73 backport] fix applier hangs which can happen with many watched objects (#925) by @moustafab (backported by @teozkr) in https://github.com/kube-rs/kube/pull/927</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.73.0...0.73.1</p>"},{"location":"changelog/#0730--2022-05-23","title":"0.73.0 / 2022-05-23","text":""},{"location":"changelog/#highlights_5","title":"Highlights","text":""},{"location":"changelog/#new-k8s-openapi-version-and-msrv","title":"New <code>k8s-openapi</code> version and MSRV","text":"<p>Support added for Kubernetes <code>v1_24</code> support via the new <code>k8s-openapi</code> version. Please also run <code>cargo upgrade --workspace k8s-openapi</code> when upgrading <code>kube</code>.</p> <p>This also bumps our MSRV to <code>1.60.0</code>.</p>"},{"location":"changelog/#reconciler-change","title":"Reconciler change","text":"<p>A small ergonomic change in the <code>reconcile</code> signature has removed the need for the <code>Context</code> object. This has been replaced by an <code>Arc</code>. The following change is needed in your controller:</p> <pre><code>-async fn reconcile(doc: Arc&lt;MyObject&gt;, context: Context&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt;\n+async fn reconcile(doc: Arc&lt;MyObject&gt;, context: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt;\n</code></pre> <p>This will simplify the usage of the <code>context</code> argument. You should no longer need to pass <code>.get_ref()</code> on its every use. See the controller-rs upgrade change for details.</p>"},{"location":"changelog/#whats-changed_6","title":"What's Changed","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Add Discovery::groups_alphabetical following kubectl sort order by @clux in https://github.com/kube-rs/kube/pull/887</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Replace runtime::controller::Context with Arc by @teozkr in https://github.com/kube-rs/kube/pull/910</li> <li>runtime: Return the object from <code>await_condition</code> by @olix0r in https://github.com/kube-rs/kube/pull/877</li> <li>Bump k8s-openapi to 0.15 for kubernetes v1_24 and bump MSRV to 1.60 by @clux in https://github.com/kube-rs/kube/pull/916</li> </ul>"},{"location":"changelog/#0720--2022-05-13","title":"0.72.0 / 2022-05-13","text":""},{"location":"changelog/#highlights_6","title":"Highlights","text":""},{"location":"changelog/#ergonomics-improvements","title":"Ergonomics improvements","text":"<p>A new <code>runtime::WatchSteamExt</code> (#899 + #906) allows for simpler setups for streams from <code>watcher</code> or <code>reflector</code>.</p> <pre><code>- let stream = utils::try_flatten_applied(StreamBackoff::new(watcher(api, lp), b));\n+ let stream = watcher(api, lp).backoff(b).applied_objects();\n</code></pre> <p>The <code>util::try_flatten_*</code> helpers have been marked as deprecated since they are not used by the stream impls.</p> <p>A new <code>reflector:store()</code> fn allows simpler reflector setups #907:</p> <pre><code>- let store = reflector::store::Writer::&lt;Node&gt;::default();\n- let reader = store.as_reader();\n+ let (reader, writer) = reflector::store();\n</code></pre> <p>Additional conveniences getters/settes to <code>ResourceExt</code> for manged_fields and creation_timestamp #888 + #898, plus a <code>GroupVersion::with_kind</code> path to a GVK, and a <code>TryFrom&lt;TypeMeta&gt; for GroupVersionKind</code> in #896.</p>"},{"location":"changelog/#crd-version-selection","title":"CRD Version Selection","text":"<p>Managing multiple version in CustomResourceDefinitions can be pretty complicated, but we now have helpers and docs on how to tackle it.</p> <p>A new function <code>kube::core::crd::merge_crds</code> have been added (in #889) to help push crd schemas generated by kube-derived crds with different <code>#[kube(version)]</code> properties. See the kube-derive#version documentation for details.</p> <p>A new example showcases how one can manage two or more versions of a crd and what the expected truncation outcomes are when moving between versions.</p>"},{"location":"changelog/#examples","title":"Examples","text":"<p>Examples now have moved to <code>tracing</code> for its logging, respects <code>RUST_LOG</code>, and namespace selection via the kubeconfig context. There is also a larger kubectl example showcasing <code>kubectl apply -f yaml</code> as well as <code>kubectl {edit,delete,get,watch}</code> via #885 + #897.</p>"},{"location":"changelog/#whats-changed_7","title":"What's Changed","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Allow merging multi-version CRDs into a single schema by @clux in https://github.com/kube-rs/kube/pull/889</li> <li>Add GroupVersion::with_kind and TypeMeta -&gt; GroupVersionKind converters by @clux in https://github.com/kube-rs/kube/pull/896</li> <li>Add managed_fields accessors to ResourceExt by @clux in https://github.com/kube-rs/kube/pull/898</li> <li>Add ResourceExt::creation_timestamp by @clux in https://github.com/kube-rs/kube/pull/888</li> <li>Support lowercase http_proxy &amp; https_proxy evars by @DevineLiu in https://github.com/kube-rs/kube/pull/892</li> <li>Add a WatchStreamExt trait for stream chaining by @clux in https://github.com/kube-rs/kube/pull/899</li> <li>Add Event::modify + reflector::store helpers by @clux in https://github.com/kube-rs/kube/pull/907</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Switch to kubernetes cluster dns for incluster url everywhere by @clux in https://github.com/kube-rs/kube/pull/876</li> <li>Update tower-http requirement from 0.2.0 to 0.3.2 by @dependabot in https://github.com/kube-rs/kube/pull/893</li> </ul>"},{"location":"changelog/#removed_4","title":"Removed","text":"<ul> <li>Remove deprecated legacy crd v1beta1 by @clux in https://github.com/kube-rs/kube/pull/890</li> </ul>"},{"location":"changelog/#0710--2022-04-12","title":"0.71.0 / 2022-04-12","text":""},{"location":"changelog/#highlights_7","title":"Highlights","text":"<p>Several quality of life changes and improvement this release for port-forwarding, a new <code>ClientBuilder</code>, better handling of <code>kube-derive</code> edge-cases.</p> <p>We highlight some changes here that you should be especially aware of.</p>"},{"location":"changelog/#eventsrecorder-publishing-to-kube-system-for-cluster-scoped-resources","title":"events::Recorder publishing to <code>kube-system</code> for cluster scoped resources","text":"<p>Publishing events via Recorder for cluster scoped resources (supported since <code>0.70.0</code>) now publish to <code>kube-system</code> rather than <code>default</code>, as all but the newest clusters struggle with publishing events in the <code>default</code> namespace.</p>"},{"location":"changelog/#default-tls-stack-set-to-openssl","title":"Default TLS stack set to OpenSSL","text":"<p>The previous <code>native-tls</code> default  was there because we used to depend on <code>reqwest</code>, but because we depended on openssl anyway the feature does not make much sense. Changing to <code>openssl-tls</code> also improves the situation on macOS where the Security Framework struggles with PKCS#12 certs from OpenSSL v3.  The <code>native-tls</code> feature will still be available in this release in case of issues, but the plan is to decommission it shortly. Of course, we all ideally want to move to rustls, but we are still blocked by #153.</p>"},{"location":"changelog/#whats-changed_8","title":"What's Changed","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Add <code>ClientBuilder</code> that lets users add custom middleware without full stack replacement by @teozkr in https://github.com/kube-rs/kube/pull/855</li> <li>Support top-level enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/856</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>portforward: Improve API and support background task cancelation by @olix0r in https://github.com/kube-rs/kube/pull/854</li> <li>Make remote commands cancellable and remove panics by @kazk in https://github.com/kube-rs/kube/pull/861</li> <li>Change the default TLS to OpenSSL by @kazk in https://github.com/kube-rs/kube/pull/863</li> <li>change event recorder cluster namespace to kube-system by @clux in https://github.com/kube-rs/kube/pull/871</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Fix schemas containing both properties and additionalProperties by @jcaesar in https://github.com/kube-rs/kube/pull/845</li> <li>Make dependency pins between sibling crates stricter by @clux in https://github.com/kube-rs/kube/pull/864</li> <li>Fix in-cluster kube_host_port generation for IPv6 by @somnusfish in https://github.com/kube-rs/kube/pull/875</li> </ul>"},{"location":"changelog/#0700--2022-03-20","title":"0.70.0 / 2022-03-20","text":""},{"location":"changelog/#highlights_8","title":"Highlights","text":""},{"location":"changelog/#support-for-ec-keys-with-rustls","title":"Support for EC keys with rustls","text":"<p>This was one of the big blockers for using <code>rustls</code> against clusters like <code>k3d</code> or <code>k3s</code> While not sufficient to fix using those clusters out of the box, it is now possible to use them with a workarodund</p>"},{"location":"changelog/#more-ergonomic-reconciler","title":"More ergonomic reconciler","text":"<p>The signature and end the <code>Ok</code> action in <code>reconcile</code> fns has been simplified slightly, and requires the following user updates:</p> <pre><code>-async fn reconcile(obj: Arc&lt;MyObject&gt;, ctx: Context&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction, Error&gt; {\n-    ...\n-    Ok(ReconcilerAction {\n-        requeue_after: Some(Duration::from_secs(300)),\n-    })\n+async fn reconcile(obj: Arc&lt;MyObject&gt;, ctx: Context&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n+    ...\n+    Ok(Action::requeue(Duration::from_secs(300)))\n</code></pre> <p>The <code>Action</code> import lives in the same place as the old <code>ReconcilerAction</code>.</p>"},{"location":"changelog/#whats-changed_9","title":"What's Changed","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Add support for EC private keys by @farcaller in https://github.com/kube-rs/kube/pull/804</li> <li>Add helper for creating a controller owner_ref on Resource by @clux in https://github.com/kube-rs/kube/pull/850</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Remove <code>scheduler::Error</code> by @teozkr in https://github.com/kube-rs/kube/pull/827</li> <li>Bump parking_lot to 0.12, but allow dep duplicates by @clux in https://github.com/kube-rs/kube/pull/836</li> <li>Update tokio-tungstenite requirement from 0.16.1 to 0.17.1 by @dependabot in https://github.com/kube-rs/kube/pull/841</li> <li>Let OccupiedEntry::commit take PostParams by @teozkr in https://github.com/kube-rs/kube/pull/842</li> <li>Change ReconcileAction to Action and add associated ctors by @clux in https://github.com/kube-rs/kube/pull/851</li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Token reloading with RwLock by @kazk in https://github.com/kube-rs/kube/pull/835</li> <li>Fix event publishing for cluster scoped crds by @zhrebicek in https://github.com/kube-rs/kube/pull/847</li> <li>Fix invalid CRD when Enum variants have descriptions by @sbernauer in https://github.com/kube-rs/kube/pull/852</li> </ul>"},{"location":"changelog/#0691--2022-02-16","title":"0.69.1 / 2022-02-16","text":""},{"location":"changelog/#highlights_9","title":"Highlights","text":"<p>This is an emergency patch release fixing a bug in 0.69.0 where a <code>kube::Client</code> would deadlock after running inside a cluster for about a minute (#829).</p> <p>All users of 0.69.0 are encouraged to upgrade immediately. 0.68.x and below are not affected.</p>"},{"location":"changelog/#whats-changed_10","title":"What's Changed","text":""},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>[0.69.x] Fix deadlock in token reloading by @clux (backported by @teozkr) in https://github.com/kube-rs/kube/pull/831</li> </ul>"},{"location":"changelog/#0690--2022-02-14","title":"0.69.0 / 2022-02-14","text":""},{"location":"changelog/#highlights_10","title":"Highlights","text":""},{"location":"changelog/#ergonomic-additions-to-api","title":"Ergonomic Additions to Api","text":"<p>Two new methods have been added to the client <code>Api</code> this release to reduce the amount of boiler-plate needed for common patterns.</p> <ul> <li><code>Api::entry</code> via 811 - to aid idempotent crud operation flows (following the style of <code>Map::Entry</code>)</li> <li><code>Api::get_opt</code> via 809 - to aid dealing with the <code>NotFound</code> type error via a returned <code>Option</code></li> </ul>"},{"location":"changelog/#in-cluster-token-reloading","title":"In-cluster Token reloading","text":"<p>Following a requirement for Kubernetes clients against versions <code>&gt;= 1.22.0</code>, our bundled <code>AuthLayer</code> will reload tokens every minute when deployed in-cluster.</p>"},{"location":"changelog/#whats-changed_11","title":"What's Changed","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Add conversion for <code>ObjectRef&lt;K&gt;</code> to <code>ObjectReference</code> by @teozkr in https://github.com/kube-rs/kube/pull/815</li> <li>Add <code>Api::get_opt</code> for better existence handling by @teozkr in https://github.com/kube-rs/kube/pull/809</li> <li>Entry API by @teozkr in https://github.com/kube-rs/kube/pull/811</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Reload token file at least once a minute by @kazk in https://github.com/kube-rs/kube/pull/768</li> <li>Prefer kubeconfig over in-cluster config by @teozkr in https://github.com/kube-rs/kube/pull/823</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Disable CSR utilities on K8s &lt;1.19 by @teozkr in https://github.com/kube-rs/kube/pull/817</li> </ul>"},{"location":"changelog/#0680--2022-02-01","title":"0.68.0 / 2022-02-01","text":""},{"location":"changelog/#interface-changes","title":"Interface Changes","text":"<p>To reduce the amount of allocation done inside the <code>runtime</code> by reflectors and controllers, the following change via #786 is needed on the signature of your <code>reconcile</code> functions:</p> <pre><code>-async fn reconcile(myobj: MyK, ctx: Context&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n+async fn reconcile(myobj: Arc&lt;MyK&gt;, ctx: Context&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n</code></pre> <p>This also affects the finalizer helper.</p>"},{"location":"changelog/#port-forwarding","title":"Port-forwarding","text":"<p>As one of the last steps toward gold level client requirements, port-forwarding landed in #446. There are 3 new examples (<code>port_forward*.rs</code>) that showcases how to use this websocket based functionality.</p>"},{"location":"changelog/#whats-changed_12","title":"What's Changed","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Add a VS Code devcontainer configuration by @olix0r in https://github.com/kube-rs/kube/pull/788</li> <li>Add support for user impersonation by @teozkr in https://github.com/kube-rs/kube/pull/797</li> <li>Add port forward by @kazk in https://github.com/kube-rs/kube/pull/446</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>runtime: Store resources in an <code>Arc</code> by @olix0r in https://github.com/kube-rs/kube/pull/786</li> <li>Propagate Arc through the finalizer reconciler helper by @teozkr in https://github.com/kube-rs/kube/pull/792</li> <li>Disable unused default features of chrono crate by @dreamer in https://github.com/kube-rs/kube/pull/801</li> </ul>"},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Use absolute path to Result in derives by @teozkr in https://github.com/kube-rs/kube/pull/795</li> <li>core: add missing reason to Display on Error::Validation in Request by @clux in https://github.com/kube-rs/kube/pull/798</li> </ul>"},{"location":"changelog/#0670--2022-01-25","title":"0.67.0 / 2022-01-25","text":""},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>runtime: Replace <code>DashMap</code> with a locked <code>AHashMap</code> by @olix0r in https://github.com/kube-rs/kube/pull/785</li> <li>update k8s-openapi for kubernetes 1.23 support by @clux in https://github.com/kube-rs/kube/pull/789</li> </ul>"},{"location":"changelog/#0660--2022-01-15","title":"0.66.0 / 2022-01-15","text":"<p>Tons of ergonomics improvements, and 3 new contributors. Highlighted first is the 3 most discussed changes:</p>"},{"location":"changelog/#support-for-auto-generating-schemas-for-enums-in-kube-derive","title":"Support for auto-generating schemas for enums in <code>kube-derive</code>","text":"<p>It is now possible to embed complex enums inside structs that use <code>#[derive(CustomResource)]</code>.</p> <p>This has been a highly requested feature since the inception of auto-generated schemas. It does not work for all cases, and has certain ergonomics caveats, but represents a huge step forwards.</p> <p>Note that if you depend on <code>kube-derive</code> directly rather than via <code>kube</code> then you must now add the <code>schema</code> feature to <code>kube-core</code></p>"},{"location":"changelog/#new-streambackoff-mechanism-in-kube-runtime","title":"New <code>StreamBackoff</code> mechanism in <code>kube-runtime</code>","text":"<p>To avoid spamming the apiserver when on certain watch errors cases, it's now possible to stream wrap the <code>watcher</code> to set backoffs. The new <code>default_backoff</code> follows existing <code>client-go</code> conventions of being kind to the apiserver.</p> <p>Initially, this is default-enabled in <code>Controller</code> watches (configurable via <code>Controller::trigger_backoff</code>) and avoids spam errors when crds are not installed.</p>"},{"location":"changelog/#new-version-priority-parser-in-kube-core","title":"New version priority parser in <code>kube-core</code>","text":"<p>To aid users picking the most appropriate version of a <code>kind</code> from api discovery or through a CRD, two new sort orders have been exposed on the new <code>kube_core::Version</code></p> <ul> <li><code>Version::priority</code> implementing kubernetes version priority</li> <li><code>Version::generation</code> implementing a more traditional; generational sort (highest version)</li> </ul>"},{"location":"changelog/#changes","title":"Changes","text":"<p>Merged PRs from github release.</p>"},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Add <code>DeleteParams</code> constructors for easily setting <code>PropagationPolicy</code> by @kate-goldenring in https://github.com/kube-rs/kube/pull/757</li> <li>Add Serialize to ObjecList and add field-selector and jsonpath example by @ChinYing-Li in https://github.com/kube-rs/kube/pull/760</li> <li>Implement cordon/uncordon for Node by @ChinYing-Li in https://github.com/kube-rs/kube/pull/762</li> <li>Export Version priority parser with Ord impls in kube_core by @clux in https://github.com/kube-rs/kube/pull/764</li> <li>Add Api fns for arbitrary subresources and approval subresource for CertificateSigningRequest by @ChinYing-Li in https://github.com/kube-rs/kube/pull/773</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>Add backoff handling for watcher and Controller by @clux in https://github.com/kube-rs/kube/pull/703</li> <li>Remove crate private <code>identity_pem</code> field from <code>Config</code> by @kazk in https://github.com/kube-rs/kube/pull/771</li> <li>Use SecretString in AuthInfo to avoid credential leaking by @ChinYing-Li in https://github.com/kube-rs/kube/pull/766</li> </ul>"},{"location":"changelog/#0650--2021-12-10","title":"0.65.0 / 2021-12-10","text":"<ul> <li>BREAKING: Removed <code>kube::Error::OpenSslError</code> - #716</li> <li>BREAKING: Removed <code>kube::Error::SslError</code> - #704 and #716</li> <li>BREAKING: Added <code>kube::Error::NativeTls(kube::client::NativeTlsError)</code> for errors from Native TLS - #716</li> <li>BREAKING: Added <code>kube::Error::RustlsTls(kube::client::RustlsTlsError)</code> for errors from Rustls TLS - #704</li> <li>Modified <code>Kubeconfig</code> parsing - allow empty kubeconfigs as per kubectl - #721</li> <li>Added <code>Kubeconfig::from_yaml</code> - #718 via #719</li> <li>Updated <code>rustls</code> to 0.20.1 - #704</li> <li>BREAKING: Added <code>ObjectRef</code> to the object that failed to be reconciled to <code>kube::runtime::controller::Error::ReconcileFailed</code> - #733</li> <li>BREAKING: Removed <code>api_version</code> and <code>kind</code> fields from <code>kind</code> structs generated by <code>kube::CustomResource</code> - #739</li> <li>Updated <code>tokio-tungstenite</code> to 0.16 - #750</li> <li>Updated <code>tower-http</code> to 0.2.0 - #748</li> <li>BREAKING: <code>kube-client</code>: replace <code>RefreshTokenLayer</code> with <code>AsyncFilterLayer</code> in <code>AuthLayer</code> - #752</li> </ul>"},{"location":"changelog/#0640--2021-11-16","title":"0.64.0 / 2021-11-16","text":"<ul> <li>BREAKING: Replaced feature <code>kube-derive/schema</code> with attribute <code>#[kube(schema)]</code> - #690</li> <li>If you currently disable default <code>kube-derive</code> default features to avoid automatic schema generation, add <code>#[kube(schema = \"disabled\")]</code> to your spec struct instead</li> <li>BREAKING: Moved <code>CustomResource</code> derive crate overrides into subattribute <code>#[kube(crates(...))]</code> - #690</li> <li>Replace <code>#[kube(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..)]</code> with <code>#[kube(crates(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..))]</code></li> <li>Added <code>openssl-tls</code> feature to use <code>openssl</code> for TLS on all platforms. Note that, even though <code>native-tls</code> uses a platform specific TLS, <code>kube</code> requires <code>openssl</code> on all platforms because <code>native-tls</code> only allows PKCS12 input to load certificates and private key at the moment, and creating PKCS12 requires <code>openssl</code>. - #700</li> <li>BREAKING: Changed to fail loading configurations with PEM-encoded certificates containing invalid sections instead of ignoring them. Updated <code>pem</code> to 1.0.1. - #702</li> <li><code>oauth</code>: Updated <code>tame-oauth</code> to 0.6.0 which supports the same default credentials flow as the Go <code>oauth2</code> for Google OAuth. In addition to reading the service account information from JSON file specified with <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, Application Default Credentials from <code>gcloud</code>, and obtaining OAuth tokens from local metadata server when running inside GCP are now supported. - #701</li> </ul>"},{"location":"changelog/#refining-errors","title":"Refining Errors","text":"<p>We started working on improving error ergonomics. See the tracking issue #688 for more details.</p> <p>The following is the summary of changes to <code>kube::Error</code> included in this release:</p> <ul> <li>Added <code>Error::Auth(kube::client::AuthError)</code> (errors related to client auth, some of them were previously in <code>Error::Kubeconfig</code>)</li> <li>Added <code>Error::BuildRequest(kube::core::request::Error)</code> (errors building request from <code>kube::core</code>)</li> <li>Added <code>Error::InferConfig(kube::config::InferConfigError)</code> (for <code>Client::try_default</code>)</li> <li>Added <code>Error::OpensslTls(kube::client::OpensslTlsError)</code> (new <code>openssl-tls</code> feature) - #700</li> <li>Added <code>Error::UpgradeConnection(kube::client::UpgradeConnectinError)</code> (<code>ws</code> feature, errors from upgrading a connection)</li> <li>Removed <code>Error::Connection</code> (was unused)</li> <li>Removed <code>Error::RequestBuild</code> (was unused)</li> <li>Removed <code>Error::RequestSend</code> (was unused)</li> <li>Removed <code>Error::RequestParse</code> (was unused)</li> <li>Removed <code>Error::InvalidUri</code> (replaced by variants of errors in <code>kube::config</code> errors)</li> <li>Removed <code>Error::RequestValidation</code> (replaced by a variant of <code>Error::BuildRequest</code>)</li> <li>Removed <code>Error::Kubeconfig</code> (replaced by <code>Error::InferConfig</code>, and <code>Error::Auth</code>)</li> <li>Removed <code>Error::ProtocolSwitch</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::MissingUpgradeWebSocketHeader</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::MissingConnectionUpgradeHeader</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::SecWebSocketAcceptKeyMismatch</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::SecWebSocketProtocolMismatch</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>impl From&lt;T&gt; for Error</code></li> </ul>  Expand for more details  The following breaking changes were made as a part of an effort to refine errors (the list is large, but most of them are lower level, and shouldn't require much change in most cases):  * Removed `impl From for kube::Error` - [#686](https://github.com/kube-rs/kube/issues/686) * Removed unused error variants in `kube::Error`: `Connection`, `RequestBuild`, `RequestSend`, `RequestParse` - [#689](https://github.com/kube-rs/kube/issues/689) * Removed unused error variant `kube::error::ConfigError::LoadConfigFile` - [#689](https://github.com/kube-rs/kube/issues/689) * Changed `kube::Error::RequestValidation(String)` to `kube::Error::BuildRequest(kube::core::request::Error)`. Includes possible errors from building an HTTP request, and contains some errors from `kube::core` that was previously grouped under `kube::Error::SerdeError` and `kube::Error::HttpError`. `kube::core::request::Error` is described below. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::core::Error` and `kube::core::Result`. `kube::core::Error` was replaced by more specific errors. - [#686](https://github.com/kube-rs/kube/issues/686)   - Replaced `kube::core::Error::InvalidGroupVersion` with `kube::core::gvk::ParseGroupVersionError`   - Changed the error returned from `kube::core::admission::AdmissionRequest::with_patch` to `kube::core::admission::SerializePatchError` (was `kube::core::Error::SerdeError`)   - Changed the error associated with `TryInto&gt;` to `kube::core::admission::ConvertAdmissionReviewError` (was `kube::core::Error::RequestValidation`)   - Changed the error returned from methods of `kube::core::Request` to `kube::core::request::Error` (was `kube::core::Error`). `kube::core::request::Error` represents possible errors when building an HTTP request. The removed `kube::core::Error` had `RequestValidation(String)`, `SerdeError(serde_json::Error)`, and `HttpError(http::Error)` variants. They are now `Validation(String)`, `SerializeBody(serde_json::Error)`, and  `BuildRequest(http::Error)` respectively in `kube::core::request::Error`. * Changed variants of error enums in `kube::runtime` to tuples. Replaced `snafu` with `thiserror`. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::error::ConfigError` and `kube::Error::Kubeconfig(ConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696)   - Error variants related to client auth were moved to a new error `kube::client::AuthError` as described below   - Remaining variants were split into `kube::config::{InferConfigError, InClusterError, KubeconfigError}` as described below * Added `kube::client::AuthError` by extracting error variants related to client auth from `kube::ConfigError` and adding more variants to preserve context - [#696](https://github.com/kube-rs/kube/issues/696) * Moved `kube::error::OAuthError` to `kube::client::OAuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Changed all errors in `kube::client::auth` to `kube::client::AuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::Error::Auth(kube::client::AuthError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InferConfigError` which is an error from `Config::infer()` and `kube::Error::InferConfig(kube::config::InferConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InClusterError` for errors related to loading in-cluster configuration by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::KubeconfigError` for errors related to loading kubeconfig by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Changed methods of `kube::Config` to return these erorrs instead of `kube::Error` - [#696](https://github.com/kube-rs/kube/issues/696) * Removed `kube::Error::InvalidUri` which was replaced by error variants preserving context, such as `KubeconfigError::ParseProxyUrl` - [#696](https://github.com/kube-rs/kube/issues/696) * Moved all errors from upgrading to a WebSocket connection into `kube::Error::UpgradeConnection(kube::client::UpgradeConnectionError)` - [#696](https://github.com/kube-rs/kube/issues/696)"},{"location":"changelog/#0632--2021-10-28","title":"0.63.2 / 2021-10-28","text":"<ul> <li><code>kube::runtime::events</code>: fix build and hide module on kubernetes &lt; 1.19 (events/v1 missing there) - #685</li> </ul>"},{"location":"changelog/#0631--2021-10-26","title":"0.63.1 / 2021-10-26","text":"<ul> <li><code>kube::runtime::wait::Condition</code> added boolean combinators (<code>not</code>/<code>and</code>/<code>or</code>) - #678</li> <li><code>kube</code>: fix docs.rs build - #681 via #682</li> </ul>"},{"location":"changelog/#0630--2021-10-26","title":"0.63.0 / 2021-10-26","text":"<ul> <li>rust <code>edition</code> bumped to <code>2021</code> - #664, #666, #667</li> <li><code>kube::CustomResource</code> derive can now take arbitrary <code>#[kube(k8s_openapi)]</code> style-paths for <code>k8s_openapi</code>, <code>schemars</code>, <code>serde</code>, and <code>serde_json</code> - #675</li> <li><code>kube</code>: fix <code>native-tls</code> included when only <code>rustls-tls</code> feature is selected - #673 via #674</li> </ul>"},{"location":"changelog/#0620--2021-10-22","title":"0.62.0 / 2021-10-22","text":"<ul> <li><code>kube</code> now re-exports <code>kube-runtime</code> under <code>runtime</code> feature - #651 via #652</li> <li>no need to keep both <code>kube</code> and <code>kube_runtime</code> in <code>Cargo.toml</code> anymore</li> <li>fixes issues with dependabot / lock-step upgrading</li> <li>change <code>kube_runtime::X</code> import paths to <code>kube::runtime::X</code> when moving to the feature</li> <li><code>kube::runtime</code> added <code>events</code> module with an event <code>Recorder</code> - #249 via #653 + #662 + #663</li> <li><code>kube::runtime::wait::conditions</code> added <code>is_crd_established</code> helper - #659</li> <li><code>kube::CustomResource</code> derive can now take an arbitrary <code>#[kube(kube_core)]</code> path for <code>kube::core</code> - #658</li> <li><code>kube::core</code> consistently re-exported across crates</li> <li>docs: major overhaul + architecture.md - #416 via #652</li> </ul>"},{"location":"changelog/#0610--2021-10-09","title":"0.61.0 / 2021-10-09","text":"<ul> <li><code>kube-core</code>: BREAKING: extend <code>CustomResourceExt</code> trait with <code>::shortnames</code> method (impl in <code>kube-derive</code>) - #641</li> <li><code>kube-runtime</code>: add <code>wait</code> module to <code>await_condition</code>, and added <code>watch_object</code> to watcher - #632 via #633</li> <li><code>kube</code>: add <code>Restart</code> marker trait to allow <code>Api::restart</code> on core workloads - #630 via #635</li> <li>bump dependencies: <code>tokio-tungstenite</code>, <code>k8s-openapi</code>, <code>schemars</code>, <code>tokio</code> in particular - #643 + #645</li> </ul>"},{"location":"changelog/#0600--2021-09-02","title":"0.60.0 / 2021-09-02","text":"<ul> <li><code>kube</code>: support <code>k8s-openapi</code> with <code>v1_22</code> features - #621 via #622</li> <li><code>kube</code>: <code>BREAKING</code>: support for <code>CustomResourceDefinition</code> at <code>v1beta1</code> now requires an opt-in <code>deprecated-crd-v1beta1</code> feature - #622</li> <li><code>kube-core</code>: add content-type header to requests with body - #626 via #627</li> </ul>"},{"location":"changelog/#0590--2021-08-09","title":"0.59.0 / 2021-08-09","text":"<ul> <li><code>BREAKING</code>: bumped <code>k8s-openapi</code> to 0.13.0 - #581 via #616</li> <li><code>kube</code> connects to kubernetes via cluster dns when using <code>rustls</code> - #587 via #597</li> <li>client now works with <code>rustls</code> feature in-cluster - #153 via #597</li> <li><code>kube</code> nicer serialization of <code>Kubeconfig</code> - #613</li> <li><code>kube-core</code> added serde traits for <code>ApiResource</code> - #590</li> <li><code>kube-core</code> added <code>CrdExtensions::crd_name</code> method (implemented by <code>kube-derive</code>) - #583</li> <li><code>kube-core</code> added the <code>HasSpec</code> and <code>HasStatus</code> traits - #605</li> <li><code>kube-derive</code> added support to automatically implement the <code>HasSpec</code> and <code>HasStatus</code> traits - #605</li> <li><code>kube-runtime</code> fix tracing span hierarchy from applier - #600</li> </ul>"},{"location":"changelog/#0581--2021-07-06","title":"0.58.1 / 2021-07-06","text":"<ul> <li><code>kube-runtime</code>: fix non-unix builds - #582</li> </ul>"},{"location":"changelog/#0580--2021-07-05","title":"0.58.0 / 2021-07-05","text":"<ul> <li><code>kube</code>: <code>BREAKING</code>: subresource marker traits renamed conjugation: <code>Log</code>, <code>Execute</code>, <code>Attach</code>, <code>Evict</code> (previously <code>Logging</code>, <code>Executable</code>, <code>Attachable</code>, <code>Evictable</code>) - #536 via #560</li> <li><code>kube-derive</code> added <code>#[kube(category)]</code> attr to set CRD categories - #559</li> <li><code>kube-runtime</code> added <code>finalizer</code> helper #291 via #475</li> <li><code>kube-runtime</code> added tracing for why reconciliations happened #457 via #571</li> <li><code>kube-runtime</code> added <code>Controller::reconcile_all_on</code> to allow scheduling all objects for reconciliation #551 via #555</li> <li><code>kube-runtime</code> added <code>Controller::graceful_shutdown_on</code> for shutting down the <code>Controller</code> while waiting for running reconciliations to finish - #552 via #573</li> <li>BREAKING: <code>controller::applier</code> now starts a graceful shutdown when the <code>queue</code> terminates</li> <li>BREAKING: <code>scheduler</code> now shuts down immediately when <code>requests</code> terminates, rather than waiting for the pending reconciliations to drain</li> <li><code>kube-runtime</code> added tracking for reconciliation reason</li> <li>Added: <code>Controller::owns_with</code> and <code>Controller::watches_with</code> to pass a <code>dyntype</code> argument for dynamic <code>Api</code>s - #575</li> <li>BREAKING: <code>Controller::owns</code> signature changed to not allow <code>DynamicType</code>s</li> <li>BREAKING: <code>controller::trigger_*</code> now returns a <code>ReconcileRequest</code> rather than <code>ObjectRef</code>. The <code>ObjectRef</code> can be accessed via the <code>obj_ref</code> field</li> </ul>"},{"location":"changelog/#known-issues","title":"Known Issues","text":"<ul> <li>Api::replace can fail to unset list values with k8s-openapi 0.12 #581</li> </ul>"},{"location":"changelog/#0570--2021-06-16","title":"0.57.0 / 2021-06-16","text":"<ul> <li><code>kube</code>: custom clients now respect default namespaces - fixes #534 via #544</li> <li>BREAKING: custom clients via <code>Client::new</code> must pass <code>config.default_namespace</code> as 2nd arg</li> <li><code>kube</code>: Added <code>CustomResourceExt</code> trait for <code>kube-derive</code> - #497 via #545</li> <li>BREAKING: <code>kube-derive</code> users must import <code>kube::CustomResourceExt</code> (or <code>kube::core::crd::v1beta1::CustomResourceExt</code> if using legacy <code>#[kube(apiextensions = \"v1beta1\")]</code>) to use generated methods <code>Foo::crd</code> or <code>Foo::api_resource</code></li> <li>BREAKING: <code>k8s_openapi</code> bumped to 0.12.0 - #531<ul> <li>Generated structs simplified + <code>Resource</code> trait expanded</li> <li>Adds support for kubernetes <code>v1_21</code></li> <li>Contains bugfix for kubernetes#102159</li> </ul> </li> <li><code>kube</code> resource plurals is no longer inferred from <code>k8s-openapi</code> structs - #284 via #556</li> <li>BREAKING: <code>kube::Resource</code> trait now requires a <code>plural</code> implementation</li> </ul>"},{"location":"changelog/#known-issues_1","title":"Known Issues","text":"<ul> <li>Api::replace can fail to unset list values with k8s-openapi 0.12 #581</li> </ul>"},{"location":"changelog/#0560--2021-06-05","title":"0.56.0 / 2021-06-05","text":"<ul> <li><code>kube</code>: added <code>Api::default_namespaced</code> - #209 via #534</li> <li><code>kube</code>: added <code>config</code> feature - #533 via #535</li> <li><code>kube</code>: BREAKING: moved <code>client::discovery</code> module to <code>kube::discovery</code> and rewritten module #538</li> <li><code>discovery</code>: added <code>oneshot</code> helpers for quick selection of recommended resources / kinds #538</li> <li><code>discovery</code>: moved <code>ApiResource</code> and <code>ApiCapabilities</code> (result of discovery) to <code>kube_core::discovery</code></li> <li> <p>BREAKING: removed internal <code>ApiResource::from_apiresource</code></p> </li> <li> <p><code>kube::Client</code> is now configurable with layers using <code>tower-http</code> #539 via #540</p> </li> <li>three new examples added: <code>custom_client</code>, <code>custom_client_tls</code> and <code>custom_client_trace</code></li> <li>Big feature streamlining, big service and layer restructuring, dependency restructurings</li> <li>Changes can hit advanced users, but unlikely to hit base use cases with <code>Api</code> and <code>Client</code>.</li> <li>In depth changes broken down below:</li> </ul>"},{"location":"changelog/#tls-enhancements","title":"TLS Enhancements","text":"<ul> <li>Add <code>kube::client::ConfigExt</code> extending <code>Config</code> for custom <code>Client</code>. This includes methods to configure TLS connection when building a custom client #539</li> <li><code>native-tls</code>: <code>Config::native_tls_https_connector</code> and <code>Config::native_tls_connector</code></li> <li><code>rustls-tls</code>: <code>Config::rustls_https_connector</code> and <code>Config::rustls_client_config</code></li> <li>Remove the requirement of having <code>native-tls</code> or <code>rustls-tls</code> enabled when <code>client</code> is enabled. Allow one, both or none.</li> <li>When both, the default Service will use <code>native-tls</code> because of #153. <code>rustls</code> can be still used with a custom client. Users will have an option to configure TLS at runtime.</li> <li>When none, HTTP connector is used.</li> <li>Remove TLS features from <code>kube-runtime</code></li> <li>BREAKING: Features must be removed if specified</li> <li>Remove <code>client</code> feature from <code>native-tls</code> and <code>rust-tls</code> features</li> <li><code>config</code> + <code>native-tls</code>/<code>rustls-tls</code> can be used independently, e.g., to create a simple HTTP client</li> <li>BREAKING: <code>client</code> feature must be added if <code>default-features = false</code></li> </ul>"},{"location":"changelog/#layers","title":"Layers","text":"<ul> <li><code>ConfigExt::base_uri_layer</code> (<code>BaseUriLayer</code>) to set cluster URL (#539)</li> <li><code>ConfigExt::auth_layer</code> that returns optional layer to manage <code>Authorization</code> header (#539)</li> <li><code>gzip</code>: Replaced custom decompression module with <code>DecompressionLayer</code> from <code>tower-http</code> (#539)</li> <li>Replaced custom <code>LogRequest</code> with <code>TraceLayer</code> from <code>tower-http</code> (#539)</li> <li>Request body is no longer shown</li> <li>Basic and Bearer authentication using <code>AddAuthorizationLayer</code> (borrowing from https://github.com/tower-rs/tower-http/pull/95 until released)</li> <li>BREAKING: Remove <code>headers</code> from <code>Config</code>. Injecting arbitrary headers is now done with a layer on a custom client.</li> </ul>"},{"location":"changelog/#dependency-changes","title":"Dependency Changes","text":"<ul> <li>Remove <code>static_assertions</code> since it's no longer used</li> <li>Replace <code>tokio_rustls</code> with <code>rustls</code> and <code>webpki</code> since we're not using <code>tokio_rustls</code> directly</li> <li>Replace uses of <code>rustls::internal::pemfile</code> with <code>rustls-pemfile</code></li> <li>Remove <code>url</code> and always use <code>http::Uri</code></li> <li>BREAKING: <code>Config::cluster_url</code> is now <code>http::Uri</code></li> <li>BREAKING: <code>Error::InternalUrlError(url::ParseError)</code> and <code>Error::MalformedUrl(url::ParseError)</code> replaced by <code>Error::InvalidUri(http::uri::InvalidUri)</code></li> </ul>"},{"location":"changelog/#0550--2021-05-21","title":"0.55.0 / 2021-05-21","text":"<ul> <li><code>kube</code>: <code>client</code> feature added (default-enabled) - #528</li> <li><code>kube</code>: <code>PatchParams</code> force now only works with <code>Patch::Apply</code> #528</li> <li><code>kube</code>: <code>api</code> <code>discovery</code> module now uses a new <code>ApiResource</code> struct #495 + #482</li> <li><code>kube</code>: <code>api</code> BREAKING: <code>DynamicObject</code> + <code>Object</code> now takes an <code>ApiResource</code> rather than a <code>GroupVersionKind</code></li> <li><code>kube</code>: <code>api</code> BREAKING: <code>discovery</code> module's <code>Group</code> renamed to <code>ApiGroup</code></li> <li><code>kube</code>: <code>client</code> BREAKING: <code>kube::client::Status</code> moved to <code>kube::core::Status</code> (accidental, re-adding in 0.56)</li> <li><code>kube-core</code> crate factored out of <code>kube</code> to reduce dependencies - #516 via #517 + #519 + #522 + #528 + #530</li> <li><code>kube</code>: <code>kube::Service</code> removed to allow <code>kube::Client</code> to take an abritrary <code>Service&lt;http::Request&lt;hyper::Body&gt;&gt;</code> - #532</li> </ul>"},{"location":"changelog/#0540--2021-05-19","title":"0.54.0 / 2021-05-19","text":"<ul> <li>yanked 30 minutes after release due to #525</li> <li>changes lifted to 0.55.0</li> </ul>"},{"location":"changelog/#0530--2021-05-15","title":"0.53.0 / 2021-05-15","text":"<ul> <li><code>kube</code>: <code>admission</code> controller module added under feature - #477 via #484 + fixes in #488 #498 #499 + #507 + #509</li> <li><code>kube</code>: <code>config</code> parsing of pem blobs now resilient against missing newlines - #504 via #505</li> <li><code>kube</code>: <code>discovery</code> module added to simplify dynamic api usage - #491</li> <li><code>kube</code>: <code>api</code> BREAKING: <code>DynamicObject::namespace</code> renamed to <code>::within</code> - #502</li> <li><code>kube</code>: <code>api</code> BREAKING: added <code>ResourceExt</code> trait moving the getters from <code>Resource</code> trait - #486</li> <li><code>kube</code>: <code>api</code> added a generic interface for subresources via <code>Request</code> - #487</li> <li><code>kube</code>: <code>api</code> fix bug in <code>PatchParams::dry_run</code> not being serialized correctly - #511</li> </ul>"},{"location":"changelog/#0530-migration-guide","title":"0.53.0 Migration Guide","text":"<p>The most likely issue you'll run into is from <code>kube</code> when using <code>Resource</code> trait which has been split:</p> <pre><code>+use kube::api::ResouceExt;\n-    let name = Resource::name(&amp;foo);\n-    let ns = Resource::namespace(&amp;foo).expect(\"foo is namespaced\");\n+    let name = ResourceExt::name(&amp;foo);\n+    let ns = ResourceExt::namespace(&amp;foo).expect(\"foo is namespaced\");\n</code></pre>"},{"location":"changelog/#0520--2021-03-31","title":"0.52.0 / 2021-03-31","text":"<ul> <li><code>kube-derive</code>: allow overriding <code>#[kube(plural)]</code> and <code>#[kube(singular)]</code> - #458 via #463</li> <li><code>kube</code>: added tracing instrumentation for io operations in <code>kube::Api</code> - #455</li> <li><code>kube</code>: <code>DeleteParams</code>'s <code>Preconditions</code> is now public - #459 via #460</li> <li><code>kube</code>: remove dependency on duplicate <code>derive_accept_key</code> for <code>ws</code> - #452</li> <li><code>kube</code>: Properly verify websocket keys in <code>ws</code> handshake - #447</li> <li><code>kube</code>: BREAKING: removed optional, and deprecated <code>runtime</code> module - #454</li> <li><code>kube</code>: BREAKING: <code>ListParams</code> bookmarks default enabled - #226 via #445</li> <li>renames member <code>::allow_bookmarks</code> to <code>::bookmarks</code></li> <li><code>::default()</code> sets <code>bookmark</code> to <code>true</code> to avoid bad bad defaults #219</li> <li>method <code>::allow_bookmarks()</code> replaced by <code>::disable_bookmarks()</code></li> <li><code>kube</code>: <code>DynamicObject</code> and <code>GroupVersionKind</code> introduced for full dynamic object support</li> <li><code>kube-runtime</code>: watchers/reflectors/controllers can be used with dynamic objects from api discovery</li> <li><code>kube</code>: Pluralisation now only happens for <code>k8s_openapi</code> objects by default #481</li> <li>inflector dependency removed #471</li> <li>added internal pluralisation helper for <code>k8s_openapi</code> objects</li> <li><code>kube</code>: BREAKING: Restructuring of low level <code>Resource</code> request builder #474</li> <li><code>Resource</code> renamed to <code>Request</code> and requires only a <code>path_url</code> to construct</li> <li><code>kube</code>: BREAKING: Mostly internal <code>Meta</code> trait revamped to support dynamic types</li> <li><code>Meta</code> renamed to <code>kube::Resource</code> to mimic <code>k8s_openapi::Resource</code> #478</li> <li>The trait now takes an optional associated type for runtime type info: <code>DynamicType</code> #385</li> <li><code>Api::all_with</code> + <code>Api::namespaced_with</code> added for querying with dynamic families</li> <li>see <code>dynamic_watcher</code> + <code>dynamic_api</code> for example usage</li> <li><code>kube-runtime</code>: BREAKING: lower level interface changes as a result of <code>kube::api::Meta</code> trait:</li> <li>THESE SHOULD NOT AFFECT YOU UNLESS YOU ARE IMPLEMENTING / CUSTOMISING LOW LEVEL TYPES DIRECTLY</li> <li><code>ObjectRef</code> now generic over <code>kube::Resource</code> rather than <code>RuntimeResource</code></li> <li><code>reflector::{Writer, Store}</code> takes a <code>kube::Resource</code> rather than a <code>k8s_openapi::Resource</code></li> <li><code>kube-derive</code>: BREAKING: Generated type no longer generates <code>k8s-openapi</code> traits</li> <li>This allows correct pluralisation via <code>#[kube(plural = \"mycustomplurals\")]</code> #467 via #481</li> </ul>"},{"location":"changelog/#0520-migration-guide","title":"0.52.0 Migration Guide","text":"<p>While we had a few breaking changes. Most are to low level internal interfaces and should not change much, but some changes you might need to make:</p>"},{"location":"changelog/#kube","title":"kube","text":"<ul> <li>if using the old, low-level <code>kube::api::Resource</code>, please consider the easier <code>kube::Api</code>, or look at tests in <code>request.rs</code> or <code>typed.rs</code> if you need the low level interface</li> <li>search replace <code>kube::api::Meta</code> with <code>kube::Resource</code> if used - trait was renamed</li> <li>if implementing the trait, add <code>type DynamicType = ();</code> to the impl</li> <li>remove calls to <code>ListParams::allow_bookmarks</code> (allow default)</li> <li>handle <code>WatchEvent::Bookmark</code> or set <code>ListParams::disable_bookmarks()</code></li> <li>look at examples if replacing the long deprecated legacy runtime</li> </ul>"},{"location":"changelog/#kube-derive","title":"kube-derive","text":"<p>The following constants from <code>k8s_openapi::Resource</code> no longer exist. Please <code>use kube::Resource</code> and: - replace <code>Foo::KIND</code> with <code>Foo::kind(&amp;())</code> - replace <code>Foo::GROUP</code> with <code>Foo::group(&amp;())</code> - replace <code>Foo::VERSION</code> with <code>Foo::version(&amp;())</code> - replace <code>Foo::API_VERSION</code> with <code>Foo::api_version(&amp;())</code></p>"},{"location":"changelog/#0510--2021-02-28","title":"0.51.0 / 2021-02-28","text":"<ul> <li><code>kube</code> <code>Config</code> now allows arbirary extension objects - #425</li> <li><code>kube</code> <code>Config</code> now allows multiple yaml documents per kubeconfig - #440 via #441</li> <li><code>kube-derive</code> now more robust and is using <code>darling</code> - #435</li> <li>docs improvements to patch + runtime</li> </ul>"},{"location":"changelog/#0501--2021-02-17","title":"0.50.1 / 2021-02-17","text":"<ul> <li>bug: fix oidc auth provider - #424 via #419</li> </ul>"},{"location":"changelog/#0500--2021-02-10","title":"0.50.0 / 2021-02-10","text":"<ul> <li>feat: added support for stacked kubeconfigs - #132 via #411</li> <li>refactor: authentication logic moved out of <code>kube::config</code> and into into <code>kube::service</code> - #409</li> <li>BREAKING: <code>Config::get_auth_header</code> removed</li> <li>refactor: remove <code>hyper</code> dependency from <code>kube::api</code> - #410</li> <li>refactor: <code>kube::Service</code> simpler auth and gzip handling - #405 + #408</li> </ul>"},{"location":"changelog/#0490--2021-02-08","title":"0.49.0 / 2021-02-08","text":"<ul> <li>dependency on <code>reqwest</code> + removed in favour of <code>hyper</code> + <code>tower</code> #394</li> <li>refactor: <code>kube::Client</code> now uses <code>kube::Service</code> (a <code>tower::Service&lt;http::Request&lt;hyper::Body&gt;&gt;</code>) instead of <code>reqwest::Client</code> to handle all requests</li> <li>refactor: <code>kube::Client</code> now uses a <code>tokio_util::codec</code> for internal buffering</li> <li>refactor: <code>async-tungstenite</code> ws feature dependency replaced with <code>tokio-tungstenite</code>. <code>WebSocketStream</code> is now created from a connection upgraded with <code>hyper</code></li> <li>refactor: <code>oauth2</code> module for GCP OAuth replaced with optional <code>tame-oauth</code> dependency</li> <li>BREAKING: GCP OAuth is now opt-in (<code>oauth</code> feature). Note that GCP provider with command based token source is supported by default.</li> <li>BREAKING: Gzip decompression is now opt-in (<code>gzip</code> feature) because Kubernetes does not have compression enabled by default yet and this feature requires extra dependencies. #399</li> <li>BREAKING: <code>Client::new</code> now takes a <code>Service</code> instead of <code>Config</code> #400. Allows custom service for features not supported out of the box and testing. To create a <code>Client</code> from <code>Config</code>, use <code>Client::try_from</code> instead.</li> <li>BREAKING: Removed <code>Config::proxy</code>. Proxy is no longer supported out of the box, but it should be possible by using a custom Service.</li> <li>fix: Refreshable token from auth provider not refreshing</li> <li>fix: Panic when loading config with non-GCP provider #238</li> <li>feat: subresource support added for <code>Evictable</code> types (marked for <code>Pod</code>) - #393</li> <li><code>kube</code>: subresource marker traits renamed to <code>Loggable</code>, <code>Executable</code>, <code>Attachable</code> (previously <code>LoggingObject</code>, <code>ExecutingObject</code>, <code>AttachableObject</code>) - #395</li> <li><code>examples</code> showcasing <code>kubectl cp</code> like behaviour #381 via #392</li> </ul>"},{"location":"changelog/#0480--2021-01-23","title":"0.48.0 / 2021-01-23","text":"<ul> <li>bump <code>k8s-openapi</code> to <code>0.11.0</code> - #388</li> <li>breaking: <code>kube</code>: no longer necessary to serialize patches yourself - #386<ul> <li><code>PatchParams</code> removes <code>PatchStrategy</code></li> <li><code>Api::patch*</code> methods now take an enum <code>Patch</code> type</li> <li>optional <code>jsonpatch</code> feature added for <code>Patch::Json</code></li> </ul> </li> </ul>"},{"location":"changelog/#0470--2021-01-06","title":"0.47.0 / 2021-01-06","text":"<ul> <li>chore: upgrade <code>tokio</code> to <code>1.0</code> - #363<ul> <li>BREAKING: This requires the whole application to upgrade to <code>tokio</code> 1.0 and <code>reqwest</code> to 0.11.0</li> </ul> </li> <li>docs: fix broken documentation in <code>kube</code> 0.46.0 #367</li> <li>bug: <code>kube</code>: removed panics from <code>ws</code> features, fix <code>rustls</code> support + improve docs #369 via #370 + #373</li> <li>bug: <code>AttachParams</code> now fixes owned method chaining (slightly breaks from 0.46 if using &amp;mut ref before) - #364</li> <li>feat: <code>AttachParams::interactive_tty</code> convenience method added - #364</li> <li>bug: fix <code>Runner</code> (and thus <code>Controller</code> and <code>applier</code>) not waking correctly when starting new tasks - #375</li> </ul>"},{"location":"changelog/#0461--2021-01-06","title":"0.46.1 / 2021-01-06","text":"<ul> <li>maintenance release for 0.46 (last supported tokio 0.2 release) from <code>tokio02</code> branch</li> <li>bug backport: fix <code>Runner</code> (and thus <code>Controller</code> and <code>applier</code>) not waking correctly when starting new tasks - #375</li> </ul>"},{"location":"changelog/#0460--2021-01-02","title":"0.46.0 / 2021-01-02","text":"<ul> <li>feat: <code>kube</code> now has optional websocket support with <code>async_tungstenite</code> under <code>ws</code> and <code>ws-*-tls</code> features #360</li> <li>feat: <code>AttachableObject</code> marker trait added and implemented for <code>k8s_openapi::api::core::v1::Pod</code> #360</li> <li>feat: <code>AttachParams</code> added for <code>Api::exec</code> and <code>Api::attach</code> for <code>AttachableObject</code>s #360</li> <li>examples: <code>pod_shell</code>, <code>pod_attach</code>, <code>pod_exec</code> demonstrating the new features #360</li> </ul>"},{"location":"changelog/#0450--2020-12-26","title":"0.45.0 / 2020-12-26","text":"<ul> <li>feat: <code>kube-derive</code> now has a default enabled <code>schema</code> feature<ul> <li>allows opting out of <code>schemars</code> dependency for handwriting crds - #355</li> </ul> </li> <li>breaking: <code>kube-derive</code> attr <code>struct_name</code> renamed to <code>struct</code> - #359</li> <li>docs: improvements on <code>kube</code>, <code>kube-runtime</code>, <code>kube-derive</code></li> </ul>"},{"location":"changelog/#0440--2020-12-23","title":"0.44.0 / 2020-12-23","text":"<ul> <li>feat: <code>kube-derive</code> now generates openapi v3 schemas and is thus usable with v1 <code>CustomResourceDefinition</code> - #129 and #264 via #348<ul> <li>BREAKING: <code>kube-derive</code> types now require <code>JsonSchema</code> derived via <code>schemars</code> libray (not breaking if going to 0.45.0)</li> </ul> </li> <li>feat: <code>kube_runtime::controller</code>: now reconciles objects in parallel - #346<ul> <li>BREAKING: <code>kube_runtime::controller::applier</code> now requires that the <code>reconciler</code>'s <code>Future</code> is <code>Unpin</code>, <code>Box::pin</code> it or submit it to a runtime if this is not acceptable</li> <li>BREAKING: <code>kube_runtime::controller::Controller</code> now requires that the <code>reconciler</code>'s <code>Future</code> is <code>Send + 'static</code>,             use the low-level <code>applier</code> interface instead if this is not acceptable</li> </ul> </li> <li>bug: <code>kube-runtime</code>: removed accidentally included <code>k8s-openapi</code> default features (you have to opt in to them yourself)</li> <li>feat: <code>kube</code>: <code>TypeMeta</code> now derives additionally <code>Debug, Eq, PartialEq, Hash</code></li> <li>bump: <code>k8s-openapi</code> to <code>0.10.0</code> - #330</li> <li>bump: <code>serde_yaml</code> - #349</li> <li>bump: <code>dirs</code> to <code>dirs-next</code> - #340</li> </ul>"},{"location":"changelog/#0430--2020-10-08","title":"0.43.0 / 2020-10-08","text":"<ul> <li>bug: <code>kube-derive</code> attr <code>#[kube(shortname)]</code> now working correctly</li> <li>bug: <code>kube-derive</code> now working with badly cased existing types - #313</li> <li>missing: <code>kube</code> now correctly exports <code>config::NamedAuthInfo</code> - #323</li> <li>feat: <code>kube</code>: expose <code>Config::get_auth_header</code> for istio use cases - #322</li> <li>feat: <code>kube</code>: local config now tackles gcloud auth exec params - #328 and #84</li> <li><code>kube-derive</code> now actually requires GVK (in particular <code>#[kube(kind = \"Foo\")]</code> which we sometimes inferred earlier, despite documenting the contrary)</li> </ul>"},{"location":"changelog/#0420--2020-09-10","title":"0.42.0 / 2020-09-10","text":"<ul> <li>bug: <code>kube-derive</code>'s <code>Default</code> derive now sets typemeta correctly - #315</li> <li>feat: <code>ListParams</code> now supports <code>continue_token</code> and <code>limit</code> - #320</li> </ul>"},{"location":"changelog/#0410--2020-09-10","title":"0.41.0 / 2020-09-10","text":"<ul> <li>yanked release. failed publish.</li> </ul>"},{"location":"changelog/#0400--2020-08-17","title":"0.40.0 / 2020-08-17","text":"<ul> <li><code>DynamicResource::from_api_resource</code> added to allow apiserver returned resources - #305 via #301</li> <li><code>Client::list_api_groups</code> added</li> <li><code>Client::list_ap_group_resources</code> added</li> <li><code>Client::list_core_api_versions</code> added</li> <li><code>Client::list_core_api_resources</code> added</li> <li><code>kube::DynamicResource</code> exposed at top level</li> <li>Bug: <code>PatchParams::default_apply()</code> now requires a manager and renamed to <code>PatchParams::apply(manager: &amp;str)</code> for #300</li> <li>Bug: <code>DeleteParams</code> no longer missing for <code>Api::delete_collection</code> - #53</li> <li>Removed paramter <code>ListParams::include_uninitialized</code> deprecated since 1.14</li> <li>Added optional <code>PostParams::field_manager</code> was missing for <code>Api::create</code> case</li> </ul>"},{"location":"changelog/#0390--2020-08-05","title":"0.39.0 / 2020-08-05","text":"<ul> <li>Bug: <code>ObjectRef</code> tweak in <code>kube-runtime</code> to allow controllers triggering across cluster and namespace scopes - #293 via #294</li> <li>Feature: <code>kube</code> now has a <code>derive</code> feature which will re-export <code>kube::CustomResource</code> from <code>kube-derive::CustomResource</code>.</li> <li>Examples: revamp examples for <code>kube-runtime</code> - #201</li> </ul>"},{"location":"changelog/#0380--2020-07-23","title":"0.38.0 / 2020-07-23","text":"<ul> <li>Marked <code>kube::runtime</code> module as deprecated - #281</li> <li><code>Config::timeout</code> can now be overridden to <code>None</code> (with caveats) #280</li> <li>Bug: reflector stores could have multiple copies inside datastore - #286<ul> <li><code>dashmap</code> backend Store driver downgraded - #286</li> <li><code>Store::iter</code> temporarily removed</li> </ul> </li> <li>Bug: Specialize WatchEvent::Bookmark so they can be deserialized - #285</li> <li>Docs: Tons of docs for kube-runtime</li> </ul>"},{"location":"changelog/#0370--2020-07-20","title":"0.37.0 / 2020-07-20","text":"<ul> <li>Bump <code>k8s-openapi</code> to <code>0.9.0</code></li> <li>All runtime components now require <code>Sync</code> objects</li> <li>reflector/watcher/Controller streams can be shared in threaded environments</li> </ul>"},{"location":"changelog/#0360--2020-07-19","title":"0.36.0 / 2020-07-19","text":"<ul> <li>https://gitlab.com/teozkr/kube-rt/ merged in for a new <code>kube-runtime</code> crate #258</li> <li><code>Controller&lt;K&gt;</code> added (#148 via #258)</li> <li><code>Reflector</code> api redesigned (#102 via #258)</li> <li>Migration release for <code>Informer</code> -&gt; <code>watcher</code> + <code>Reflector</code> -&gt; <code>reflector</code></li> <li><code>kube::api::CustomResource</code> removed in favour of <code>kube::api::Resource::dynamic</code></li> <li><code>CrBuilder</code> removed in favour of <code>DynamicResource</code> (with new error handling)</li> <li>support level bumped to beta</li> </ul>"},{"location":"changelog/#0351--2020-06-18","title":"0.35.1 / 2020-06-18","text":"<ul> <li>Fix in-cluster Client when using having multiple certs in the chain - #251</li> </ul>"},{"location":"changelog/#0350--2020-06-15","title":"0.35.0 / 2020-06-15","text":"<ul> <li><code>Config::proxy</code> support added - #246</li> <li><code>PartialEq</code> can be derived with <code>kube-derive</code> - #242</li> <li>Windows builds no longer clashes with runtime - #240</li> <li>Rancher hosts (with path specifiers) now works - #244</li> </ul>"},{"location":"changelog/#0340--2020-05-08","title":"0.34.0 / 2020-05-08","text":"<ul> <li>Bump <code>k8s-openapi</code> to <code>0.8.0</code></li> <li><code>Config::from_cluster_env</code> &lt;- renamed from <code>Config::new_from_cluster_env</code></li> <li><code>Config::from_kubeconfig</code> &lt;- renamed from <code>Config::new_from_kubeconfig</code></li> <li><code>Config::from_custom_kubeconfig</code> added - #236</li> <li>Majorly overhauled error handlind in config module - #237</li> </ul>"},{"location":"changelog/#0330--2020-04-27","title":"0.33.0 / 2020-04-27","text":"<ul> <li>documentation fixes for <code>Api::patch</code></li> <li>Config: add automatic token refresh - #72 / #224 / #234</li> </ul>"},{"location":"changelog/#0321--2020-04-15","title":"0.32.1 / 2020-04-15","text":"<ul> <li>add missing tokio <code>signal</code> feature as a dependency</li> <li>upgrade all dependencies, including minor bumps to rustls and base64</li> </ul>"},{"location":"changelog/#0320--2020-04-10","title":"0.32.0 / 2020-04-10","text":"<ul> <li>Major <code>config</code> + <code>client</code> module refactor</li> <li><code>Config</code> is the new <code>Configuration</code> struct</li> <li><code>Client</code> is now just a configured <code>reqwest::Client</code> plus a <code>reqwest::Url</code></li> <li>implement <code>From&lt;Config&gt; for reqwest::ClientBuilder</code></li> <li>implement <code>TryFrom&lt;Config&gt; for Client</code></li> <li><code>Client::try_default</code> or <code>Client::new</code> now recommended constructors</li> <li>People parsing <code>~/.kube/config</code> must use the <code>KubeConfig</code> struct instead</li> <li><code>Reflector&lt;K&gt;</code> now only takes an <code>Api&lt;K&gt;</code> to construct (.params method)</li> <li><code>Informer&lt;K&gt;</code> now only takes an <code>Api&lt;K&gt;</code> to construct (.params method)</li> <li><code>Informer::init_from</code> -&gt; <code>Informer::set_version</code></li> <li><code>Reflector</code> now self-polls #151 + handles signals #152</li> <li><code>Reflector::poll</code> made private in favour of <code>Reflector::run</code></li> <li><code>Api::watch</code> no longer filters out error events (<code>next</code> -&gt; <code>try_next</code>)</li> <li><code>Api::watch</code> returns <code>Result&lt;WatchEvent&gt;</code> rather than <code>WatchEvent</code></li> <li><code>WatchEvent::Bookmark</code> added to enum</li> <li><code>ListParams::allow_bookmarks</code> added</li> <li><code>PatchParams::default_apply</code> ctor added</li> <li><code>PatchParams</code> builder mutators: <code>::force</code> and <code>::dry_run</code> added</li> </ul>"},{"location":"changelog/#0310--2020-03-27","title":"0.31.0 / 2020-03-27","text":"<ul> <li>Expose <code>config::Configuration</code> at root level</li> <li>Add <code>Configuration::infer</code> as a recommended constructor</li> <li>Rename <code>client::APIClient</code> to <code>client::Client</code></li> <li>Expose <code>client::Client</code> at root level</li> <li><code>Client</code> now implements <code>From&lt;Configuration&gt;</code></li> <li>Added comprehensive documentation on <code>Api</code></li> <li>Rename <code>config::KubeConfigLoader</code> -&gt; <code>config::ConfigLoader</code></li> <li>removed <code>futures-timer</code> dependency for <code>tokio</code> (feature=timer)</li> </ul>"},{"location":"changelog/#0300--2020-03-17","title":"0.30.0 / 2020-03-17","text":"<ul> <li>Fix <code>#[kube(printcolumn)]</code> when <code>#[kube(apiextensions = \"v1beta1\")]</code></li> <li>Fix <code>#[kube(status)]</code> causing serializes of empty optional statuses</li> </ul>"},{"location":"changelog/#0290--2020-03-12","title":"0.29.0 / 2020-03-12","text":"<ul> <li><code>Api::log</code> -&gt; <code>Api::logs</code> (now matches <code>Resource::logs</code>)</li> <li><code>Object&lt;FooSpec, FooStatus&gt;</code> back for ad-hoc ser/de</li> <li>kube-derive now derives <code>Debug</code> (requires <code>Debug</code> on spec struct)</li> <li>kube-derive now allows multiple derives per file</li> <li><code>Api::create</code> now takes data <code>K</code> rather than bytes</li> <li><code>Api::replace</code> now takes data <code>K</code> rather than bytes<ul> <li>(note that <code>Resource::create</code> and <code>Resource::replace</code> still takes bytes)</li> </ul> </li> </ul>"},{"location":"changelog/#0281--2020-03-07","title":"0.28.1 / 2020-03-07","text":"<ul> <li><code>#[derive(CustomResource)]</code> now implements <code>::new</code> on the generated <code>Kind</code></li> <li>derived <code>Kind</code> now properly contains <code>TypeMeta</code> - #170</li> </ul>"},{"location":"changelog/#0280--2020-03-05","title":"0.28.0 / 2020-03-05","text":"<ul> <li><code>RawApi</code> removed -&gt; <code>Resource</code> added</li> <li><code>Resource</code> implements <code>k8s_openapi::Resource</code></li> <li>ALL OBJECTS REMOVED -&gt; Depening on light version of <code>k8s-openapi</code> now<ul> <li>NB: should generally just mean a few import changes (+casings / unwraps)</li> </ul> </li> <li><code>openapi</code> feature removed (light dependency mandatory now)</li> <li>LIBRARY WORKS WITH ALL <code>k8s_openapi</code> KUBERNETES OBJECTS</li> <li><code>KubeObject</code> trait removed in favour of <code>Meta</code> trait</li> <li><code>Object&lt;FooSpec, FooStatus&gt;</code> removed -&gt; types implementing <code>k8s_openapi::Resource</code> required instead</li> <li><code>kube-derive</code> crate added to derive this trait + other kubebuilder like codegen</li> </ul>"},{"location":"changelog/#0270--2020-02-26","title":"0.27.0 / 2020-02-26","text":"<ul> <li><code>Reflector</code> + <code>Informer</code> moved from <code>kube::api</code> to <code>kube::runtime</code></li> <li><code>Informer</code> now resets the version to 0 rather than dropping events - #134</li> <li>Removed <code>Informer::init</code>, since it is now a no-op when building the <code>Informer</code></li> <li>Downgrade spurious log message when using service account auth</li> </ul>"},{"location":"changelog/#0260--2020-02-25","title":"0.26.0 / 2020-02-25","text":"<ul> <li>Fix a large percentage of EOFs from watches #146</li> <li>=&gt; default timeout down to 290s from 300s</li> <li>=&gt; <code>Reflector</code> now re-lists a lot less #146</li> <li>Fix decoder panic with async-compression (probably) #144</li> <li><code>Informer::poll</code> can now be used with <code>TryStream</code></li> <li>Exposed <code>Config::read</code> and <code>Config::read_from</code> - #124</li> <li>Fix typo on <code>Api::StatefulSet</code></li> <li>Fix typo on <code>Api::Endpoints</code></li> <li>Add <code>Api::v1CustomResourceDefinition</code> when on k8s &gt;= 1.17</li> <li>Renamed <code>Void</code> to <code>NotUsed</code></li> </ul>"},{"location":"changelog/#0250--2020-02-09","title":"0.25.0 / 2020-02-09","text":"<ul> <li>initial rustls support #114 (some local kube config issues know #120)</li> <li>crate does better version checking against openapi features - #106</li> <li>initial <code>log_stream</code> support - #109</li> </ul>"},{"location":"changelog/#0240--2020-01-26","title":"0.24.0 / 2020-01-26","text":"<ul> <li>Add support for ServiceAccount, Role, ClusterRole, RoleBinding, Endpoint - #113 + #111</li> <li>Upgrade k8s-openapi to 0.7 =&gt; breaking changes: https://github.com/Arnavion/k8s-openapi/blob/master/CHANGELOG.md#v070-2020-01-23</li> </ul>"},{"location":"changelog/#0230--2019-12-31","title":"0.23.0 / 2019-12-31","text":"<ul> <li>Bump tokio and reqwest to 0.2 and 0.10</li> <li>Fix bug in <code>log</code> fetcher - #107</li> <li>Temporarily allow invalid certs when testing on macosx - #105</li> </ul>"},{"location":"changelog/#0222--2019-12-04","title":"0.22.2 / 2019-12-04","text":"<ul> <li>Allow sharing Reflectors between threads - #97</li> <li>Fix Reflector pararall lock issue (<code>poll</code> no longer blocks <code>state</code>)</li> </ul>"},{"location":"changelog/#0221--2019-11-30","title":"0.22.1 / 2019-11-30","text":"<ul> <li>Improve Reflector reset algorithm (clear history less)</li> </ul>"},{"location":"changelog/#0220--2019-11-29","title":"0.22.0 / 2019-11-29","text":"<ul> <li>Default watch timeouts changed to 300s everywhere</li> <li>This increases efficiency of Informers and Reflectors by keeping the connection open longer.</li> <li>However, if your Reflector relies on frequent polling you can set <code>timeout</code> or hide the <code>poll()</code> in a different context so it doesn't block your main work</li> <li>Internal <code>RwLock</code> changed to a <code>futures::Mutex</code> for soundness / proper non-blocking - #94</li> <li>blocking <code>Reflector::read()</code> renamed to <code>async Reflector::state()</code></li> <li>Expose <code>metadata.creation_timestamp</code> and <code>.deletion_timestamp</code> (behind openapi flag) - #93</li> </ul>"},{"location":"changelog/#0210--2019-11-29","title":"0.21.0 / 2019-11-29","text":"<ul> <li>All watch calls returns a stream of <code>WatchEvent</code> - #92</li> <li><code>Informer::poll</code> now returns a stream - #92</li> </ul>"},{"location":"changelog/#0201--2019-11-21","title":"0.20.1 / 2019-11-21","text":"<ul> <li>ObjectList now implements Iterator - #91</li> <li>openapi feature no longer accidentally hardcoded to v1.15 feature - #90</li> </ul>"},{"location":"changelog/#0190--2019-11-15","title":"0.19.0 / 2019-11-15","text":"<ul> <li>kube::Error is now a proper error enum and not a Fail impl (thiserror)</li> <li>soft-tokio dependency removed for futures-timer</li> <li>gzip re-introduced</li> </ul>"},{"location":"changelog/#0181--2019-11-11","title":"0.18.1 / 2019-11-11","text":"<ul> <li>Fix unpinned gzip dependency breakage - #87</li> </ul>"},{"location":"changelog/#0180--2019-11-07","title":"0.18.0 / 2019-11-07","text":"<ul> <li>api converted to use async/await with 1.39.0 (primitively)</li> <li>hyper upgraded to 0.10-alpha</li> <li>synchronous sleep replaced with tokio timer</li> <li><code>Log</code> trait removed in favour of internal marker trait</li> </ul>"},{"location":"changelog/#0170--2019-10-22","title":"0.17.0 / 2019-10-22","text":"<ul> <li>Add support for oidc providerss with <code>auth-provider</code> w/o <code>access-token</code> - #70</li> <li>Bump most dependencies to more recent versions</li> <li>Expose custom client creation</li> <li>Added support for <code>v1beta1Ingress</code></li> <li>Expose incluster_config::load_default_ns - #74</li> </ul>"},{"location":"changelog/#0161--2019-08-09","title":"0.16.1 / 2019-08-09","text":"<ul> <li>Add missing <code>uid</code> field on <code>ObjectMeta::ownerReferences</code></li> </ul>"},{"location":"changelog/#0160--2019-08-09","title":"0.16.0 / 2019-08-09","text":"<ul> <li>Add <code>Reflector::get</code> and <code>Reflector::get_within</code> as cheaper getters</li> <li>Add support for OpenShift kube configs with multiple CAs - via #64</li> <li>Add missing <code>ObjectMeta::ownerReferences</code></li> <li>Reduced memory consumption during compile with <code>k8s-openapi@0.5.1</code> - #62</li> </ul>"},{"location":"changelog/#0151--2019-08-18","title":"0.15.1 / 2019-08-18","text":"<ul> <li>Fix compile issue on <code>1.37.0</code> with <code>Utc</code> serialization</li> <li>Fix <code>Void</code> not having <code>Serialize</code> derive</li> </ul>"},{"location":"changelog/#0150--2019-08-11","title":"0.15.0 / 2019-08-11","text":"<ul> <li>Added support for <code>v1Job</code> resources - via #58</li> <li>Added support for <code>v1Namespace</code>, <code>v1DaemonSet</code>, <code>v1ReplicaSet</code>, <code>v1PersistentVolumeClaim</code>, <code>v1PersistentVolume</code>, <code>v1ResourceQuota</code>, <code>v1HorizontalPodAutoscaler</code> - via #59</li> <li>Added support for <code>v1beta1CronJob</code>, <code>v1ReplicationController</code>, <code>v1VolumeAttachment</code>, <code>v1NetworkPolicy</code> - via #60</li> <li><code>k8s-openapi</code> optional dependency bumped to <code>0.5.0</code> (for kube 1.14 structs)</li> </ul>"},{"location":"changelog/#0140--2019-08-03","title":"0.14.0 / 2019-08-03","text":"<ul> <li><code>Reflector::read</code> now returns a <code>Vec&lt;K&gt;`` rather than a</code>Vec&lt;(name, K)&gt;`:     This fixes an unsoundness bug internally - #56 via @gnieto</li> </ul>"},{"location":"changelog/#0130--2019-07-22","title":"0.13.0 / 2019-07-22","text":"<ul> <li>Experimental oauth2 support for some providers - via #44 :<ul> <li>a big cherry-pick from various prs upstream originally for GCP</li> <li>EKS works with setup in https://github.com/kube-rs/kube/pull/20#issuecomment-511767551</li> </ul> </li> </ul>"},{"location":"changelog/#0120--2019-07-18","title":"0.12.0 / 2019-07-18","text":"<ul> <li>Added support for <code>Log</code> subresource - via #50</li> <li>Added support for <code>v1ConfigMap</code> with example - via #49</li> <li>Demoted some spammy info messages from Reflector</li> </ul>"},{"location":"changelog/#0110--2019-07-10","title":"0.11.0 / 2019-07-10","text":"<ul> <li>Added <code>PatchParams</code> with <code>PatchStrategy</code> to allow arbitrary patch types - #24 via @ragne</li> <li><code>Event</code> renamed to <code>v1Event</code> to match non-slowflake type names</li> <li><code>v1Service</code> support added</li> <li>Added <code>v1Secret</code> snowflake type and a <code>secret_reflector</code> example</li> </ul>"},{"location":"changelog/#0100--2019-06-03","title":"0.10.0 / 2019-06-03","text":"<ul> <li><code>Api&lt;P, U&gt;</code> is now <code>Api&lt;K&gt;</code> for some <code>KubeObject</code> K:<ul> <li>Big change to allow snowflake objects (#35) - but also slightly nicer</li> <li>You want aliases <code>type Pod = Object&lt;PodSpec, PodStatus&gt;</code></li> <li>This gives you the required <code>KubeObject</code> trait impl for free</li> </ul> </li> <li> <p>Added <code>Event</code> native type to prove snowflakes can be handled - #35</p> </li> <li> <p><code>ApiStatus</code> renamed to <code>Status</code> to match kube api conventions #36</p> </li> <li>Rename <code>Metadata</code> to <code>ObjectMeta</code> #36</li> <li>Added <code>ListMeta</code> for <code>ObjectList</code> and <code>Status</code> #36</li> <li>Added <code>TypeMeta</code> object which is flattened onto <code>Object</code>, so:<ul> <li><code>o.types.kind</code> rather than <code>o.kind</code></li> <li><code>o.types.version</code> rather than <code>o.version</code></li> </ul> </li> </ul>"},{"location":"changelog/#090--2019-06-02","title":"0.9.0 / 2019-06-02","text":"<ul> <li>Status subresource api commands added to <code>Api</code>:<ul> <li><code>patch_status</code></li> <li><code>get_status</code></li> <li><code>replace_status</code>   ^ See <code>crd_openapi</code> or <code>crd_api</code> examples</li> </ul> </li> <li>Scale subresource commands added to <code>Api</code>:<ul> <li><code>patch_scale</code></li> <li><code>get_scale</code></li> <li><code>replace_scale</code>   ^ See <code>crd_openapi</code> example</li> </ul> </li> </ul>"},{"location":"changelog/#080--2019-05-31","title":"0.8.0 / 2019-05-31","text":"<ul> <li>Typed <code>Api</code> variant called <code>OpenApi</code> introduced (see crd_openapi example)</li> <li>Revert <code>client.request</code> return type change (back to response only from pre-0.7.0 #28)</li> <li><code>delete</code> now returns `Either, ApiStatus&gt; - for bug#32 <li><code>delete_collection</code> now returns `Either&gt;, ApiStatus&gt; - for bug#32 <li><code>Informer::new</code> renamed to <code>Informer::raw</code></li> <li><code>Reflector::new</code> renamed to <code>Reflector::raw</code></li> <li><code>Reflector::new</code> + <code>Informer::new</code> added for \"openapi\" compile time feature (does not require specifying the generic types)</li>"},{"location":"changelog/#070--2019-05-27","title":"0.7.0 / 2019-05-27","text":"<ul> <li>Expose list/watch parameters #11</li> <li>Many API struct renames:<ul> <li><code>ResourceMap</code> -&gt; <code>Cache</code></li> <li><code>Resource</code> -&gt; <code>Object</code></li> <li><code>ResourceList</code> -&gt; <code>ObjectList</code></li> <li><code>ApiResource</code> -&gt; <code>Api</code></li> </ul> </li> <li><code>ResourceType</code> has been removed in favour of <code>Api::v1Pod()</code> say</li> <li><code>Object::status</code> now wrapped in an <code>Option</code> (not present everywhere)</li> <li><code>ObjectList</code> exposed</li> <li>Major API overhaul to support generic operations on <code>Object</code></li> <li>Api can be used to perform generic actions on resources:<ul> <li><code>create</code></li> <li><code>get</code></li> <li><code>delete</code></li> <li><code>watch</code></li> <li><code>list</code></li> <li><code>patch</code></li> <li><code>replace</code></li> <li><code>get_scale</code> (when scale subresource exists)</li> <li><code>patch_scale</code> (ditto)</li> <li><code>replace_scale</code> (ditto)</li> <li><code>get_status</code> (when status subresource exists)</li> <li><code>patch_status</code> (ditto)</li> <li><code>replace_status</code> (ditto)</li> </ul> </li> <li>crd_api example added to track the action api</li> <li>Bunch of generic parameter structs exposed for common operations:<ul> <li><code>ListParams</code> exposed</li> <li><code>DeleteParams</code> exposed</li> <li><code>PostParams</code> exposed</li> </ul> </li> <li>Errors from <code>Api</code> exposed in <code>kube::Error</code>:<ul> <li><code>Error::api_error -&gt; Option&lt;ApiError&gt;</code> exposed</li> <li>Various other error types also in there (but awkward setup atm)</li> </ul> </li> <li><code>client.request</code> now returns a tuple <code>(T, StatusCode)</code> (before only <code>T</code>)</li> </ul>"},{"location":"changelog/#060--2019-05-12","title":"0.6.0 / 2019-05-12","text":"<ul> <li>Expose getter <code>Informer::version</code></li> <li>Exose ctor <code>Informer::from_version</code></li> <li>Expose more attributes in <code>Metadata</code></li> <li><code>Informer::reset</code> convenience method added</li> <li><code>Informer::poll</code> no longer returns events straight</li> <li>an <code>Informer</code> now caches <code>WatchEvent</code> elements into an internal queue</li> <li><code>Informer::pop</code> pops a single element from its internal queue</li> <li><code>Reflector::refresh</code> renamed to <code>Reflector::reset</code> (matches <code>Informer</code>)</li> <li><code>Void</code> type added so we can use <code>Reflector&lt;ActualSpec, Void&gt;</code><ul> <li>removes need for Spec/Status structs:</li> <li><code>ReflectorSpec</code>, <code>ReflectorStatus</code> removed</li> <li><code>InformerSpec</code>, <code>InformerStatus</code> removed</li> <li><code>ResourceSpecMap</code>, <code>ResourceStatusMap</code> removed</li> </ul> </li> <li><code>WatchEvents</code> removed</li> <li><code>WatchEvent</code> exposed, and now wraps `Resource``"},{"location":"changelog/#050--2019-05-09","title":"0.5.0 / 2019-05-09","text":"<ul> <li>added <code>Informer</code> struct dedicated to handling events</li> <li>Reflectors no longer cache <code>events</code> - see #6</li> </ul>"},{"location":"changelog/#040--2019-05-09","title":"0.4.0 / 2019-05-09","text":"<ul> <li>ResourceMap now contains the full Resource struct rather than a tuple as the value. =&gt; <code>value.metadata</code> is available in the cache. <li>Reflectors now also cache <code>events</code> to allow apps to handle them</li>"},{"location":"changelog/#030--2019-05-09","title":"0.3.0 / 2019-05-09","text":"<ul> <li><code>Named</code> trait removed (inferring from metadata.name now)</li> <li>Reflectors now take two type parameters (unless you use <code>ReflectorSpec</code> or <code>ReflectorStatus</code>) - see examples for usage</li> <li>Native kube types supported via <code>ApiResource</code></li> <li>Some native kube resources have easy converters to <code>ApiResource</code></li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-guide","title":"Contributing Guide","text":"<p>This document describes the requirements for committing to this repository.</p>"},{"location":"contributing/#developer-certificate-of-origin-dco","title":"Developer Certificate of Origin (DCO)","text":"<p>In order to contribute to this project, you must sign each of your commits to attest that you have the right to contribute that code. This is done with the <code>-s</code>/<code>--signoff</code> flag on <code>git commit</code>. More information about <code>DCO</code> can be found here</p>"},{"location":"contributing/#pull-request-management","title":"Pull Request Management","text":"<p>All code that is contributed to kube-rs must go through the Pull Request (PR) process. To contribute a PR, fork this project, create a new branch, make changes on that branch, and then use GitHub to open a pull request with your changes.</p> <p>Every PR must be reviewed by at least one Maintainer of the project. Once a PR has been marked \"Approved\" by a Maintainer (and no other Maintainer has an open \"Rejected\" vote), the PR may be merged. While it is fine for non-maintainers to contribute their own code reviews, those reviews do not satisfy the above requirement.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the CNCF Code of Conduct.</p>"},{"location":"contributing/#rust-guidelines","title":"Rust Guidelines","text":"<ul> <li>Channel: Code is built and tested using the stable channel of Rust, but documented and formatted with nightly *</li> <li>Formatting: To format the codebase, run <code>just fmt</code></li> <li>Documentation To check documentation, run <code>just doc</code></li> <li>Testing: To run tests, run <code>just test</code> and see below.</li> </ul> <p>For a list of tooling that we glue together everything see TOOLS.md.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We have 3 classes of tests.</p> <ul> <li>Unit tests &amp; Documentation Tests</li> <li>Integration tests (requires Kubernetes)</li> <li>End to End tests (requires Kubernetes)</li> </ul> <p>The last two will try to access the Kubernetes cluster that is your <code>current-context</code>; i.e. via your local <code>KUBECONFIG</code> evar or <code>~/.kube/config</code> file.</p> <p>The easiest way set up a minimal Kubernetes cluster for these is with <code>k3d</code> (<code>just k3d</code>).</p>"},{"location":"contributing/#unit-tests--documentation-tests","title":"Unit Tests &amp; Documentation Tests","text":"<p>Most unit/doc tests are run from <code>cargo test --lib --doc --all</code>, but because of feature-sets, and examples, you will need a couple of extra invocations to replicate our CI.</p> <p>For the complete variations, run the <code>just test</code> target in the <code>justfile</code>.</p> <p>All public interfaces must be documented, and most should have minor documentation examples to show usage.</p>"},{"location":"contributing/#integration-tests","title":"Integration Tests","text":"<p>Slower set of tests within the crates marked with an <code>#[ignore]</code> attribute.</p> <p> These  WILL try to modify resources in your current cluster </p> <p>Most integration tests are run with <code>cargo test --all --lib -- --ignored</code>, but because of feature-sets, you will need a few invocations of these to replicate our CI. See <code>just test-integration</code></p>"},{"location":"contributing/#end-to-end-tests","title":"End to End Tests","text":"<p>We have a small set of e2e tests that tests difference between in-cluster and local configuration.</p> <p>These tests are the heaviest tests we have because they require a full <code>docker build</code>, image import (or push/pull flow), yaml construction, and <code>kubectl</code> usage to verify that the outcome was sufficient.</p> <p>To run E2E tests, use (or follow) <code>just e2e</code> as appropriate.</p>"},{"location":"contributing/#test-guidelines","title":"Test Guidelines","text":""},{"location":"contributing/#when-to-add-a-test","title":"When to add a test","text":"<p>All public interfaces should have doc tests with examples for docs.rs.</p> <p>When adding new non-trivial pieces of logic that results in a drop in coverage you should add a test.</p> <p>Cross-reference with the coverage build  and go to your branch. Coverage can also be run locally with <code>cargo tarpaulin</code> at project root. This will use our tarpaulin.toml config, and will run both unit and integration tests.</p>"},{"location":"contributing/#what-type-of-test","title":"What type of test","text":"<ul> <li>Unit tests MUST NOT try to contact a Kubernetes cluster</li> <li>Doc tests MUST be marked as <code>no_run</code> when they need to contact a Kubernetes cluster</li> <li>Integration tests MUST NOT be used when a unit test is sufficient</li> <li>Integration tests MUST NOT assume existence of non-standard objects in the cluster</li> <li>Integration tests MUST NOT cross-depend on other unit tests completing (and installing what you need)</li> <li>E2E tests MUST NOT be used where an integration test is sufficient</li> </ul> <p>In general: use the least powerful method of testing available to you:</p> <ul> <li>use unit tests in <code>kube-core</code></li> <li>use unit tests in <code>kube-client</code> (and in rare cases integration tests)</li> <li>use unit tests in <code>kube-runtime</code> (and occassionally integration tests)</li> <li>use e2e tests when testing differences between in-cluster and local configuration</li> </ul>"},{"location":"contributing/#support","title":"Support","text":""},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The high-level architecture document is written for contributors.</p>"},{"location":"contributing/#contact","title":"Contact","text":"<p>You can ask general questions / share ideas / query the community at the kube-rs discussions forum. You can reach the maintainers of this project at #kube channel on the Tokio discord.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Select a version of <code>kube</code> along with the generated k8s-openapi types corresponding for your cluster version:</p> <pre><code>[dependencies]\nkube = { version = \"0.78.0\", features = [\"runtime\", \"derive\"] }\nk8s-openapi = { version = \"0.17.0\", features = [\"v1_26\"] }\n</code></pre> <p>Features are available.</p>"},{"location":"getting-started/#upgrading","title":"Upgrading","text":"<p>Please check the CHANGELOG when upgrading. All crates herein are versioned and released together to guarantee compatibility before 1.0.</p>"},{"location":"getting-started/#usage","title":"Usage","text":"<p>See the examples directory for how to use any of these crates.</p> <ul> <li>kube API Docs</li> </ul> <p>Official examples:</p> <ul> <li>version-rs: lightweight deployment <code>reflector</code> using axum</li> <li>controller-rs: <code>Controller</code> of a crd inside actix</li> </ul> <p>For real world projects see ADOPTERS.</p>"},{"location":"getting-started/#api","title":"Api","text":"<p>The <code>Api</code> is what interacts with kubernetes resources, and is generic over <code>Resource</code>:</p> <pre><code>use k8s_openapi::api::core::v1::Pod;\nlet pods: Api&lt;Pod&gt; = Api::default_namespaced(client);\n\nlet p = pods.get(\"blog\").await?;\nprintln!(\"Got blog pod with containers: {:?}\", p.spec.unwrap().containers);\n\nlet patch = json!({\"spec\": {\n    \"activeDeadlineSeconds\": 5\n}});\nlet pp = PatchParams::apply(\"kube\");\nlet patched = pods.patch(\"blog\", &amp;pp, &amp;Patch::Apply(patch)).await?;\nassert_eq!(patched.spec.active_deadline_seconds, Some(5));\n\npods.delete(\"blog\", &amp;DeleteParams::default()).await?;\n</code></pre> <p>See the examples ending in <code>_api</code> examples for more detail.</p>"},{"location":"getting-started/#custom-resource-definitions","title":"Custom Resource Definitions","text":"<p>Working with custom resources uses automatic code-generation via proc_macros in kube-derive.</p> <p>You need to <code>#[derive(CustomResource)]</code> and some <code>#[kube(attrs..)]</code> on a spec struct:</p> <pre><code>#[derive(CustomResource, Debug, Serialize, Deserialize, Default, Clone, JsonSchema)]\n#[kube(group = \"kube.rs\", version = \"v1\", kind = \"Document\", namespaced)]\npub struct DocumentSpec {\n    title: String,\n    content: String,\n}\n</code></pre> <p>Then you can use the generated wrapper struct <code>Document</code> as a <code>kube::Resource</code>:</p> <pre><code>let docs: Api&lt;Document&gt; = Api::default_namespaced(client);\nlet d = Document::new(\"guide\", DocumentSpec::default());\nprintln!(\"doc: {:?}\", d);\nprintln!(\"crd: {:?}\", serde_yaml::to_string(&amp;Document::crd()));\n</code></pre> <p>There are a ton of kubebuilder-like instructions that you can annotate with here. See the documentation or the <code>crd_</code> prefixed examples for more.</p> <p>NB: <code>#[derive(CustomResource)]</code> requires the <code>derive</code> feature enabled on <code>kube</code>.</p>"},{"location":"getting-started/#runtime","title":"Runtime","text":"<p>The <code>runtime</code> module exports the <code>kube_runtime</code> crate and contains higher level abstractions on top of the <code>Api</code> and <code>Resource</code> types so that you don't have to do all the <code>watch</code>/<code>resourceVersion</code>/storage book-keeping yourself.</p>"},{"location":"getting-started/#watchers","title":"Watchers","text":"<p>A low level streaming interface (similar to informers) that presents <code>Applied</code>, <code>Deleted</code> or <code>Restarted</code> events.</p> <pre><code>let api = Api::&lt;Pod&gt;::default_namespaced(client);\nlet stream = watcher(api, ListParams::default()).applied_objects();\n</code></pre> <p>This now gives a continual stream of events and you do not need to care about the watch having to restart, or connections dropping.</p> <pre><code>while let Some(event) = stream.try_next().await? {\n    println!(\"Applied: {}\", event.name_any());\n}\n</code></pre> <p>NB: the plain items in a <code>watcher</code> stream are different from <code>WatchEvent</code>. If you are following along to \"see what changed\", you should flatten it with one of the utilities from <code>WatchStreamExt</code>, such as <code>applied_objects</code>.</p>"},{"location":"getting-started/#reflectors","title":"Reflectors","text":"<p>A <code>reflector</code> is a <code>watcher</code> with <code>Store</code> on <code>K</code>. It acts on all the <code>Event&lt;K&gt;</code> exposed by <code>watcher</code> to ensure that the state in the <code>Store</code> is as accurate as possible.</p> <pre><code>let nodes: Api&lt;Node&gt; = Api::all(client);\nlet lp = ListParams::default().labels(\"kubernetes.io/arch=amd64\");\nlet (reader, writer) = reflector::store();\nlet rf = reflector(writer, watcher(nodes, lp));\n</code></pre> <p>At this point you can listen to the <code>reflector</code> as if it was a <code>watcher</code>, but you can also query the <code>reader</code> at any point.</p>"},{"location":"getting-started/#controllers","title":"Controllers","text":"<p>A <code>Controller</code> is a <code>reflector</code> along with an arbitrary number of watchers that schedule events internally to send events through a reconciler:</p> <pre><code>Controller::new(root_kind_api, ListParams::default())\n    .owns(child_kind_api, ListParams::default())\n    .run(reconcile, error_policy, context)\n    .for_each(|res| async move {\n        match res {\n            Ok(o) =&gt; info!(\"reconciled {:?}\", o),\n            Err(e) =&gt; warn!(\"reconcile failed: {}\", Report::from(e)),\n        }\n    })\n    .await;\n</code></pre> <p>Here <code>reconcile</code> and <code>error_policy</code> refer to functions you define. The first will be called when the root or child elements change, and the second when the <code>reconciler</code> returns an <code>Err</code>.</p>"},{"location":"getting-started/#rustls","title":"Rustls","text":"<p>Kube has basic support (with caveats) for rustls as a replacement for the <code>openssl</code> dependency. To use this, turn off default features, and enable <code>rustls-tls</code>:</p> <pre><code>[dependencies]\nkube = { version = \"0.78.0\", default-features = false, features = [\"client\", \"rustls-tls\"] }\nk8s-openapi = { version = \"0.17.0\", features = [\"v1_26\"] }\n</code></pre> <p>This will pull in <code>rustls</code> and <code>hyper-rustls</code>.</p>"},{"location":"getting-started/#musl-libc","title":"musl-libc","text":"<p>Kube will work with distroless, scratch, and <code>alpine</code> (it's also possible to use alpine as a builder with some caveats).</p>"},{"location":"getting-started/#license","title":"License","text":"<p>Apache 2.0 licensed. See LICENSE for details.</p>"},{"location":"governance/","title":"Governance","text":"<p>This document defines project governance for Kube-rs.</p>"},{"location":"governance/#contributors","title":"Contributors","text":"<p>Kube-rs is for everyone. Anyone can become a Kube-rs contributor simply by contributing to the project, whether through code, documentation, blog posts, community management, or other means. As with all Kube-rs community members, contributors are expected to follow the Kube-rs Code of Conduct.</p> <p>All contributions to Kube-rs code, documentation, or other components in the Kube-rs GitHub org must follow the guidelines in CONTRIBUTING.md. Whether these contributions are merged into the project is the prerogative of the maintainers.</p>"},{"location":"governance/#maintainer-expectations","title":"Maintainer Expectations","text":"<p>Maintainers have the ability to merge code into the project. Anyone can become a Kube-rs maintainer (see \"Becoming a maintainer\" below.)</p> <p>As such, there are certain expectations for maintainers. Kube-rs maintainers are expected to:</p> <ul> <li>Review pull requests, triage issues, and fix bugs in their areas of expertise, ensuring that all changes go through the project's code review and integration processes.</li> <li>Monitor the Kube-rs Discord, and Discussions and help out when possible.</li> <li>Rapidly respond to any time-sensitive security release processes.</li> <li>Participate on discussions on the roadmap.</li> </ul> <p>If a maintainer is no longer interested in or cannot perform the duties listed above, they should move themselves to emeritus status. If necessary, this can also occur through the decision-making process outlined below.</p>"},{"location":"governance/#maintainer-decision-making","title":"Maintainer decision-making","text":"<p>Ideally, all project decisions are resolved by maintainer consensus. If this is not possible, maintainers may call a vote. The voting process is a simple majority in which each maintainer receives one vote.</p>"},{"location":"governance/#special-tasks","title":"Special Tasks","text":"<p>In addition to the outlined abilities and responsibilities outlined above, some maintainers take on additional tasks and responsibilities.</p>"},{"location":"governance/#release-tasks","title":"Release Tasks","text":"<p>As a maintainer on the release team, you are expected to be cut releases. In particular:</p> <ul> <li>Cut releases, and update the CHANGELOG</li> <li>Pre-verify big releases against example repos</li> <li>Publish and update versions in example repos</li> <li>Verify the release</li> </ul>"},{"location":"governance/#becoming-a-maintainer","title":"Becoming a maintainer","text":"<p>Anyone can become a Kube-rs maintainer. Maintainers should be highly proficient in Rust; have relevant domain expertise; have the time and ability to meet the maintainer expectations above; and demonstrate the ability to work with the existing maintainers and project processes.</p> <p>To become a maintainer, start by expressing interest to existing maintainers. Existing maintainers will then ask you to demonstrate the qualifications above by contributing PRs, doing code reviews, and other such tasks under their guidance. After several months of working together, maintainers will decide whether to grant maintainer status.</p>"},{"location":"integrations/","title":"Integrations","text":""},{"location":"kubernetes-version/","title":"Kubernetes version","text":""},{"location":"kubernetes-version/#compatibility","title":"Compatibility","text":"<p>Our Kubernetes version compatibility is similar to the strategy employed by client-go and can interoperate well under a wide range of target Kubernetes versions defined by a soft minimum (MK8SV) and  the current latest available Kubernetes feature version.</p>    kube version MK8SV Latest Generated Source     <code>0.78.0</code> <code>1.21</code> <code>1.26</code> k8s-openapi@0.17.0   <code>0.75.0</code> <code>1.20</code> <code>1.25</code> k8s-openapi@0.16.0   <code>0.73.0</code> <code>1.19</code> <code>1.24</code> k8s-openapi@0.15.0   <code>0.67.0</code> <code>1.18</code> <code>1.23</code> k8s-openapi@0.14.0   <code>0.66.0</code> <code>1.17</code> <code>1.22</code> k8s-openapi@0.13.0   <code>0.57.0</code> <code>1.16</code> <code>1.21</code> k8s-openapi@0.12.0   <code>0.48.0</code> <code>1.15</code> <code>1.20</code> k8s-openapi@0.11.0    <p>The MK8SV is listed in our README as a badge:</p>    <p>The minimum indicates the lower bound of our testing range, and the latest is the maximum Kubernetes version selectable as a target version.</p>  <p>Minimum Kubernetes Version Policy</p> <p>The Minimum Supported Kubernetes Version (MK8SV) is set as 5 releases below the latest Kubernetes version.</p>  <p>This policy is intended to match stable channel support within major cloud providers. Compare with: EKS, AKS, GKE, upstream Kubernetes.</p>"},{"location":"kubernetes-version/#picking-versions","title":"Picking Versions","text":"<p>Given a <code>kube</code> versions, you must pick a target Kubernetes version from the available ones in the generated source that is used by that kube version.</p> <p>E.g. if using <code>kube@0.73.0</code>, we see its generated source is <code>k8s-openapi@0.15.0</code>, which exports the following version features.</p> <p>You can find the latest supported from this feature list and pick this as your target. In this case the latest supported version feature is <code>v1_24</code>.</p> <p>By default; you SHOULD pick the latest as your target version even when running against older clusters. The exception is if you are programming explicitly against apis that have been removed in newer versions.</p> <p>With k8s-pb, we plan on doing this automatically.</p> <p>See below for details on a skew between your cluster and your target version.</p>"},{"location":"kubernetes-version/#version-skew","title":"Version Skew","text":"<p>How kube version skew interacts with clusters is largely determined by how Kubernetes deprecates api versions upstream.</p> <p>Consider the following outcomes when picking target versions based on your cluster version:</p> <ol> <li>if <code>target version == cluster version</code> (cluster in sync with kube), then:<ul> <li>kube has api parity with cluster</li> <li>Rust structs are all queryable via kube</li> </ul> </li> <li>if <code>target version &gt; cluster version</code> (cluster behind kube), then:<ul> <li>kube has more recent api features than the cluster supports</li> <li>recent Rust api structs might not work with the cluster version yet</li> <li>deprecated/alpha apis might have been removed from Rust structs \u26a1</li> </ul> </li> <li>if <code>target version &lt; cluster version</code> (cluster ahead of kube), then:<ul> <li>kube has less recent api features than the cluster supports</li> <li>recent Kubernetes resources might not have Rust struct counterparts</li> <li>deprecated/alpha apis might have been removed from the cluster \u26a1</li> </ul> </li> </ol> <p>Kubernetes takes a long time to remove deprecated apis (unless they alpha or beta apis), so the acceptable distance from your cluster version actually depends on what apis you target.</p> <p>In particular, when using your own custom or stable official api resources - where exceeding the range will have little impact.</p> <p>If you are targeting deprecated/alpha apis on the other hand, then you should pick a target version in sync with your cluster. Note that alpha apis may vanish or change significantly in a single release, and is not covered by any guarantees.</p> <p>As a result; relying on alpha apis will make the amount of upgrades required to an application more frequent. To alleviate this; consider using api discovery to match on available api versions rather than writing code against each Kubernetes version.</p>"},{"location":"kubernetes-version/#outside-the-range","title":"Outside The Range","text":"<p>We recommend developers stay within the supported version range for the best experience, but it is technically possible to operate outside the bounds of this range (by picking older features from <code>k8s-openapi</code>, or by running against older clusters).</p>  <p>Untested Version Combinations</p> <p>While exceeding the supported version range is likely to work for most api resources: we do not test kube's functionality outside this version range.</p>  <p>In minor skews, kube and Kubernetes will share a large functioning API surface, while relying on deprecated apis to fill the gap. However, the further you stray from the range you are increasingly likely to encounter Rust structs that doesn't work against your cluster, or miss support for resources entirely.</p>"},{"location":"kubernetes-version/#special-abstractions","title":"Special Abstractions","text":"<p>For a small number of api resources, kube provides abstractions that are not managed along with the generated sources. For these cases we currently track the source and remove when Kubernetes removes them.</p> <p>This only affects a small number of special resources such as <code>CustomResourceDefinition</code>, <code>Event</code>, <code>Lease</code>, <code>AdmissionReview</code>.</p>"},{"location":"kubernetes-version/#example","title":"Example","text":"<p>The <code>CustomResourceDefinition</code> resource at <code>v1beta1</code> was removed in Kubernetes <code>1.22</code>:</p>  <p>The apiextensions.k8s.io/v1beta1 API version of CustomResourceDefinition is no longer served as of v1.22.</p>  <p>Their replacement; in <code>v1</code> was released in Kubernetes <code>1.16</code>.</p> <p>Kube had special support for both versions of <code>CustomResourceDefinition</code> from <code>0.26.0</code> up until <code>0.72.0</code> when kube supported structs from Kubernetes &gt;= 1.22.</p> <p>This special support took the form of the proc macro CustomResource and associated helpers that allowing pinning the crd version to <code>v1beta1</code> up until its removal. It is now <code>v1</code> only.</p>"},{"location":"maintainers/","title":"Maintainers","text":"<p>The Kube-rs maintainers are:</p> <ul> <li>Eirik Albrigtsen sszynrae@gmail.com @clux</li> <li>Natalie Klestrup R\u00f6ijezon nat@nullable.se @nightkr</li> <li>Kaz Yoshihara kazk.dev@gmail.com @kazk</li> </ul>"},{"location":"maintainers/#emeriti","title":"Emeriti","text":"<p>Former maintainers include:</p> <ul> <li>Ryan Levick ryan.levick@gmail.com @rylev</li> </ul>"},{"location":"quick-tutorial/","title":"Quick Tutorial","text":""},{"location":"release-process/","title":"Release Process","text":"<p>The release process for all the crates in kube is briefly outlined in release.toml.</p>"},{"location":"release-process/#versioning","title":"Versioning","text":"<p>We currently release all crates with the same version.</p> <p>The crates are thus version-locked and new version in certain sub-crates does not necessarily guarantee changes to that crate. Our changelog considers changes to the facade crate <code>kube</code> as the highest importance.</p> <p>The crates are published in reverse order of importance, releasing the final facade crate <code>kube</code> last, so users who depend on this do not notice any version-mismatches during releases.</p>"},{"location":"release-process/#cadence","title":"Cadence","text":"<p>We currently have no fixed cadence, but we still try to release roughly once a month, or whenever important PRs are merged (whichever is earliest).</p>"},{"location":"release-process/#for-maintainers-cutting-releases","title":"For maintainers: Cutting Releases","text":"<p>Cutting releases is a task for the maintenance team (contributing) and requires developer tools installed.</p> <p>The process is automated where possible, and the non-writing bits usually only take a few minutes, whereas the management of documentation and release resources require a bit of manual oversight.</p>"},{"location":"release-process/#preliminary-steps","title":"Preliminary Steps","text":"<p>Close the current ongoing milestone, and ensure the prs merged since the last version are included in the milestone.</p> <p>Ensure the PRs in the milestone all have exactly one <code>changelog-*</code> label to ensure the release notes are generated correctly (we follow Keep a Changelog with the setup as outlined in #754).</p>"},{"location":"release-process/#publishing-crates","title":"Publishing Crates","text":"<p>Start the process by publishing to crates.io (crate-by-crate) locally with the latest stable rust toolchain installed and active:</p> <pre><code>PUBLISH_GRACE_SLEEP=20 cargo release minor --execute\n</code></pre> <p>once this completes, double check that all the crates are correctly published to crates.io:</p> <ul> <li>kube-core</li> <li>kube-derive</li> <li>kube-client</li> <li>kube-runtime</li> <li>kube</li> </ul> <p>This will enqueue a documentation build in docs.rs to complete.</p>  <p>Docs build usually completes in less than <code>30m</code>, but we have seen it take around half a day when publishing during an anticipated rust release.</p>"},{"location":"release-process/#generating-the-release","title":"Generating the release","text":"<p>Once the crates have been published, we can start the process for creating a GitHub Release.</p> <p>If you just published, you will have at least one commit unpushed locally. You can push and tag this in one go using it:</p> <pre><code>./scripts/release-post.sh\n</code></pre> <p>This creates a tag, and a draft release using our release workflow. The resulting github release will show up on kube-rs/releases immediately.</p> <p>However, we should not publish this until the enqueued documentation build in docs.rs completes.</p> <p>We use this wait-time to fully prepare the release, and write the manual release header:</p>"},{"location":"release-process/#editing-the-draft","title":"Editing the draft","text":"<p>At this point we can edit the draft release. Click the edit release pencil icon, and start editing.</p> <p>You will notice auto-generated notes already present in the <code>textarea</code> along with new contributors - please leave these lines intact!</p> <p>Check if any of the PRs in the release contain any notices or are particularly noteworthy. We strongly advocate for highlighting some or more of the following, as part of the manually written header:</p> <ul> <li>big features</li> <li>big fixes</li> <li>contributor recognition</li> <li>interface changes</li> </ul>  <p>A release is more than just a <code>git tag</code>, it should be something to celebrate, for the maintainers, the contributors, and the community.</p>  <p>See the appendix below for ideas.</p> <p>Of course, not every release is going to be noteworthy. For these cases, it's perfectly OK to just hit Publish without much ceremony.</p>"},{"location":"release-process/#completion-steps","title":"Completion Steps","text":"<ul> <li>Create a new milestone for current minor version + 1</li> <li>Press Publish on the release once docs.rs build completes</li> <li>Run <code>./scripts/release-afterdoc.sh</code> to port the changed release notes into the <code>CHANGELOG.md</code> and push</li> <li>Run <code>./sync.sh</code> in the website repo to port the new release notes onto the website</li> </ul>"},{"location":"release-process/#appendix","title":"Appendix","text":""},{"location":"release-process/#header-formatting-tips","title":"Header Formatting Tips","text":"<p>Some example release notes from recent history has some ideas:</p> <ul> <li>0.68.0</li> <li>0.66.0</li> </ul> <p>Note that headers should link to PRs/important documents, but it is not necessary to link into the release or the milestone in this document yourself (the <code>afterdoc</code> step automates this).</p> <p>For breaking changes; consider including migration code samples for users if it provides an easier way to understand the changes. Fenced code blocks with <code>diff</code> language are easy to scan:</p> <pre><code>-async fn reconcile(myobj: MyK, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n+async fn reconcile(myobj: Arc&lt;MyK&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n</code></pre> <p>New features should link to the new additions under docs.rs/kube once the documentation build completes.</p> <p>...</p>"},{"location":"rust-version/","title":"Rust Version","text":"<p>Builds on CI always run against the most stable rust version, but we also support stable versions down to a specified Minimum Supported Rust Version (MSRV).</p>"},{"location":"rust-version/#minimum-supported-rust-version","title":"Minimum Supported Rust Version","text":"<p>The MSRV is shown in the main Cargo.toml and as a readme badge:</p>    <p>Our MSRV policy is to try to let our MSRV trail 2 stable versions behind the latest stable as a convenience to users downstream.</p>  <p>Best Effort Policy</p> <p>Note that this policy, while sometimes more lenient than 2 versions, it is also not guaranteed to hold due to dependencies we have to upgrade. The version shown should be taken as best effort and descriptive, and it is verified as buildable by CI.</p>"},{"location":"rust-version/#for-maintainers-bumping-the-msrv","title":"For maintainers: Bumping the MSRV","text":"<p>Bumping the MSRV is done either as:</p> <ul> <li>a response to a PR that brings in a dependency with a higher MSRV</li> <li>an explicit choice to get new rust features</li> </ul> <p>Performing the change requires developer tools, and is done by running <code>just bump-msrv 1.60.0</code> to bump the all the <code>Cargo.toml</code> files:</p> <pre><code>-rust-version = \"1.56.0\"\n+rust-version = \"1.60.0\"\n</code></pre> <p>as well as the badge plus devcontainer. If the bump is sufficient, CI will verify that the specified MSRV works with the current dependencies and feature usage before it can be merged.</p> <p>NB: An MSRV change must have the <code>changelog-change</code> label so that it is sufficiently highlighted in our changelog.</p>"},{"location":"rust-version/#for-contributors-nightly-tooling","title":"For contributors: Nightly tooling","text":"<p>Due to some limitations in stable <code>rustdoc</code> and <code>rustfmt</code>, we use the <code>nightly</code> toolchain for auto-formatting and documentation.</p> <p>NB: This is contributor quirk only. All crates will always build with the stable toolchain.</p> <p>CI always runs documentation and formatting builds against the latest <code>nightly</code>, but in general, the relevant features change very infrequently, so it's generally safe to rely on older nightly builds unless CI complains.</p> <pre><code>rustup update nightly\n</code></pre>"},{"location":"security/","title":"Security","text":""},{"location":"security/#security-policy","title":"Security Policy","text":""},{"location":"security/#supported-versions","title":"Supported Versions","text":"<p>We provide security updates for the two most recent minor versions released on <code>crates.io</code>.</p> <p>For example, if <code>0.70.1</code> is the most recent stable version, we will address security updates for <code>0.69</code> and later. Once <code>0.71.1</code> is released, we will no longer provide updates for <code>0.69</code> releases.</p>"},{"location":"security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>To report a security problem in Kube-rs, please contact at least two maintainers.</p> <p>These people will help diagnose the severity of the issue and determine how to address the issue. Issues deemed to be non-critical will be filed as GitHub issues. Critical issues will receive immediate attention and be fixed as quickly as possible.</p>"},{"location":"security/#security-advisories","title":"Security Advisories","text":"<p>When serious security problems in Kube-rs are discovered and corrected, we issue a security advisory, describing the problem and containing a pointer to the fix.</p> <p>These are announced the RustSec Advisory Database, to our github issues under the label <code>critical</code>, as well as discord and other primary communication channels.</p> <p>Security issues are fixed as soon as possible, and the fixes are propagated to the stable branches as fast as possible. However, when a vulnerability is found during a code audit, or when several other issues are likely to be spotted and fixed in the near future, the security team may delay the release of a Security Advisory, so that one unique, comprehensive Security Advisory covering several vulnerabilities can be issued. Communication with vendors and other distributions shipping the same code may also cause these delays.</p>"},{"location":"tools/","title":"Tools","text":"<p>All repositories under kube-rs are buildable using FLOSS tools, and they are listed herein.</p>"},{"location":"tools/#user-dependencies","title":"User Dependencies","text":"<p>Dependencies a user needs to use kube-rs.</p> <ul> <li>Rust</li> <li>various crates satisfying our license allowlist</li> </ul>"},{"location":"tools/#development-dependencies","title":"Development Dependencies","text":"<p>Dependencies a developer might find helpful to develop on kube-rs.</p>"},{"location":"tools/#build-dependencies","title":"Build Dependencies","text":"<p>CLIs that are used for occasional build or release time manipulation. Maintainers need these.</p> <ul> <li>fd</li> <li>jq</li> <li>just</li> <li>sd</li> <li>rg</li> <li>choose</li> <li>fastmod</li> <li>curl</li> <li>cargo-release</li> </ul> <p>GNU tools like <code>grep</code> + <code>head</code> + <code>tail</code> + <code>awk</code> + <code>sed</code>, are also referenced a handful of times, but are generally avoided due to cross-platform compatibility issues, plus the existence of more modern tools above.</p>"},{"location":"tools/#ci-dependencies","title":"CI Dependencies","text":"<ul> <li>cargo-audit</li> <li>cargo-deny</li> <li>cargo-msrv</li> <li>cargo-tarpaulin</li> </ul>"},{"location":"tools/#integration-tests","title":"Integration Tests","text":"<p>CLIs that are used in integration tests, or referenced as ways recommended to test locally.</p> <ul> <li>k3d</li> <li>tilt</li> <li>docker/cli</li> </ul>"},{"location":"website/","title":"Website","text":"<p>This website is hosted on kube.rs via github pages from the kube-rs/website repo, and accepts contributions in the form of pull requests to change the markdown files that generate the website.</p>"},{"location":"website/#structure","title":"Structure","text":"<p>The docs folder contains all the resources that's inlined on the webpage and can be edited on this page using any editor.</p> <p>It is recommended having markdown preview, wikilink, and the foam extension, but what is rendered is ultimately just generated from markdown with wikilinks.</p>"},{"location":"website/#synchronization","title":"Synchronization","text":"<p>A subset of markdown documents show up in certain paths of the github contribution process and must remain in these original repos.</p>  <p>Synchronized markdown documents will be overwritten if edited herein!</p>  <p>Notice the first line of these files contain a line like the following:</p> <pre><code>&lt;!--GENERATED FROM\nhttps://github.com/kube-rs/kube-rs/blob/master/CONTRIBUTING.md\nCHANGES MUST BE MADE THERE --&gt;\n</code></pre> <p>These files must be edited upstream at the given path, and will be synchronized to this site on the next kube release or sooner.</p>"},{"location":"controllers/admission/","title":"Admission WIP","text":"<p>admission into Kubernetes.</p>"},{"location":"controllers/application/","title":"The Application","text":"<p>The application starts the Controller and links it up with the reconciler for your object.</p>"},{"location":"controllers/application/#goal","title":"Goal","text":"<p>This document shows the basics of creating a simple controller with a <code>Pod</code> as the main object.</p>"},{"location":"controllers/application/#requirements","title":"Requirements","text":"<p>We will assume that you have latest stable rust installed, along with cargo-edit:</p>"},{"location":"controllers/application/#project-setup","title":"Project Setup","text":"<pre><code>cargo new --bin ctrl\ncd ctrl\n</code></pre> <p>add then install <code>kube</code>, <code>k8s-openapi</code>, <code>thiserror</code>, <code>futures</code>, and <code>tokio</code> using cargo-edit:</p> <pre><code>cargo add kube --features=runtime,client,derive\ncargo add k8s-openapi --features=v1_25\ncargo add thiserror\ncargo add tokio --features=macros,rt-multi-thread\ncargo add futures\n</code></pre>  <p>This will populate some <code>[dependencies]</code> in your <code>Cargo.toml</code> file.</p>"},{"location":"controllers/application/#main-dependencies","title":"Main Dependencies","text":"<p>The kube dependency is what we provide. It's used here with its controller <code>runtime</code> feature, its Kubernetes <code>client</code> and the <code>derive</code> macro for custom resources.</p> <p>The k8s-openapi dependency is needed if using core Kubernetes resources.</p> <p>The thiserror dependency is used in this guide as an easy way to do basic error handling, but it is optional.</p> <p>The futures dependency provides helpful abstractions when working with asynchronous rust.</p> <p>The tokio runtime dependency is needed to use async rust features, and is the supported way to use futures created by kube.</p>  <p>Alternate async runtimes</p> <p>We depend on <code>tokio</code> for its <code>time</code>, <code>signal</code> and <code>sync</code> features, and while it is in theory possible to swap out a runtime, you would be sacrificing the most actively supported and most advanced runtime available. Avoid going down this alternate path unless you have a good reason.</p>  <p>Additional dependencies are useful, but we will go through these later as we add more features.</p>"},{"location":"controllers/application/#setting-up-errors","title":"Setting up errors","text":"<p>We will start with the right thing from the start and define a proper <code>Error</code> enum:</p> <pre><code>#[derive(thiserror::Error, Debug)]\npub enum Error {}\n\npub type Result&lt;T, E = Error&gt; = std::result::Result&lt;T, E&gt;;\n</code></pre>"},{"location":"controllers/application/#define-the-object","title":"Define the object","text":"<p>Import the object that you want to control into your <code>main.rs</code>.</p> <p>For the purposes of this demo we are going to use Pod (hence the explicit <code>k8s-openapi</code> dependency):</p> <pre><code>use k8s_openapi::api::core::v1::Pod;\n</code></pre>"},{"location":"controllers/application/#seting-up-the-controller","title":"Seting up the controller","text":"<p>This is where we will start defining our <code>main</code> and glue everything together:</p> <pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), kube::Error&gt; {\n    let client = Client::try_default().await?;\n    let pods = Api::&lt;Pod&gt;::all(client);\n\n    Controller::new(pods.clone(), Default::default())\n        .run(reconcile, error_policy, Arc::new(()))\n        .for_each(|_| futures::future::ready(()))\n        .await;\n\n    Ok(())\n}\n</code></pre> <p>This creates a Client, a Pod Api object (for all namespaces), and a Controller for the full list of pods defined by a default ListParams.</p> <p>We are not using relations here, so we merely tell the controller to call reconcile when a pod changes.</p>"},{"location":"controllers/application/#creating-the-reconciler","title":"Creating the reconciler","text":"<p>You need to define at least a basic <code>reconcile</code> fn</p> <pre><code>async fn reconcile(obj: Arc&lt;Pod&gt;, ctx: Arc&lt;()&gt;) -&gt; Result&lt;Action&gt; {\n    println!(\"reconcile request: {}\", obj.name_any());\n    Ok(Action::requeue(Duration::from_secs(3600)))\n}\n</code></pre> <p>and a basic error handler (for what to do when <code>reconcile</code> returns an <code>Err</code>):</p> <pre><code>fn error_policy(_object: Arc&lt;Pod&gt;, _err: &amp;Error, _ctx: Arc&lt;()&gt;) -&gt; Action {\n    Action::requeue(Duration::from_secs(5))\n}\n</code></pre> <p>To make this reconciler useful, we can reuse the one created in the reconciler document, on a custom object.</p>"},{"location":"controllers/application/#checkpoint","title":"Checkpoint","text":"<p>If you copy-pasted everything above, and fixed imports, you should have a <code>src/main.rs</code> in your <code>ctrl</code> directory with this:</p> <pre><code>use std::{sync::Arc, time::Duration};\nuse futures::StreamExt;\nuse k8s_openapi::api::core::v1::Pod;\nuse kube::{\n    Api, Client, ResourceExt,\n    runtime::controller::{Action, Controller}\n};\n\n#[derive(thiserror::Error, Debug)]\npub enum Error {}\npub type Result&lt;T, E = Error&gt; = std::result::Result&lt;T, E&gt;;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), kube::Error&gt; {\n    let client = Client::try_default().await?;\n    let pods = Api::&lt;Pod&gt;::all(client);\n\n    Controller::new(pods.clone(), Default::default())\n        .run(reconcile, error_policy, Arc::new(()))\n        .for_each(|_| futures::future::ready(()))\n        .await;\n\n    Ok(())\n}\n\nasync fn reconcile(obj: Arc&lt;Pod&gt;, ctx: Arc&lt;()&gt;) -&gt; Result&lt;Action&gt; {\n    println!(\"reconcile request: {}\", obj.name_any());\n    Ok(Action::requeue(Duration::from_secs(3600)))\n}\n\nfn error_policy(_object: Arc&lt;Pod&gt;, _err: &amp;Error, _ctx: Arc&lt;()&gt;) -&gt; Action {\n    Action::requeue(Duration::from_secs(5))\n}\n</code></pre>"},{"location":"controllers/application/#developing","title":"Developing","text":"<p>At this point, you are ready start the app and see if it works. I.e. you need Kubernetes.</p>"},{"location":"controllers/application/#prerequisites","title":"Prerequisites","text":"<p>If you already have a cluster, skip this part.</p>  <p>We will develop locally against a <code>k3d</code> cluster (which requires <code>docker</code> and <code>kubectl</code>).</p> <p>Install the latest k3d release, then run:</p> <pre><code>k3d cluster create kube --servers 1 --agents 1 --registry-create kube\n</code></pre> <p>If you can run <code>kubectl get nodes</code> after this, you are good to go. See k3d/quick-start for help.</p>"},{"location":"controllers/application/#local-development","title":"Local Development","text":"<p>In your <code>ctrl</code> directory, you can now <code>cargo run</code> and check that you can successfully connect to your cluster.</p> <p>You should see an output like the following:</p> <pre><code>reconcile request: helm-install-traefik-pxnnd\nreconcile request: helm-install-traefik-crd-8z56p\nreconcile request: traefik-97b44b794-wj5ql\nreconcile request: svclb-traefik-5gmsm\nreconcile request: coredns-7448499f4d-72rvq\nreconcile request: metrics-server-86cbb8457f-8fct5\nreconcile request: local-path-provisioner-5ff76fc89d-4x86w\nreconcile request: svclb-traefik-q8zkw\n</code></pre> <p>I.e. you should get a reconcile request for every pod in your cluster (<code>kubectl get pods --all</code>).</p> <p>If you now edit a pod (via <code>kubectl edit pod traefik-xxx</code> and make a change), or create a new pod, you should immediately get a reconcile request.</p> <p>Congratulations. You have just built your first kube controller. \ud83c\udf89</p>  <p>Continuation</p> <p>At this point, you have gotten the 3 main components; an object, a reconciler and an application, but there are many topics we have not touched on. Follow the links to other pages to learn more.</p>"},{"location":"controllers/application/#deploying","title":"Deploying","text":""},{"location":"controllers/application/#containerising","title":"Containerising","text":"<p>WIP. Showcase both multi-stage rust build and musl builds into distroless.</p>"},{"location":"controllers/application/#containerised-development","title":"Containerised Development","text":"<p>WIP. Showcase a basic <code>tilt</code> setup with <code>k3d</code>.</p>"},{"location":"controllers/application/#continuous-integration","title":"Continuous Integration","text":"<p>See testing and security.</p>"},{"location":"controllers/application/#extras","title":"Extras","text":"<p>TODO: link completed WIP documents here.</p>"},{"location":"controllers/application/#adding-observability","title":"Adding observability","text":"<p>Want to add tracing, metrics or just get better logs than <code>println</code>, see the observability document.</p>"},{"location":"controllers/application/#useful-dependencies","title":"Useful Dependencies","text":"<p>The following dependencies are already used transitively within kube that may be of use to you. Use of these will generally not inflate your total build times due to already being present in the tree:</p> <ul> <li>tracing</li> <li>futures</li> <li>k8s-openapi</li> <li>serde</li> <li>serde_json</li> <li>serde_yaml</li> <li>tower</li> <li>tower-http</li> <li>hyper</li> </ul> <p>These in turn also pull in their own dependencies (and tls features, depending on your tls stack), consult cargo-tree for help minimizing your dependency tree.</p>"},{"location":"controllers/internals/","title":"Internals","text":"<p>This is a brief overview of Controller internals.</p> <p>Suppose you have a <code>Controller</code> for a main object <code>K</code> which owns a child object <code>C</code>.</p> <p>I.e. if <code>child_c</code> is an <code>Api&lt;C&gt;</code> and <code>main_k</code> is an <code>Api&lt;K&gt;</code>, then the following code sets up this basic scenario:</p> <pre><code>Controller::new(main_k, ListParams::default())\n    .owns(child_c, ListParams::default())\n    .run(reconcile, error_policy, context)\n    .for_each(|_| futures::future::ready(()))\n    .await\n</code></pre> <p>This <code>Controller</code> builder sets up a series of streams and links between them:</p> <pre><code>graph TD\n    K{{Kubernetes}} --&gt;|Event K| W(watchers)\n    K --&gt;|Event C| W\n    W --&gt;|map C -&gt; K| I(queue)\n    W --&gt;|owned K| I\n    RU --&gt;|run| R(reconciler)\n    A(applier) --&gt;|poll| I\n    A --&gt;|schedule| S(scheduler)\n    A --&gt;|poll| S\n    A --&gt;|next| RU(runner)\n    R --&gt;|Update| X{{World}}\n    R -.-&gt;|result| A\n    subgraph \"Controller\"\n    W\n    I\n    S\n    A\n    RU\n    end\n    subgraph \"Application\"\n    Controller\n    R\n    end</code></pre> <p>I.e. basic flow.</p> <ul> <li><code>watcher</code>s poll the Kubernetes api for changes to configured objects (<code>K</code> and <code>C</code>)</li> <li>stream of events from each <code>watcher</code> needs to be turned into a stream of the same type</li> <li>streams of non K type run through mappers (<code>C</code> maps to <code>K</code> through relations)</li> <li>initial queue created as the union of these streams</li> </ul> <p>The <code>applier</code> then polls this stream and merges it further with previous reconciliation results (requeues for a later time). This forces the need for a <code>scheduler</code>.</p>"},{"location":"controllers/internals/#queue","title":"Queue","text":"<p>The queue is the stream of inputs.</p> <p>It takes <code>N</code> main inputs (root object, related objects, external triggers), and it is our <code>trigger_selector</code>.</p>"},{"location":"controllers/internals/#scheduler","title":"Scheduler","text":"<p>The <code>scheduler</code> wraps the object to be reconciled in a <code>future</code> that will resolve when its associated timer is up.</p> <p>All reconcile requests go through this, but only requeues are delayed significantly.</p>"},{"location":"controllers/internals/#applier","title":"Applier","text":"<p>The applier is the most complicated component on the diagram because it is in charge of juggling the various input streams, invoking the scheduler and runner, and also somehow produce a stream of reconcile results at the same time.</p> <p>The flow of doing this is quite complicated in rust so the way this is done internally is likely subject to change in the long run. Please refer to the source of the applier for more details.</p> <p>It is possible to set up the controller machinery yourself by creating the queue yourself from watchers and then calling the <code>applier</code> with the queue injected, but this is not recommended.</p>"},{"location":"controllers/intro/","title":"Introduction","text":"<p>This is a larger guide to showcase how to build controllers, and is a WIP with a progress issue.</p>"},{"location":"controllers/intro/#overview","title":"Overview","text":"<p>A controller a long-running program that ensures the kubernetes state of an object, matches the state of the world.</p> <p>As users update the desired state, the controller sees the change and schedules a reconciliation, which will update the state of the world:</p> <pre><code>flowchart TD\n    A[User/CD] -- kubectl apply object.yaml --&gt; K[Kubernetes Api]\n    C[Controller] -- watch objects --&gt; K\n    C -- schedule object --&gt; R[Reconciler]\n    R -- result --&gt; C\n    R -- update state --&gt; K</code></pre> <p>any unsuccessful reconciliations are retried or requeued, so a controller should eventually apply the desired state to the world.</p> <p>Writing a controller requires three pieces:</p> <ul> <li>an object dictating what the world should see</li> <li>an reconciler function that ensures the state of one object is applied to the world</li> <li>an application living in kubernetes watching the object and related objects</li> </ul>"},{"location":"controllers/intro/#the-object","title":"The Object","text":"<p>The main object is the source of truth for what the world should be like, and it takes the form of a Kubernetes object like a:</p> <ul> <li>Pod</li> <li>Deployment</li> <li>..any native Kubernetes Resource</li> <li>a partially typed or dynamically typed Kubernetes Resource</li> <li>an object from api discovery</li> <li>a Custom Resource</li> </ul> <p>Kubernetes already has a core controller manager for the core native objects, so the most common use-case for controller writing is a Custom Resource, but many more fine-grained use-cases exist. </p> <p>See the object document for how to use the various types.</p>"},{"location":"controllers/intro/#the-reconciler","title":"The Reconciler","text":"<p>The reconconciler is the part of the controller that ensures the world is up to date.</p> <p>It takes the form of an <code>async fn</code> taking the object along with some context, and performs the alignment between the state of world and the <code>object</code>.</p> <p>In its simplest form, this is what a reconciler (that does nothing) looks like:</p> <pre><code>async fn reconcile(object: Arc&lt;MyObject&gt;, data: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n    // TODO: logic here\n    Ok(Action::requeue(Duration::from_secs(3600 / 2)))\n}\n</code></pre> <p>As a controller writer, your job is to complete the logic that align the world with what is inside the <code>object</code>. The core reconciler must at minimum contain mutating api calls to what your <code>object</code> is meant to manage, and in some situations, handle annotations management for ownership or garbage collection.</p> <p>Writing a good idempotent reconciler is the most difficult part of the whole affair, and its difficulty is the reason we generally provide diagnostics and observability:</p> <p>See the reconciler document for more information.</p>"},{"location":"controllers/intro/#the-application","title":"The Application","text":"<p>The controller application is the part that watches for changes, determines what root object needs reconciliations, and then schedules reconciliations for those changes. It is the glue that turns what you want into something running in Kubernetes.</p> <p>In this guide; the application is written in rust, using the kube crate as a dependency with the <code>runtime</code> feature, compiled into a container, and deployed in Kubernetes as a <code>Deployment</code>.</p> <p>The core features inside the application are:</p> <ul> <li>an encoding of the main object + relevant objects</li> <li>an infinite watch loop around relevant objects</li> <li>a system that maps object changes to the relevant main object</li> <li>an idempotent reconciler acting on a main object</li> </ul> <p>The system must be fault-tolerant, and thus must be able to recover from crashes, downtime, and resuming even having missed messages.</p> <p>Setting up a blank controller in rust satisfying these constraints is fairly simple, and can be done with minimal boilerplate (no generated files need be inlined in your project).</p> <p>See the application document for the high-level details.</p>"},{"location":"controllers/intro/#controllers-and-operators","title":"Controllers and Operators","text":"<p>The terminology between controllers and operators are quite similar:</p> <ol> <li>Kubernetes uses the following controller terminology:</li> </ol>  <p>In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.</p>  <ol> <li>The term operator, on the other hand, was originally introduced by <code>CoreOS</code> as:</li> </ol>  <p>An Operator is an application-specific controller that extends the Kubernetes API to create, configure and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts, but also includes domain or application-specific knowledge to automate common tasks better managed by computers.</p>  <p>Which is further reworded now under their new agglomerate banner.</p> <p>They key differences between the two is that operators generally a specific type of controller, sometimes more than one in a single application. A controller would at the very least need to:</p> <ul> <li>manage custom resource definition(s)</li> <li>maintain single app focus</li> </ul> <p>to be classified as an operator.</p> <p>The term operator is a flashier term that makes the common use-case for user-written CRD controllers more understandable. If you have a CRD, you likely want to write a controller for it (otherwise why go through the effort of making a custom resource?).</p>"},{"location":"controllers/intro/#guide-focus","title":"Guide Focus","text":"<p>Our goal is that with this guide, you will learn how to use and apply the various controller patterns, so that you can avoid scaffolding out a large / complex / underutilized structure.</p> <p>We will focus on all the patterns as to not betray the versatility of the Kubernetes API, because components found within complex controllers can generally be mixed and matched as you see fit.</p> <p>We will focus on how the various element composes so you can take advantage of any controller archetypes - operators included.</p>"},{"location":"controllers/object/","title":"The Object","text":"<p>A controller always needs a source of truth for what the world should look like, and this object always lives inside kubernetes.</p> <p>Depending on how the object was created/imported or performance optimization reasons, you can pick one of the following object archetypes:</p> <ul> <li>typed Kubernetes native resource</li> <li>Derived Custom Resource for Kubernetes</li> <li>Imported Custom Resource already in Kubernetes</li> <li>untyped Kubernetes resource</li> <li>partially typed Kubernetes resource</li> </ul> <p>We will outline how they interact with controllers and the basics of how to set them up.</p>"},{"location":"controllers/object/#typed-resource","title":"Typed Resource","text":"<p>This is the most common, and simplest case. Your source of truth is an existing Kubernetes object found in the openapi spec.</p> <p>To use a typed Kubernetes resource as a source of truth in a Controller, import it from k8s-openapi, and create an Api from it, then pass it to the Controller.</p> <pre><code>use k8s_openapi::api::core::v1::Pod;\n\nlet pods = Api::&lt;Pod&gt;::all(client);\nController::new(pods, ListParams::default())\n</code></pre> <p>This is the simplest flow and works right out of the box because the openapi implementation ensures we have all the api information via the Resource traits.</p> <p>If you have a native Kubernetes type, you generally want to start with k8s-openapi. If will likely do exactly what you want without further issues. That said, if both your clusters and your chosen object are large, then you can consider optimizing further by changing to a partially typed resource for smaller memory profile.</p> <p>A separate k8s-pb repository for our future protobuf serialization structs also exists, and while it will slot into this category and should hotswappable with k8s-openapi, it is not yet usable here.</p>"},{"location":"controllers/object/#custom-resources","title":"Custom Resources","text":""},{"location":"controllers/object/#derived-custom-resource","title":"Derived Custom Resource","text":"<p>The operator use case is heavily based on you writing your own struct, and a schema, and extending the kuberntes api with it.</p> <p>This has historically required a lot of boilerplate for both the api information and the (now required) schema, but this is a lot simpler with kube thanks to the CustomResource derive proc_macro.</p> <pre><code>/// Our Document custom resource spec\n#[derive(CustomResource, Deserialize, Serialize, Clone, Debug, JsonSchema)]\n#[kube(kind = \"Document\", group = \"kube.rs\", version = \"v1\", namespaced)]\n#[kube(status = \"DocumentStatus\")]\npub struct DocumentSpec {\n    name: String,\n    author: String,\n}\n\n#[derive(Deserialize, Serialize, Clone, Debug, JsonSchema)]\npub struct DocumentStatus {\n    checksum: String,\n    last_updated: Option&lt;DateTime&lt;Utc&gt;&gt;,\n}\n</code></pre> <p>This will generate a <code>pub struct Document</code> in this scope which implements Resource. In other words, to use it with the a controller is at this point analogous to a fully typed resource:</p> <pre><code>let docs = Api::&lt;Document&gt;::all(client);\nController::new(docs, ListParams::default())\n</code></pre>  <p>Custom resources require schemas</p> <p>Since v1 of CustomResourceDefinition became the main variant (<code>v1beta1</code> was removed in Kubernetes 1.22), a schema is required. These schemas are generated using schemars by specifying the <code>JsonSchema</code> derive. See the schemas section (TODO) for further information on advanced usage.</p>"},{"location":"controllers/object/#installation","title":"Installation","text":"<p>Before Kubernetes accepts api calls for a custom resource, we need to install it. This is the usual pattern for creating the yaml definition:</p> <pre><code># Cargo.toml\n[[bin]]\nname = \"crdgen\"\npath = \"src/crdgen.rs\"\n</code></pre> <pre><code>// crdgen.rs\nuse kube::CustomResourceExt;\nfn main() {\n    print!(\"{}\", serde_yaml::to_string(&amp;mylib::Document::crd()).unwrap())\n}\n</code></pre> <p>Here, a separate <code>crdgen</code> bin entry would install your custom resource using <code>cargo run --bin crdgen | kubectl -f -</code>.</p>  <p>Installation outside the controller</p> <p>While it is tempting to install a custom resource within your controller at startup, this is not advisable. The permissions needed to write to the cluster-level <code>customresourcedefinition</code> resource is almost always much higher than what your controller needs to run. It is thus advisable to generate the yaml out-of-band, and bundle it with the rest of the controller's installation yaml.</p>"},{"location":"controllers/object/#imported-custom-resource","title":"Imported Custom Resource","text":"<p>In the case that a <code>customresourcedefinition</code> already exists in your cluster, but it was implemented in another language, then we can generate structs from the schema using kopium.</p> <p>Suppose you want to write some extra controller or replace the native controller for <code>PrometheusRule</code>:</p> <pre><code>curl -sSL https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml \\\n    | kopium -Af - &gt; prometheusrule.rs\n</code></pre> <p>this will read the crd from the cluster, and generate rust-optimized structs for it:</p> <pre><code>use kube::CustomResource;\nuse schemars::JsonSchema;\nuse serde::{Serialize, Deserialize};\nuse std::collections::BTreeMap;\nuse k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;\n\n/// Specification of desired alerting rule definitions for Prometheus.\n#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)]\n#[kube(group = \"monitoring.coreos.com\", version = \"v1\", kind = \"PrometheusRule\", plural = \"prometheusrules\")]\n#[kube(namespaced)]\npub struct PrometheusRuleSpec {\n    /// Content of Prometheus rule file\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub groups: Option&lt;Vec&lt;PrometheusRuleGroups&gt;&gt;,\n}\n\n/// RuleGroup is a list of sequentially evaluated recording and alerting rules.\n#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]\npub struct PrometheusRuleGroups {\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub interval: Option&lt;String&gt;,\n    pub name: String,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub partial_response_strategy: Option&lt;String&gt;,\n    pub rules: Vec&lt;PrometheusRuleGroupsRules&gt;,\n}\n\n/// Rule describes an alerting or recording rule\n#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]\npub struct PrometheusRuleGroupsRules {\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub alert: Option&lt;String&gt;,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub annotations: Option&lt;BTreeMap&lt;String, String&gt;&gt;,\n    pub expr: IntOrString,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub r#for: Option&lt;String&gt;,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub labels: Option&lt;BTreeMap&lt;String, String&gt;&gt;,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub record: Option&lt;String&gt;,\n}\n</code></pre> <p>you typically would then import this file as a module and use it as follows:</p> <pre><code>use prometheusrule::PrometheusRule;\n\nlet prs: Api&lt;PrometheusRule&gt; = Api::default_namespaced(client);\nController::new(prs, ListParams::default())\n</code></pre>  <p>Kopium is unstable</p> <p>Kopium is a relatively new project and it is neither feature complete nor bug free at the moment. While feedback has been very positive, and people have so far contributed fixes for several major customresources; expect some snags.</p>"},{"location":"controllers/object/#dynamic-typing","title":"Dynamic Typing","text":""},{"location":"controllers/object/#untyped-resources","title":"Untyped Resources","text":"<p>Untyped resources are using DynamicObject; an umbrella container for arbitrary Kubernetes resources.</p>  <p>Hard to use with controllers</p> <p>This type is the most unergonomic variant available. You will have to operate on untyped json to grab data out of specifications and is best suited for general (non-controller) cases where you need to look at common metadata properties from ObjectMeta like <code>labels</code> and <code>annotations</code> across different object types.</p>  <p>The DynamicObject consists of just the unavoidable properties like <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code>, whereas the entire spec is loaded onto an arbitrary serde_json::Value via flattening.</p> <p>The benefits you get is that:</p> <ul> <li>you avoid having to write out fields manually</li> <li>you can achieve tolerance against multiple versions of your object</li> <li>it is compatible with api discovery</li> </ul> <p>but you do have to find out where the object lives on the api (its ApiResource) manually:</p> <pre><code>use kube::{api::{Api, DynamicObject}, discovery};\n\n// Discover most stable version variant of `documents.kube.rs`\nlet apigroup = discovery::group(&amp;client, \"kube.rs\").await?;\nlet (ar, caps) = apigroup.recommended_kind(\"Document\").unwrap();\n\n// Use the discovered kind in an Api, and Controller with the ApiResource as its DynamicType\nlet api: Api&lt;DynamicObject&gt; = Api::all_with(client, &amp;ar);\nController::new_with(api, ListParams::default(), &amp;ar)\n</code></pre> <p>Other ways of doing discovery are also available. We are highlighting recommended_kind in particular here because it can be used to achieve version agnosticity.</p>  <p>Multiple versions of an object</p> <p>Kubernetes supports specifying multiple versions of a specification, and using DynamicObject above can help solve that. There are other potential ways of achieving similar results, but it does require some work.</p>"},{"location":"controllers/object/#partially-typed-resource","title":"Partially-typed Resource","text":"<p>A very special-case setup where we specify a subset of the normal typed information, and allows tighter control over memory characteristics, and deserialization cost of the program, but at the cost of more <code>struct</code> code.</p>  <p>Better methods available for improving memory characteristics</p> <p>Because almost all methods on Kubernetes objects such as PodSpec are wrapped in <code>Option</code>s, as long as unnecessary properties are unset before passing them to a reflector, similar memory reductions can be achieved. One method is to use Event::modify chained onto the watcher stream. See the pod_reflector for details.</p> <p>Because of these advances, the partially-typed resource pattern is not recommended.</p>  <p>It is similar to DynamicObject (above) in that Object is another umbrella container for arbitrary Kubernetes resources, and also requires you to discover or hard-code an ApiResource for extra type information to be queriable.</p> <p>Here is an example of handwriting a new implementation of Pod by overriding its spec and status and placing it inside Object, then stealing its type information:</p> <pre><code>use kube::api::{Api, ApiResource, NotUsed, Object};\n\n// Here we replace heavy type k8s_openapi::api::core::v1::PodSpec with\n#[derive(Clone, Deserialize, Debug)]\nstruct PodSpecSimple {\n    containers: Vec&lt;ContainerSimple&gt;,\n}\n#[derive(Clone, Deserialize, Debug)]\nstruct ContainerSimple {\n    #[allow(dead_code)]\n    image: String,\n}\n// Pod replacement\ntype PodSimple = Object&lt;PodSpecSimple, NotUsed&gt;;\n\n// steal api resource information from k8s-openapi\nlet ar = ApiResource::erase::&lt;k8s_openapi::api::core::v1::Pod&gt;(&amp;());\n\nController::new_with(api, ListParams::default(), &amp;ar)\n</code></pre> <p>In the end, we end up with some extra lines to define our Pod, but we also drop every field inside spec + status except <code>spec.container.image</code>. If your cluster has thousands of pods and you want to do some kind of common operation on a small subset of fields, then this can give a very quick win in terms of memory use (a Controller will usually maintain a <code>Store</code> of all owned objects).</p>"},{"location":"controllers/object/#dynamic-new_with-constructors","title":"Dynamic new_with constructors","text":"<p>Partial or dynamic typing always needs additional type information</p> <p>All usage of <code>DynamicObject</code> or <code>Object</code> require the use of alternate constructors for multiple interfaces such as Api and Controller. These constructors have an additional <code>_with</code> suffix to carry an associated type for the Resource trait.</p>"},{"location":"controllers/object/#summary","title":"Summary","text":"<p>All the fully typed methods all have a consistent usage pattern once the types have been generated. The dynamic and partial objects have more niche use cases and require a little more work such as alternate constructors.</p>    typing Source Implementation      full k8s-openapi <code>use k8s-openapi::X</code>    full kube::CustomResource <code>#[derive(CustomResource)]</code>    full kopium <code>kopium &gt; gen.rs</code>    partial kube::core::Object partial copy-paste    none kube::core::DynamicObject write nothing"},{"location":"controllers/observability/","title":"Observability","text":"<p>This document showcases common techniques for instrumentation:</p> <ul> <li>logs (via tracing + tracing-subscriber + EnvFilter)</li> <li>traces (via tracing + tracing-subscriber + opentelemetry-otlp + tonic)</li> <li>metrics (via tikv/prometheus exposed via actix-web)</li> </ul> <p>and follows the approach of controller-rs.</p> <p>Most of this logic happens in <code>main</code>, before any machinery starts, so it will liberally <code>.unwrap()</code>.</p>"},{"location":"controllers/observability/#adding-logs","title":"Adding Logs","text":"<p>We will use the tracing library for logging because it allows us reusing the same system for tracing later.</p> <pre><code>cargo add tracing\ncargo add tracing-subscriber --features=json,env-filter\n</code></pre> <p>We will configure this in <code>main</code> by creating a <code>json</code> log layer with an EnvFilter picking up on the common <code>RUST_LOG</code> environment variable:</p> <pre><code>let logger = tracing_subscriber::fmt::layer().json();\nlet env_filter = EnvFilter::try_from_default_env()\n    .or_else(|_| EnvFilter::try_new(\"info\"))\n    .unwrap();\n</code></pre> <p>This can be set as the global collector using:</p> <pre><code>let collector = Registry::default().with(logger).with(env_filter);\ntracing::subscriber::set_global_default(collector).unwrap();\n</code></pre> <p>We will change how the <code>collector</code> is built if using tracing, but for now, this is sufficient for adding logging.</p>"},{"location":"controllers/observability/#adding-traces","title":"Adding Traces","text":"<p>Following on from logging section, we add extra dependencies to let us push traces to an opentelemetry collector (sending over gRPC with tonic):</p> <pre><code>cargo add opentelemetry --features=trace,rt-tokio\ncargo add opentelemetry-otlp --features=tokio\ncargo add tonic\n</code></pre> <p>Setting up the layer and configuring the <code>collector</code> follows fundamentally the same process:</p> <pre><code>let telemetry = tracing_opentelemetry::layer().with_tracer(init_tracer().await);\n</code></pre> <p>Note 3 layers now:</p> <pre><code>let collector = Registry::default().with(telemetry).with(logger).with(env_filter);\ntracing::subscriber::set_global_default(collector).unwrap();\n</code></pre> <p>However, tracing requires us to have a configurable location of where to send spans, so creating the actual <code>tracer</code> requires a bit more work:</p> <pre><code>async fn init_tracer() -&gt; opentelemetry::sdk::trace::Tracer {\n    let otlp_endpoint = std::env::var(\"OPENTELEMETRY_ENDPOINT_URL\")\n        .expect(\"Need a otel tracing collector configured\");\n\n    let channel = tonic::transport::Channel::from_shared(otlp_endpoint)\n        .unwrap()\n        .connect()\n        .await\n        .unwrap();\n\n    opentelemetry_otlp::new_pipeline()\n        .tracing()\n        .with_exporter(opentelemetry_otlp::new_exporter().tonic().with_channel(channel))\n        .with_trace_config(opentelemetry::sdk::trace::config().with_resource(\n            opentelemetry::sdk::Resource::new(vec![opentelemetry::KeyValue::new(\n                \"service.name\",\n                \"ctlr\", // TODO: change to controller name\n            )]),\n        ))\n        .install_batch(opentelemetry::runtime::Tokio)\n        .unwrap()\n}\n</code></pre> <p>Note the gRPC address (e.g. <code>OPENTELEMETRY_ENDPOINT_URL=https://0.0.0.0:55680</code>) must be explicitly wrapped in a <code>tonic::Channel</code>, and this forces an explicit dependency on tonic.</p>"},{"location":"controllers/observability/#instrumenting","title":"Instrumenting","text":"<p>At this point, you can start adding <code>#[instrument]</code> attributes onto functions you want, in particular <code>reconcile</code>:</p> <pre><code>#[instrument(skip(ctx))]\nasync fn reconcile(foo: Arc&lt;Foo&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt;\n</code></pre> <p>Note that the <code>reconcile</code> span should be the root span in the context of a controller. A reconciliation starting is the root of the chain: nothing called into the controller to reconcile an object, this happens regularly automatically.</p>  <p>Higher levels spans</p> <p>Do not <code>#[instrument]</code> any function that creates a Controller as this would create an unintentionally wide (application lifecycle wide) span being a parent to all <code>reconcile</code> spans. Such a span will be problematic to manage.</p>"},{"location":"controllers/observability/#linking-logs-and-traces","title":"Linking Logs and Traces","text":"<p>To link logs and traces we take advantage that tracing data is being outputted to both logs and our tracing collector, and attach the <code>trace_id</code> onto our root span:</p> <pre><code>#[instrument(skip(ctx), fields(trace_id))]\nasync fn reconcile(foo: Arc&lt;Foo&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n    let trace_id = get_trace_id();\n    Span::current().record(\"trace_id\", &amp;field::display(&amp;trace_id));\n    todo!(\"reconcile implementation\")\n}\n</code></pre> <p>This part is useful for Loki or other logging systems as a way to cross-link from logs to traces.</p> <p>Extracting the <code>trace_id</code> requires a helper function atm:</p> <pre><code>pub fn get_trace_id() -&gt; opentelemetry::trace::TraceId {\n    // opentelemetry::Context -&gt; opentelemetry::trace::Span\n    use opentelemetry::trace::TraceContextExt as _;\n    // tracing::Span -&gt; opentelemetry::Context\n    use tracing_opentelemetry::OpenTelemetrySpanExt as _;\n\n    tracing::Span::current()\n        .context()\n        .span()\n        .span_context()\n        .trace_id()\n}\n</code></pre> <p>and it is the only reason for needing to directly add opentelemetry as a dependency.</p>"},{"location":"controllers/observability/#adding-metrics","title":"Adding Metrics","text":"<p>This is the most verbose part of instrumentation because it introduces the need for a webserver, along with data modelling choices and library choices.</p> <p>We will use tikv's prometheus library as its the most battle tested library available:</p> <pre><code>cargo add prometheus\n</code></pre>  <p>Limitations</p> <p>The <code>prometheus</code> crate outlined herein does not support exemplars nor the openmetrics standard, at current writing. For newer features we will likely look toward the new official client, or the metrics crate suite.</p>"},{"location":"controllers/observability/#registering","title":"Registering","text":"<p>We will start creating a basic <code>Metrics</code> struct to house two metrics, a histogram and a counter:</p> <pre><code>#[derive(Clone)]\npub struct Metrics {\n    pub reconciliations: IntCounter,\n    pub failures: IntCounterVec,\n    pub reconcile_duration: HistogramVec,\n}\n\nimpl Default for Metrics {\n    fn default() -&gt; Self {\n        let reconcile_duration = HistogramVec::new(\n            histogram_opts!(\n                \"doc_controller_reconcile_duration_seconds\",\n                \"The duration of reconcile to complete in seconds\"\n            )\n            .buckets(vec![0.01, 0.1, 0.25, 0.5, 1., 5., 15., 60.]),\n            &amp;[],\n        )\n        .unwrap();\n        let failures = IntCounterVec::new(\n            opts!(\n                \"doc_controller_reconciliation_errors_total\",\n                \"reconciliation errors\",\n            ),\n            &amp;[\"instance\", \"error\"],\n        )\n        .unwrap();\n        let reconciliations =\n            IntCounter::new(\"doc_controller_reconciliations_total\", \"reconciliations\").unwrap();\n        Metrics {\n            reconciliations,\n            failures,\n            reconcile_duration,\n        }\n    }\n}\n</code></pre> <p>and as these metrics are measurable entirely from within <code>reconcile</code> or <code>error_policy</code> we can attach the struct to the context passed to the reconciler##using-context.</p>"},{"location":"controllers/observability/#measuring","title":"Measuring","text":"<p>Measuring our metric values can then be done by explicitly taking a <code>Duration</code>  inside <code>reconcile</code>, but it is easier to wrap this in a struct that relies on <code>Drop</code> with a convenience constructor:</p> <pre><code>pub struct ReconcileMeasurer {\n    start: Instant,\n    metric: HistogramVec,\n}\n\nimpl Drop for ReconcileMeasurer {\n    fn drop(&amp;mut self) {\n        let duration = self.start.elapsed().as_millis() as f64 / 1000.0;\n        self.metric.with_label_values(&amp;[]).observe(duration);\n    }\n}\n\nimpl Metrics {\n    pub fn count_and_measure(&amp;self) -&gt; ReconcileMeasurer {\n        self.reconciliations.inc();\n        ReconcileMeasurer {\n            start: Instant::now(),\n            metric: self.reconcile_duration.clone(),\n        }\n    }\n}\n</code></pre> <p>and call this from <code>reconcile</code> with one line:</p> <pre><code>async fn reconcile(foo: Arc&lt;Foo&gt;, ctx: Arc&lt;Context&gt;) -&gt; Result&lt;Action, Error&gt; {\n    let _timer = ctx.metrics.count_and_measure(); // increments now\n\n    // main reconcile body here\n\n    Ok(...) // drop impl invoked, computes time taken\n}\n</code></pre> <p>and handle the <code>failures</code> metric inside your  <code>error_policy</code>:</p> <pre><code>fn error_policy(doc: Arc&lt;Document&gt;, error: &amp;Error, ctx: Arc&lt;Context&gt;) -&gt; Action {\n    warn!(\"reconcile failed: {:?}\", error);\n    ctx.metrics.reconcile_failure(&amp;doc, error);\n    Action::requeue(Duration::from_secs(5 * 60))\n}\n\nimpl Metrics {\n    pub fn reconcile_failure(&amp;self, doc: &amp;Document, e: &amp;Error) {\n        self.failures\n            .with_label_values(&amp;[doc.name_any().as_ref(), e.metric_label().as_ref()])\n            .inc()\n    }\n}\n</code></pre> <p>We could increment the failure metric directly, but we have also made a helper function stashed away that extracts the object name and a short error name as labels for the metric.</p> <p>This type of error extraction requires an impl on your <code>Error</code> type. We use <code>Debug</code> here:</p> <pre><code>impl Error {\n    pub fn metric_label(&amp;self) -&gt; String {\n        format!(\"{self:?}\").to_lowercase()\n    }\n}\n</code></pre>  <p>Future exemplar work</p> <p>If we had exemplar support, we could have attached our <code>trace_id</code> to the histogram metric - through <code>count_and_measure</code> - to be able to cross-browse from grafana metric panels into a trace-viewer.</p>"},{"location":"controllers/observability/#exposing","title":"Exposing","text":"<p>For prometheus to obtain our metrics, we require a web server. As per the webserver guide, we will assume actix-web.</p> <p>In our case, we will pass a <code>State</code> struct that contains the <code>Metrics</code> struct and attach it to the <code>HttpServer</code> in <code>main</code>:</p> <pre><code>HttpServer::new(move || {\n    App::new()\n        .app_data(Data::new(state.clone())) // new state\n        .service(metrics) // new endpoint\n    })\n</code></pre> <p>the <code>metrics</code> service is the important one here, and its implementation is able to extract the <code>Metrics</code> struct from actix's <code>web::Data</code>:</p> <pre><code>#[get(\"/metrics\")]\nasync fn metrics(c: web::Data&lt;State&gt;, _req: HttpRequest) -&gt; impl Responder {\n    let metrics = c.metrics(); // grab out of actix data\n    let encoder = TextEncoder::new();\n    let mut buffer = vec![];\n    encoder.encode(&amp;metrics, &amp;mut buffer).unwrap();\n    HttpResponse::Ok().body(buffer)\n}\n</code></pre>"},{"location":"controllers/observability/#what-metrics","title":"What Metrics","text":"<p>The included metrics <code>failures</code>, <code>reconciliations</code> and a <code>reconcile_duration</code> histogram will be sufficient to have prometheus compute a wide array of details:</p> <ul> <li>reconcile amounts in last hour - <code>sum(increase(reconciliations[1h]))</code></li> <li>hourly error rates - <code>sum(rate(failures[1h]) / sum(rate(reconciliations[1h]))</code></li> <li>success rates - same rate setup but <code>reconciliations / (reconciliations + failures)</code></li> <li>p90 reconcile duration - <code>histogram_quantile(0.9, sum(rate(reconciliations[1h])))</code></li> </ul> <p>and you could then create alerts on aberrant values (e.g. say 10% error rate, zero reconciliation rate, and maybe p90 durations &gt;30s).</p> <p>The above metric setup should comprise the core need of a standard controller (although you may have more things to care about than our simple example).</p>  <p>kube-state-metrics</p> <p>It is possible to derive metrics from conditions and fields in your CRD schema using runtime flags to <code>kube-state-metrics</code> without instrumentation, but since this is an implicit dependency for operators, it should not be a default.</p>  <p>You will also want resource utilization metrics, but this is typically handled upstream. E.g. cpu/memory utilization metrics are generally available via kubelet's metrics and other utilization metrics can be gathered from node_exporter.</p>  <p>tokio-metrics</p> <p>New experimental runtime metrics are also availble for the tokio runtime via tokio-metrics.</p>"},{"location":"controllers/observability/#external-references","title":"External References","text":"<ul> <li>Metrics in axum using metrics crate</li> </ul>"},{"location":"controllers/reconciler/","title":"The Reconciler","text":"<p>The reconciler is the user-defined function in charge of reconciling the state of the world.</p> <pre><code>async fn reconcile(o: Arc&lt;K&gt;, ctx: Arc&lt;T&gt;) -&gt; Result&lt;Action, Error&gt;\n</code></pre> <p>It is always called with the object type that you instantiate the Controller with, regardless of what auxillary objects you end up watching:</p> <pre><code>graph TD\n    K{{Kubernetes}} --&gt;|changes| W(watchers)\n    W --&gt;|correlate| A(applier)\n    A --&gt;|run| R(reconciler)\n    R --&gt;|Update| X{{World}}\n    R -.-&gt;|result| A\n    subgraph \"Controller\"\n    W\n    A\n    end\n    subgraph \"Application\"\n    Controller\n    R\n    end</code></pre> <p>A Controller contains a machinery that will:</p> <ul> <li>watch api endpoints in Kubernetes (main object and related objects)</li> <li>map changes from those apis (via relations) into your main object</li> <li>schedule and apply reconciliations</li> <li>observe the result of reconciliations to decide when to reschedule</li> <li>tolerate a wide class of failures</li> </ul> <p>We will largerly treat Controller as a black-box, but details are explored in internals and architecture.</p> <p>As a user of <code>kube</code>, you will just to have to instantiate a Controller (see application) and define your <code>reconcile</code> fn.</p>"},{"location":"controllers/reconciler/#the-world","title":"The World","text":"<p>The state of the world is your main Kubernetes object along with anything your reconciler touches.</p>  <p>The World &gt;= Kubernetes</p> <p>While your main object must reside within Kubernetes, it is possibly to manage/act on changes outside Kubernetes.</p>  <p>You do not have to configure the world, as any side effect you perform implicitly becomes the world for your controller. It is, however, beneficial to specify any relations your object has with the world to ensure <code>reconcile</code> is correctly invoked:</p>"},{"location":"controllers/reconciler/#what-triggers-reconcile","title":"What triggers reconcile","text":"<p>The reconciler is invoked - for an instance of your <code>object</code> - if:</p> <ul> <li>that main object changed</li> <li>an owned object (with ownerReferences to the main object) changed</li> <li>a related object/api (pointing to the main object) changed</li> <li>the object had failed reconciliation before and was requeued earlier</li> <li>the object received an infrequent periodic reconcile request</li> </ul> <p>In other words; <code>reconcile</code> will be triggered periodically (infrequently), and immediately upon changes to the main object, or related objects.</p> <p>It is therefore beneficial to configure relations so the Controller will know what counts as a reconcile-worthy change.</p> <p>Typically, this is accomplished with a call to Controller::owns on any owned Api and ensuring ownerReferences are created in your reconciler. See relations for details.</p>"},{"location":"controllers/reconciler/#reasons-for-reconciliation","title":"Reasons for reconciliation","text":"<p>Notice that the reason for why the reconciliation started is not included in the signature of <code>reconcile</code>; you only get the object. The reason for this omission is fault-tolerance:</p>  <p>Fault-tolerance against missed messages</p> <p>If your controller is down / crashed earlier, you might have missed messages. In fact, no matter how well you guard against downtime (e.g with multiple replicas, rolling upgrades, pdbs, leases), the Kubernetes watch api is not sufficiently safe to guarantee unmissed messages. </p>   <p>It is unsafe to give you a reason for why you got a <code>reconcile</code> call, because it is sometimes impossible to know.</p>  <p>We therefore have to hide this information from you, and you are forced to write a more defensive reconciler.</p> <p>We have to:</p> <ul> <li>assume nothing about why reconciliation started</li> <li>assume the reconciler could have failed at any point during the function</li> <li>check every property independently (you always start at the beginning)</li> </ul> <p>The type of defensive function writing described above is intended to grant a formal property called idempotency.</p>"},{"location":"controllers/reconciler/#idempotency","title":"Idempotency","text":"<p>A function is said to be idempotent if it can be applied multiple times without changing the result beyond the initial application.</p>  <p>A reconciler must be idempotent</p> <p>If a reconciler is triggered twice for the same object, it must cause the same outcome. Care must be taken to ensure operations are not dependent on all-or-nothing approaches, and the flow of the reconciler must be able to recover from errors occurring in a previous reconcile runs.</p>  <p>Let us create a reconciler for a custom <code>PodManager</code> resource that will:</p> <ul> <li>create an associated <code>Pod</code> with ownerReferences</li> <li>note its <code>creation</code> time on the status object of <code>PodManager</code></li> </ul> <p>Both of these operations can be done in isolation in an idempotent manner (we will show below), but it is possible to compose the two operations in an erroneous way.</p>"},{"location":"controllers/reconciler/#combining-idempotent-operations","title":"Combining Idempotent Operations","text":"<p>A naive approach to the above problem might be to take a shortcut, and simply check if the work has been done, and if not, do it:</p> <pre><code>if pod_missing {\n    create_owned_pod()?;\n    set_timestamp_on_owner()?;\n}\n</code></pre> <p>Is this a good optimization? Can we avoid calling set timestamps over and over again?</p> <p>Unfortunately, this reasoning is flawed; if the timestamp creation fails after the pod got created the first time, the second action will never get done!</p>  <p>Reconciler interruptions</p> <p>If your reconciler errored half-way through a run; the only way you would know what failed, is if you check everything.</p>  <p>Therefore the correct way to do these two actions it to do them independently:</p> <pre><code>if pod_missing {\n    create_owned_pod()?;\n}\nif is_timestamp_missing() {\n    set_timestamp_on_owner()?;\n}\n</code></pre>"},{"location":"controllers/reconciler/#in-depth-solution","title":"In-depth Solution","text":"<p>Let's suppose we have access to an <code>Api&lt;Pod&gt;</code> and an <code>Api&lt;PodManager&gt;</code> (via a context):</p> <pre><code>let api: Api&lt;PodManager&gt; = ctx.api.clone();\nlet pods: Api&lt;Pod&gt; = Api::default_namespaced(ctx.client.clone());\n</code></pre> <p>and let's define the <code>Pod</code> we want to create as:</p> <pre><code>fn create_owned_pod(source: &amp;PodManager) -&gt; Pod {\n    let oref = source.controller_owner_ref(&amp;()).unwrap();\n    Pod {\n        metadata: ObjectMeta {\n            name: source.metadata.name.clone(),\n            owner_references: Some(vec![oref]),\n            ..ObjectMeta::default()\n        },\n        spec: my_pod_spec(),\n        ..Default::default()\n    }\n}\n</code></pre> <p>one approach of achieving idempotency is to check every property carefully::</p> <pre><code>// TODO: find using ownerReferences instead - has to be done using jsonpath...\n// {range .items[?(.metadata.ownerReferences.uid=262bab1a-1c79-11ea-8e23-42010a800016)]}{.metadata.name}{end}\n// make a helper for this?\nlet podfilter = ListParams::default()\n    .labels(format!(\"owned-by/{}\", obj.name_any()));\n\n// if owned pod is not created, do the work to create it\nlet pod: Pod = match &amp;pods.list(&amp;podfilter).await?[..] {\n    [p, ..] =&gt; p, // return the first found pod\n    [] =&gt; {\n        let pod_data = create_owned_pod(&amp;obj);\n        pods.create(pod_data).await? // return the new pod from the apiserver\n    },\n};\n\nif obj.status.pod_created.is_none() {\n    // update status object with the creation_timestamp of the owned Pod\n    let status = json!({\n        \"status\": PodManagerStatus { pod_created: pod.meta().creation_timestamp }\n    });\n    api.patch_status(&amp;obj.name_any(), &amp;PatchParams::default(), &amp;Patch::Merge(&amp;status))\n        .await?;\n}\n</code></pre> <p>but we can actually simplify this significantly by taking advantage of idempotent Kubernetes apis:</p> <pre><code>let pod_data = create_owned_pod(&amp;obj);\nlet serverside = PatchParams::apply(\"mycontroller\");\nlet pod = pods.patch(pod.name_any(), serverside, Patch::Apply(pod_data)).await?\n\n// update status object with the creation_timestamp of the owned Pod\nlet status = json!({\n    \"status\": PodManagerStatus { pod_created: pod.meta().creation_timestamp }\n});\napi.patch_status(&amp;obj.name_any(), &amp;PatchParams::default(), &amp;Patch::Merge(&amp;status))\n    .await?;\n</code></pre> <p>Here we are taking advantage of Server-Side Apply and deterministic naming of the owned pod to call the equivalent of <code>kubectl apply</code> on the <code>pod_data</code>.</p> <p>The <code>patch_status</code> is already idempotent, and does not technically need the pre-check. However, we might wish to keep the check, as this will lead to less networked requests.</p>"},{"location":"controllers/reconciler/#using-context","title":"Using Context","text":"<p>To do anything useful inside the reconciler like persisting your changes, you typically need to inject some client in there.</p> <p>The way this is done is through the context parameter on Controller::run. It's whatever you want, packed in an <code>Arc</code>.</p> <pre><code>// Context for our reconciler\n#[derive(Clone)]\nstruct Data {\n    /// kubernetes client\n    client: Client,\n    /// In memory state\n    state: Arc&lt;RwLock&lt;State&gt;&gt;,\n}\n\nlet context = Arc::new(Data {\n    client: client.clone(),\n    state: state.clone(),\n});\nController::new(foos, ListParams::default())\n    .run(reconcile, error_policy, context)\n</code></pre> <p>then you can pull out your user defined struct (here <code>Data</code>) items inside <code>reconcile</code>:</p> <pre><code>async fn reconcile(object: Arc&lt;MyObject&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n    ctx.state.write().await.last_event = Utc::now();\n    let reporter = ctx.state.read().await.reporter.clone();\n    let objs: Api&lt;MyObject&gt; = Api::all(ctx.client.clone());\n    // ...\n    Ok(Action::await_change())\n}\n</code></pre>"},{"location":"controllers/reconciler/#cleanup","title":"Cleanup","text":"<p>Kubernetes provides two methods of cleanup of resources; the automatic ownerReferences, and the manual (but safe) finalizers.</p> <p>WIP. Separate document describing these.</p>"},{"location":"controllers/reconciler/#instrumentation","title":"Instrumentation","text":"<p>The root <code>reconcile</code> function should be instrumented with logs, traces and metrics, and can also post diagnostic events to the Kubernetes api.</p> <p>See the observability document for how to add good instrumentation to your <code>reconcile</code> fn.</p>"},{"location":"controllers/reconciler/#diagnostics","title":"Diagnostics","text":"<p>WIP. Separate document for posting diagnostic events to the events api + using the status object.</p>"},{"location":"controllers/relations/","title":"Related Objects","text":"<p>A Controller needs to specify related resources if changes to them is meant to trigger the reconciler.</p> <p>These relations are generally set up with Controller::owns, but we will go through the different variants below.</p>"},{"location":"controllers/relations/#owned-relation","title":"Owned Relation","text":"<p>The Controller::owns relation is the most straight-forward and most ubiquitous one. One object controls the lifecycle of a child object, and cleanup happens automatically via ownerReferences.</p> <pre><code>let cmgs = Api::&lt;ConfigMapGenerator&gt;::all(client.clone());\nlet cms = Api::&lt;ConfigMap&gt;::all(client.clone());\n\nController::new(cmgs, ListParams::default())\n    .owns(cms, ListParams::default())\n</code></pre> <p>This configmapgen example uses one custom resource <code>ConfigMapGenerator</code> whose controller is in charge of the lifecycle of the child <code>ConfigMap</code>.</p> <ul> <li>What happens if we delete a <code>ConfigMapGenerator</code> instance here? Well, there will be a <code>ConfigMap</code> with ownerReferences matching the <code>ConfigMapGenerator</code> so Kubernetes will automatically cleanup the associated <code>ConfigMap</code>.</li> <li>What happens if we modify the managed <code>ConfigMap</code>? The Controller sees a change and associates the change with the owning <code>ConfigMapGenerator</code>, ultimately triggering a reconciliation of the root <code>ConfigMapGenerator</code>.</li> </ul> <p>This relation relies on ownerReferences being created on the managed/owned objects for Kubernetes automatic cleanup, and the Controller relies on it for association with its owner.</p>"},{"location":"controllers/relations/#watched-relations","title":"Watched Relations","text":"<p>The Controller::watches relation is for related Kubernetes objects without ownerReferences, i.e. without a standard way for the controller to map the object to the root object. Thus, you need to define this mapper yourself:</p> <pre><code>let main = Api::&lt;MainObj&gt;::all(client);\nlet related = Api::&lt;RelatedObject&gt;::all(client);\n\nlet mapper = |obj: RelatedObject| {\n    obj.spec.object_ref.map(|oref| {\n        ReconcileRequest::from(oref)\n    })\n};\n\nController::new(main, ListParams::default())\n    .watches(related, ListParams::default(), mapper)\n</code></pre>  <p>In this case we are extracing an object reference from the spec of our object. Regardless of how you get the information, your mapper must return an iterator of ObjectRef for the root object(s) that must be reconciled as a result of the change.</p> <p>As a theoretical example; every HPA object bundles a scale ref to the workload, so you could use this to build a Controller for <code>Deployment</code> using HPA as a watched object.</p>"},{"location":"controllers/relations/#external-relations","title":"External Relations","text":"<p>It is possible to be dependent on some external api that you have semantically linked to your cluster, either as a managed resource or a source of information.</p> <ul> <li>If you want to populate an external API from a custom resource, you will want to use finalizers to ensure the api gets cleaned up on CRD deletion.</li> <li>If you want changes to the external API to trigger reconciliations, then you need to write some custom logic.</li> </ul> <p>The current best way to do this is to inject reconciliation requests to the Controller using Controller::reconcile_all_on.</p>"},{"location":"controllers/relations/#subsets","title":"Subsets","text":"<p>With owned and watched relations, it is not always necessary to watch the full space. Use ListParams to filter on the categories you want to reduce IO utilization:</p> <pre><code>let myobjects = Api::&lt;MyObject&gt;::all(client.clone());\nlet pods = Api::&lt;Pod&gt;::all(client.clone())\n\nController::new(myobjects, ListParams::default())\n    .owns(pods, ListParams::default().labels(\"managed-by=my-controller\"))\n</code></pre>"},{"location":"controllers/relations/#summary","title":"Summary","text":"<p>Depending on what type of child object and its relation with the main object, you will need the following setup and cleanup:</p>    Child Controller relation Setup Cleanup     Kubernetes object Owned Controller::owns ownerReferences   Kubernetes object Related Controller::watches n/a   External API Managed custom finalizers   External API Related custom n/a"},{"location":"controllers/security/","title":"Security","text":"<p>Best practices for creating secure, least-privilege controllers with kube.</p>"},{"location":"controllers/security/#problem-statement","title":"Problem Statement","text":"<p>When we are deploying a <code>Pod</code> into a cluster with elevated controller credentials, we are creating an attractive escalation target for attackers. Because of the numerous attack paths on pods and clusters that exists, we should be extra vigilant and following the least-privilege principle.</p> <p>While we can reap some security benefits from the Rust language itself (e.g. memory safety, race condition protection), this alone is insufficient.</p>"},{"location":"controllers/security/#potential-consequences-of-a-breach","title":"Potential Consequences of a Breach","text":"<p>If an attacker can compromise your pod, or in some other ways piggy-back on a controller's access, the consequences could be severe.</p> <p>The incident scenarios usually vary based on what access attackers acquire:</p> <ul> <li>cluster wide secret access \u21d2 secret oracle for attackers / data exfiltration</li> <li>cluster wide write access to common objects \u21d2 denial of service attacks / exfiltration</li> <li>external access \u21d2 access exfiltration</li> <li>pod creation access \u21d2 bitcoin miner installation</li> <li>host/privileged access \u21d2 secret data exfiltration/app installation</li> </ul> <p>See Trampoline Pods: Node to Admin PrivEsc Built Into Popular K8s Platorms as an example of how these types of attacks can work.</p>"},{"location":"controllers/security/#access-constriction","title":"Access Constriction","text":"<p>Depending on the scope of what your controller is in charge of, you should review and constrict:</p>    Access Scope Access to review     Cluster Wide <code>ClusterRole</code> rules   Namespaced <code>Role</code> rules   External Token permissions / IAM roles"},{"location":"controllers/security/#rbac-access","title":"RBAC Access","text":"<p>Managing the RBAC rules requires a declaration somewhere (usually in your yaml/chart) of your controllers access intentions.</p> <p>Kubernetes manifests with such rules can be kept up-to-date via testing#end-to-end-tests in terms of sufficiency, but one should also document the intent of your controller so that excessive permissions are not just \"assumed to be needed\" down the road.</p>  <p>RBAC Rules Generation</p> <p>It is possible to generate rbac rules using audit2rbac (see controller-rs example). This approach has limitations: it needs a full e2e setup with an initial rbac config, and the output may need yaml conversion and refinement steps. However, you can use it to sanity check that your rbac rules are not scoped too broadly.</p>"},{"location":"controllers/security/#crd-access","title":"CRD Access","text":"<p>Installing a CRD into a cluster requires write access to <code>customresourcedefinitions</code>. This can be requested for the controller, but because this is such a heavy access requirement that is only really needed at the install/upgrade time, it is often handled separately. This also means that a controller often assumes the CRD is installed when running (and panicking if not).</p> <p>If you do need CRD write access, consider scoping this to non-delete access, and only for the <code>resourceNames</code> you expect:</p> <pre><code>- apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRole\n  metadata:\n    name: NAME\n  rules:\n  - apiGroups:\n    - apiextensions.k8s.io\n    resourceNames:\n    - mycrd.kube.rs\n    resources:\n    - customresourcedefinitions\n    verbs:\n    - create\n    - get\n    - list\n    - patch\n</code></pre>"},{"location":"controllers/security/#role-vs-clusterrole","title":"Role vs. ClusterRole","text":"<p>Use <code>Role</code> (access for a single namespace only) over <code>ClusterRole</code> unless absolutely necessary.</p> <p>Some common access downgrade paths:</p> <ul> <li>if a controller is only working on an enumerable list of namespaces, create a <code>Role</code> with the access <code>rules</code>, and a <code>RoleBinding</code> for each namespace</li> <li>if a controller is always generating its dependent resources in a single namespace, you could expect the crd to also be installed in that same namespace.</li> </ul>"},{"location":"controllers/security/#namespace-separation","title":"Namespace Separation","text":"<p>Deploy the controller to its own namespace to ensure leaked access tokens cannot be used on anything but the controller itself.</p> <p>The installation namespace can also easily be separated from the controlled namespace.</p>"},{"location":"controllers/security/#container-permissions","title":"Container Permissions","text":"<p>Follow the standard guidelines for securing your controller pods. The following properties are recommended security context flags to constrain access:</p> <ul> <li><code>runAsNonRoot: true</code> or <code>runAsUser</code></li> <li><code>allowPrivilegeEscalation: false</code></li> <li><code>readOnlyRootFilesystem: true</code></li> <li><code>capabilities.drop: [\"ALL\"]</code></li> </ul> <p>But they might not be compatible with your current container setup. See documentation of Kubernetes Security Context Object.</p> <p>For cluster operators, the Pod Security Standards are also beneficial.</p>"},{"location":"controllers/security/#base-images","title":"Base Images","text":"<p>Minimizing the attack surface and amount extraneous code in your base image is also beneficial. It's worth reconsidering and finding alternatives for:</p> <ul> <li> <code>ubuntu</code> or <code>debian</code> (out of date deps hitting security scanners)</li> <li> <code>busybox</code> or <code>alpine</code> for your shell/debug access (escalation attack surface)</li> <li> <code>scratch</code> (basically a blank default root user)</li> </ul> <p>Instead, consider these security optimized base images:</p> <ul> <li> distroless base images (e.g. <code>:cc</code> for glibc / <code>:static</code> for musl)</li> <li> chainguard base images (e.g. gcc-glibc / static for musl)</li> </ul>"},{"location":"controllers/security/#supply-chain-security","title":"Supply Chain Security","text":"<p>If malicious code gets injected into your controller through dependencies, you can still get breached even when following all the above. Thankfully, you will also most likely hear about it quickly from your security scanners, so make sure to use one.</p> <p>We recommend the following selection of tools that play well with the Rust ecosystem:</p> <ul> <li>dependabot or renovate for automatic dependency updates</li> <li><code>cargo audit</code> against rustsec</li> <li><code>cargo deny</code></li> <li><code>cargo auditable</code> embedding an SBOM for trivy / <code>cargo audit</code> / syft</li> </ul>"},{"location":"controllers/security/#references","title":"References","text":"<ul> <li>CNCF Operator WhitePaper</li> <li>Red Hat Blog: Kubernetes Operators: good security practices</li> <li>CNL: Creating a \u201cPaved Road\u201d for Security in K8s Operators</li> <li>Kubernetes Philly, November 2021 - Distroless Docker Images</li> <li>Wolfi OS and Building Declarative Containers (Chainguard)</li> </ul>"},{"location":"controllers/testing/","title":"Testing","text":"<p>This chapter covers controller testing and example Rust Kubernetes test patterns.</p>"},{"location":"controllers/testing/#terminology","title":"Terminology","text":"<p>We will loosely re-use the kube test categories and outline four types of tests:</p> <ul> <li>End to End tests (requires Kubernetes through an in-cluster Client)</li> <li>Integration tests (requires Kubernetes)</li> <li>Mocked unit tests (requires mocking Kubernetes dependencies)</li> <li>Unit tests (no Kubernetes calls)</li> </ul> <p>These types should roughly match what you see in a standard test pyramid where testing power and maintenance costs both increase as you go up the list.</p>  <p>Classification Subjectivity</p> <p>This classification and terminology re-use herein is partially subjective. Variant approaches are discussed.</p>"},{"location":"controllers/testing/#unit-tests","title":"Unit Tests","text":"<p>The basic unit <code>#[test]</code>. Typically composed of individual test function in a tests only module inlined in files containing what you want to test.</p> <p>We will defer to various official guides on good unit test writing in rust:</p> <ul> <li>rust by example - unit tests</li> <li>rust book - writing tests</li> </ul>"},{"location":"controllers/testing/#benefits","title":"Benefits","text":"<p>Very simple to setup, with generally no extra interfaces needed.</p> <p>Works extremely well for algorithms, state machinery, and business logic that has been separated out from network behavior (e.g. the sans-IO approach). Splitting out business logic from IO will reduce the need for more expensive tests below and should be favored where possible.</p>"},{"location":"controllers/testing/#downsides","title":"Downsides","text":"<p>While it is definitely possible to go overboard with unit tests and test too deeply (without protecting any real invariants), this is not what we will focus on here. When unit tests are appropriate, they are great.</p> <p>In the controller context, the main unit test downside is that we cannot cover the IO component without something standing in for Kubernetes - such as an apiserver mock or an actual cluster - making it, by definition, not a plain unit test anymore. </p> <p>The controller is fundamentally tied up in the reconciler, so there is always going to be a sizable chunk of code that you cannot do with plain unit tests.</p>"},{"location":"controllers/testing/#kubernetes-io-strategies","title":"Kubernetes IO Strategies","text":"<p>For the reconciler (and similar Kubernetes calling logic you may have), there are 3 major strategies to test this code.</p> <p>You have one basic choice:</p> <ol> <li>stay in unit-test land by mocking out your network dependencies (worse test code)</li> <li>move up the test pyramid and do full-scale integration testing (worse test reliability)</li> </ol> <p>and then you can also choose to do e2e testing either as an additional bonus, or as a substitute for integration testing. Larger projects may wish to do everything.</p>  <p>Idempotency reducing the need for tests</p> <p>The more you learn to lean on using Server-Side Apply, the less if/else gates will end up with in your reconciler, and thus the less testing you will need.</p>"},{"location":"controllers/testing/#unit-tests-with-mocks","title":"Unit Tests with Mocks","text":"<p>It is possible to to test your reconciler and IO logic and retain the speed and isolation of unit tests by using mocks. This is common practice to avoid having to bring in your all your dependencies and is typically done through crates such as wiremock, mockito, tower-test, or mockall.</p> <p>Out of these, tower-test integrates well with our Client out of the box, and is the one we will focus on here.</p>"},{"location":"controllers/testing/#example","title":"Example","text":"<p>To create a mocked Client with <code>tower-test</code> it is sufficient to instantiate one on a mock service:</p> <pre><code>let (mocksvc, handle) = tower_test::mock::pair::&lt;Request&lt;Body&gt;, Response&lt;Body&gt;&gt;();\nlet client = Client::new(mocksvc, \"default\");\n</code></pre> <p>This is using the generic:</p> <ul> <li><code>http::Request</code> + <code>http::Response</code> objects</li> <li><code>hyper::Body</code> as request/response content</li> </ul> <p>This <code>Client</code> can then be passed into to reconciler in the usual way through a context object (reconciler##using-context), allowing you to test <code>reconcile</code> directly.</p> <p>You do need to write a bit of code to make the test <code>handle</code> do the right thing though, and this does require a bit of boilerplate because there is nothing equivalent to <code>envtest</code> in Rust (so far). Effectively, we need to mock bits of the kube-apiserver and it can look something like this:</p> <pre><code>// We wrap tower_test::mock::Handle\ntype ApiServerHandle = tower_test::mock::Handle&lt;Request&lt;Body&gt;, Response&lt;Body&gt;&gt;;\npub struct ApiServerVerifier(ApiServerHandle);\n\n/// Scenarios we want to test for\npub enum Scenario {\n    /// We expect exactly one `patch_status` call to the `Document` resource\n    StatusPatch(Document),\n    /// We expect nothing to be sent to Kubernetes\n    RadioSilence,\n}\nimpl ApiServerVerifier {\n    pub fn run(self, scenario: Scenario) -&gt; tokio::task::JoinHandle&lt;()&gt; {\n        tokio::spawn(async move { // moving self =&gt; one scenario per test\n            match scenario {\n                Scenario::StatusPatch(doc) =&gt; self.handle_status_patch(doc).await,\n                Scenario::RadioSilence =&gt; Ok(self),\n            }\n            .expect(\"scenario completed without errors\");\n        })\n    }\n\n    /// Respond to PATCH /status with passed doc + status from request body\n    async fn handle_status_patch(mut self, doc: Document) -&gt; Result&lt;Self&gt; {\n        let (request, send) = self.0.next_request().await.expect(\"service not called\");\n        assert_eq!(request.method(), http::Method::PATCH);\n        let exp_url = format!(\"/apis/kube.rs/v1/namespaces/testns/documents/{}/status?&amp;force=true&amp;fieldManager=cntrlr\", doc.name_any());\n        assert_eq!(request.uri().to_string(), exp_url);\n        // extract the `status` object from the `patch_status` call via the body of the request\n        let req_body = to_bytes(request.into_body()).await.unwrap();\n        let json: serde_json::Value = serde_json::from_slice(&amp;req_body).expect(\"patch_status object is json\");\n        let status_json = json.get(\"status\").expect(\"status object\").clone();\n        let status: DocumentStatus = serde_json::from_value(status_json).expect(\"contains valid status\");\n        // attach it to the supplied document to pretend we are an apiserver\n        let response = serde_json::to_vec(&amp;doc.with_status(status)).unwrap();\n        send.send_response(Response::builder().body(Body::from(response)).unwrap());\n        Ok(self)\n    }\n}\n</code></pre> <p>Here we have made an apiserver mock wrapper that will run certain scenarios. Each scenario calls a number of handler functions that assert on certain basic expectations about the nature of the message we receive. Here we only have made one for <code>handle_status_patch</code>, but more are used in controller-rs/fixtures.rs.</p> <p>An actual tests that uses the above wrapper can end up being quite readable:</p> <pre><code>#[tokio::test]\nasync fn doc_reconcile_causes_status_patch() {\n    let (testctx, fakeserver) = Context::test();\n    let doc = Document::test();\n    let mocksrv = fakeserver.run(Scenario::StatusPatch(doc.clone()));\n    reconcile(Arc::new(doc), testctx).await.expect(\"reconciler\");\n    timeout_after_1s(mocksrv).await;\n}\n</code></pre> <p>Effectively, this is an exercise in running two futures together (one in a task), and one in the main test fn, then joining at the end.</p> <p>In this test we are effectively verifying that:</p> <ol> <li>reconcile ran successfully in the given scenario</li> <li>apiserver handler saw all expected messages</li> <li>apiserver handler saw no unexpected messages.</li> </ol> <p>This is satisfied because:</p> <ol> <li>reconcile is unwrapped while handler is running through the scenario</li> <li>Each scenario blocked on sequential api calls to happen (we await each message), so mockserver's joinhandle will not resolve until every expected message in the given scenario has happened (hence the timeout) </li> <li>If the mock server is receiving more Kubernetes calls than expected the reconciler will error with a <code>KubeError(Service(Closed(())))</code> caught by the reconcilers <code>expect</code></li> </ol>  <p>Context and Document constructors omitted</p> <p>Test functions to create the rest of the reconciler context and a test document used by a reconciler are not shown, see controller-rs/fixtures.rs for a relatively small <code>Context</code>. Note that the more things you pass in to your reconciler the larger your <code>Context</code> will be, and the more stuff you will want to mock. </p>"},{"location":"controllers/testing/#benefits_1","title":"Benefits","text":"<p>Using mocks are comparable to using integration tests in power and versatility. It lets us move up the pyramid in terms of testing power, but without needing an actual network boundary and a real cluster. As a result, we maintain test reliability.</p>"},{"location":"controllers/testing/#downsides_1","title":"Downsides","text":"<p>Compared to using a real cluster, the amount of code we need to write - to compensate for a missing apiserver - is currently quite significant. This verbosity means a higher initial cost of writing these tests, and also more complexity to keep in your head and maintain. We hope that some of this complexity can be reduced in the future with more Kubernetes focused test helpers.</p>"},{"location":"controllers/testing/#external-examples","title":"External Examples","text":"<ul> <li>nais/hahaha exec tests using an automock trait</li> </ul>"},{"location":"controllers/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests run against a real Kubernetes cluster, and lets you verify that the IO components of your controller is doing the right thing in a real environment. The big selling point is that they require little code to write and are easy to understand.</p>"},{"location":"controllers/testing/#example_1","title":"Example","text":"<p>Let us try to verify the same status patching scenario from above using an integration test.</p> <p>First, we need a working Client. Using <code>Client::try_default()</code> inside an async-aware <code>#[test]</code> we end up using using the <code>current-context</code> set in your local kubeconfig.</p> <pre><code>    // Integration test without mocks\n    use kube::api::{Api, Client, ListParams, Patch, PatchParams};\n    #[tokio::test]\n    #[ignore = \"uses k8s current-context\"]\n    async fn integration_reconcile_should_set_status() {\n        let client = Client::try_default().await.unwrap();\n        let ctx = State::default().to_context(client.clone());\n\n        // create a test doc and run it through ~= kubectl apply --server-side\n        let doc = Document::test().finalized().needs_hide();\n        let docs: Api&lt;Document&gt; = Api::namespaced(client.clone(), \"default\");\n        let ssapply = PatchParams::apply(\"ctrltest\");\n        let patch = Patch::Apply(doc.clone());\n        docs.patch(\"test\", &amp;ssapply, &amp;patch).await.unwrap();\n\n        // reconcile it (as if it was just applied to the cluster like this)\n        reconcile(Arc::new(doc), ctx).await.unwrap();\n\n        // verify side-effects happened\n        let output = docs.get_status(\"test\").await.unwrap();\n        assert!(output.status.is_some());\n    }\n</code></pre> <p>this sets up a <code>Client</code>, a <code>Context</code> (to be passed to the reconciler), then applies an actual document into the cluster, and at the same time giving it to the reconciler.</p> <p>Feeding the apply result (usually seen by watching the api) is what the Controller internals does, so we skip testing this part. As a result, we get a much simpler test call around only <code>reconcile</code> that we can verify by querying the api after it has completed.</p>  <p>The tests at the bottom of controller-rs/controller.rs go a little deeper, testing a larger scenario.</p>  <p>We need a cluster for these tests though, so on CI we will spin up a k3d instance for each PR. Here is a GitHub Actions based setup:</p> <pre><code>  integration:\n    runs-on: ubuntu-latest\n    strategy:\n      # Prevent GitHub from cancelling all in-progress jobs when a matrix job fails.\n      fail-fast: false\n      matrix:\n        # Run these tests against older clusters as well\n        k8s: [v1.22, latest]\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions-rs/toolchain@v1\n        with:\n          override: true\n          toolchain: stable\n          profile: minimal\n      # Smart caching for Rust\n      - uses: Swatinem/rust-cache@v2\n      - uses: nolar/setup-k3d-k3s@v1\n        with:\n          version: ${{matrix.k8s}}\n          k3d-name: kube\n          # Used to avoid rate limits when fetching the releases from k3s repo.\n          # Anonymous access is limited to 60 requests / hour / worker\n          # github-token: ${{ secrets.GITHUB_TOKEN }}\n          k3d-args: \"--no-lb --no-rollback --k3s-arg --disable=traefik,servicelb,metrics-server@server:*\"\n\n      # Real CI work starts here\n      - name: Build workspace\n        run: cargo build\n\n      # Run the integration tests\n      - name: install crd\n        run: cargo run --bin crdgen | kubectl apply -f -\n      - name: Run all default features integration library tests\n        run: cargo test --lib --all -- --ignored\n</code></pre> <p>This creates a minimal k3d cluster (against both the latest k3d version and our last supported k8s version), and then runs <code>cargo test -- --ignored</code> to specifically only run the <code>#[ignore]</code> marked integration tests.</p>  <p><code>#[ignore]</code> annotations on integration tests</p> <p>We advocate for using the <code>#[ignore]</code> attribute as a visible opt-in for developers. The <code>Client::try_default</code> will work against whatever arbitrary cluster a developer has set to their <code>current-context</code>, so makes it harder (than merely typing <code>cargo test</code>) to accidentally modify random clusters.</p>"},{"location":"controllers/testing/#benefits_2","title":"Benefits","text":"<p>As you can see, this is a lot simpler than the mocking version on the Rust side; no request/response handling and task spawning.</p> <p>At the same time, these tests are more powerful than mocks; we can test the major flow path of a controller in a cluster against different Kubernetes versions with very little code.</p>"},{"location":"controllers/testing/#downsides_2","title":"Downsides","text":""},{"location":"controllers/testing/#low-reliability","title":"Low Reliability","text":"<p>While this will vary between CI providers, cluster setup problems are common.</p> <p>As you now depend on both cluster specific actions to set up a cluster (e.g. setup-k3d-k3s), and the underlying cluster interface (e.g. k3d), you have to deal with compatibility issues between these. Spurious cluster creation failures on GHA are common (particularly on <code>latest</code>).</p> <p>You also have to wait for resources to be ready. Usually, this involves waiting for a Condition, but Kubernetes does not have conditions for everything*, so you can still run into race conditions.</p> <p>It is possible to reduce the reliability problems a bit by using dedicated clusters, but that brings us onto the second pain point;</p>"},{"location":"controllers/testing/#no-isolation","title":"No Isolation","text":"<p>Tests from one file can cause interactions and race conditions with other tests, and re-using a cluster across test runs makes this problem worse as tests now need to be idempotent.</p> <p>It is possible to achieve full test isolation for integration tests, but it often brings impractical costs (such as setting up a new cluster per test, or writing all tests to be idempotent and using disjoint resources).</p> <p>Thus, you can only (realistically) write so many of these tests because you have to keep in your head which tests is doing what to your environment and they may be competing for the same resource names.</p>"},{"location":"controllers/testing/#black-box-variant","title":"Black Box Variant","text":"<p>The setup above is not a black-box integration test, because we pull out internals to create state, and call <code>reconcile</code> almost like a unit test.</p>  <p>Rust conventions on integration tests</p> <p>Rust defines integration tests as acting only on public interfaces and residing in a separate <code>tests</code> directory. </p>  <p>We effectively have a white-box integration test instead.</p> <p>It is possible to export our <code>reconcile</code> plus associated <code>Context</code> types as a new public interface from a new controller library, and then black-box test that (per the Rust definition) from a separate <code>tests</code> directory.</p> <p>In the most basic cases, this is effectively a definitional hack as we;</p> <ul> <li>introduce an arbitrary public boundary that's only used by tests and controller main</li> <li>declare this boundary as public, and test that (in the exact same way)</li> <li>re-plumb controller main to use this interface rather than the old private internals</li> </ul> <p>But this does also separate the code that we consider important enough to test from the rest, and that boundary has been made explicit via the (technically unnecessary) library (at the cost of having more files and boundaries).</p> <p>Doing so will make make your interfaces more explicit, and this can be valuable for more advanced controllers using multiple reconcilers.</p>"},{"location":"controllers/testing/#functional-variant","title":"Functional Variant","text":"<p>Rather than moving interfaces around to fit definitions of black-box tests, we can can also remove all our assumptions about code layout in the first place, and create more functional tests.</p> <p>In functional tests, we instead run the controller directly, and test against it, via something like:</p> <ol> <li>explicitly <code>cargo run &amp;</code> the controller binary</li> <li><code>kubectl apply</code> a test document</li> <li>verify your conditions outside (e.g. <code>kubectl wait</code> or a separate test suite)</li> </ol> <p>CoreDB's operator follows this approach, and it is definitely an important thing to test. In this guide, you can see functional testing done as part of End to End Tests.</p>"},{"location":"controllers/testing/#end-to-end-tests","title":"End to End Tests","text":"<p>End to End tests install your release unit (image + yaml) into a cluster, then runs verification against the cluster and the application.</p> <p>The most common use-case of this type of test is smoke testing, but we can also test a multitude of integration scenarios using this approach.</p>"},{"location":"controllers/testing/#example_2","title":"Example","text":"<p>We will do e2e testing to get a basic verification of our:</p> <ul> <li>packaging system (does the image work and install with the yaml pipeline?)</li> <li>controller happy path (does it reconcile on cluster mutation?)</li> </ul> <p>This thus focuses entirely on the extreme high-level details, leaving lower-level specifics to integration tests, mocked unit tests, or even linting tools (for yaml verification).</p> <p>As a result, we do not require any additional Rust code, as here we will treat the controller as a black box, and do all verification with <code>kubectl</code>.</p>  <p>E2E Container Building</p> <p>An e2e test will require access to the built container image, so spending time on CI caching can be helpful.</p>   <p>An example setup for GitHub Actions:</p> <pre><code>  e2e:\n    runs-on: ubuntu-latest\n    needs: [docker]\n    steps:\n      - uses: actions/checkout@v2\n      - uses: nolar/setup-k3d-k3s@v1\n        with:\n          version: v1.25\n          k3d-name: kube\n          k3d-args: \"--no-lb --no-rollback --k3s-arg --disable=traefik,servicelb,metrics-server@server:*\"\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n      - name: Download docker image artifact from docker job\n        uses: actions/download-artifact@v3\n        with:\n          name: controller-image\n          path: /tmp\n      - name: Load docker image from tarball\n        run: docker load --input /tmp/image.tar\n      # install crd + controller (via chart)\n      - run: kubectl apply -f yaml/crd.yaml\n      - run: helm template charts/doc-controller | kubectl apply -f -\n      - run: kubectl wait --for=condition=available deploy/doc-controller --timeout=20s\n      # add a test intance\n      - run: kubectl apply -f yaml/instance-samuel.yaml\n      - run: kubectl wait --for=condition=somecondition doc/samuel --timeout 2\n      # verify basic happy path outcomes have happened\n      - run: kubectl get event --field-selector \"involvedObject.kind=Document,involvedObject.name=samuel\" | grep \"HideRequested\"\n      - run: kubectl get doc -oyaml | grep -A1 finalizers | grep documents.kube.rs\n</code></pre> <p>Here we are loading a built container via docker buildx from a different test job (named <code>docker</code> here) stashed as a build artifact. Building images will be covered elsewhere, but you can see the CI configuration for controller-rs as a reference.</p> <p>Once the image and the cluster (same k3d setup) is available, we can install the CRD, a test document, and the deployment yaml using whatever yaml pipeline we want/have to deal with (here helm).</p> <p>After installations and resources are ready (checked by kubectl wait or a simpler <code>sleep</code> if you do not have enough conditions), we can verify that basic changes have occurred in the cluster.</p>   <p>CRD installation</p> <p>We separated the CRD installation and the deployment installation because CRD write access is generally a much stronger security requirement that is often controlled separately in a corporate environment.</p>"},{"location":"controllers/testing/#benefits_3","title":"Benefits","text":"<p>These tests are useful because they cover the interface between the yaml and the application along with most unknown-unknowns. E.g. do you read a new evar in the app now? Did you typo something in RBAC, or provide insufficient access?</p> <p>By having a single e2e test we can avoid most of those awkward post-release hot-fixes.</p>  <p>Different approaches</p> <p>It is possible to detect some of these failure modes in other ways. Schema testing via kubeconform, or client-side admission policy verification via conftest for OPA, or kwctl for kubewarden, or polaris CLI for polaris to name a few. You should consider these, but note that they are not foolproof. Policy tests generally verify security constraints, and schema tests are limited by schema completeness and openapi.</p>"},{"location":"controllers/testing/#downsides_3","title":"Downsides","text":"<p>In addition to requiring slightly more complicated CI, the main new downside of using e2e tests over integration tests is error handling complexity; all possible failures modes can occur - often with bad error messages. On top of this, all the previous integration test downsides still apply.</p> <p>As a result, we only want a small number of e2e tests as the signal to noise ratio is going to be low, and errors may not be obvious from failures.</p>"},{"location":"controllers/testing/#summary","title":"Summary","text":"<p>Each test category comes with its own unique set of benefits and challenges:</p>    Test Type Isolation Maintenance Cost Main Test Case     End-to-End  No Reliability + Isolation + Debug Real IO + Full Smoke   Integration  No Reliability + Isolation Real IO + Smoke   Unit w/mocks  Yes Complexity + Readability Substitute IO   Unit  Yes Unrealistic Scenarios Non-IO    <p>The high cost of end-to-end and integration tests is almost entirely due to reliability issues with clusters on CI that ends up being a constant cost. The lack of test isolation in these real environments also make them more attractive as a form of sanity verification/smoke.</p> <p>Focusing on the lower end of the test pyramid (by separating the IO code from your business logic, or by mocking liberally), and proving a few specialized tests at the top end, is likely to to have the biggest benefit-to-pain ratio. As an exercise in redundancy, controller-rs does everything, and can be inspected as a reference.</p>"},{"location":"controllers/webserver/","title":"Web Server","text":"<p>This is a WIP document.</p>"},{"location":"controllers/webserver/#actix-web","title":"Actix-web","text":"<p>Now that we have a more stable release chain of actix-web (version 4 is out), it is easier to write guides, and will use this heavily battle tested web-framework.</p> <pre><code>cargo add actix-web\n</code></pre>  <p>Heavy Weight Framework</p> <p>The <code>actix-web</code> crate is fairly heavy-weight for just exposing metrics. For a simpler web framework that we have partial support for, consider axum and our version-rs application using it.</p>"},{"location":"controllers/webserver/#usage","title":"Usage","text":"<p>This document is unfinished so we refer to controller-rs which is a full-featured example of using actix-web with kube.</p>"},{"location":"crates/kube-client/","title":"kube-client","text":"<p><code>kube-client</code> is the client crate with config and client abstractions. It is re-exported from kube under the <code>kube::client</code> and <code>kube::config</code> modules.</p> <p>This crate has the most extensive documentation on</p> <ul> <li>docs.rs/kube/client</li> <li>docs.rs/kube/config</li> </ul>"},{"location":"crates/kube-core/","title":"kube-core","text":"<p><code>kube-core</code> is the core crate with the lowest level abstractions. It is re-exported from kube under <code>kube::core</code>.</p> <p>This crate has the most extensive documentation on docs.rs/kube/core.</p>"},{"location":"crates/kube-core/#contains","title":"Contains","text":"<p>Core traits and types necessary for interacting with the kubernetes API. This crate is the rust counterpart to kubernetes/apimachinery.</p>"},{"location":"crates/kube-derive/","title":"kube-derive","text":"<p><code>kube-derive</code> is a procedural macro crate with helpers for managing Custom Resource Definitions. Its macros are re-exported from kube.</p> <p>The macros exported are heavily documented on:</p> <ul> <li>docs.rs/kube/CustomResource</li> </ul>"},{"location":"crates/kube-runtime/","title":"kube-runtime","text":"<p><code>kube-runtime</code> is the runtime crate with the highest level abstractions. It is re-exported from kube under <code>kube::runtime</code>.</p> <p>This crate has the most extensive documentation on docs.rs/kube/runtime.</p>"},{"location":"crates/kube/","title":"kube","text":"<p><code>kube</code> is the facade crate that re-exports all the other crates.</p> <p>This crate has the most extensive documentation on docs.rs/kube.</p>"},{"location":"crates/kube/#re-exports","title":"Re-exports","text":"<ul> <li><code>kube</code> re-exports kube-runtime under <code>kube::runtime</code></li> <li><code>kube</code> re-exports kube-core under <code>kube::core</code></li> <li><code>kube</code> re-exports kube-derive's proc macros onto <code>kube</code></li> <li><code>kube</code> re-exports kube-client (flattened) under <code>kube</code> (<code>kube::client</code> and <code>kube::config</code>)</li> </ul>"}]}